# Table of Contents

1. [K-clusters to K-chains as m increases. Our work aims to provide a framework for stakeholders to enhance their supply chain networks’](#k-clusters-to-k-chains-as-m-increases.-our-work-aims-to-provide-a-framework-for-stakeholders-to-enhance-their-supply-chain-networks’)
2. [New Models for Project Management and Scheduling Under Uncertainty](#new-models-for-project-management-and-scheduling-under-uncertainty)
3. [DEI Ambassador Program](#dei-ambassador-program)
4. [Facility/Hub Location 2](#facility/hub-location-2)
5. [Innovative Applications in Business and Technology](#innovative-applications-in-business-and-technology)
6. [Optimization Applications in Defense](#optimization-applications-in-defense)
7. [Network Analysis and Graph Neural Networks I](#network-analysis-and-graph-neural-networks-i)
8. [Sustainability and Power System Efficiency](#sustainability-and-power-system-efficiency)
9. [Blockchain and Governance](#blockchain-and-governance)
10. [Advancements in Choice Modeling and Assortment Optimization](#advancements-in-choice-modeling-and-assortment-optimization)
11. [[tent] Emerging Topics in Pricing and Revenue Management in Retail Industry](#[tent]-emerging-topics-in-pricing-and-revenue-management-in-retail-industry)
12. [Empirical Research in Revenue Management](#empirical-research-in-revenue-management)
13. [Revenue Management: From Theory to Practice](#revenue-management:-from-theory-to-practice)
14. [Theory, Algorithms, and Applications of Multi-objective Optimization](#theory,-algorithms,-and-applications-of-multi-objective-optimization)
15. [Innovative Approaches in Stochastic and Explanatory Modeling](#innovative-approaches-in-stochastic-and-explanatory-modeling)
16. [Optimization Society's Award Session II](#optimization-society's-award-session-ii)
17. [Large-Scale Methods for Linear and Nonlinear Optimization](#large-scale-methods-for-linear-and-nonlinear-optimization)
18. [Methods for Large-Scale Nonlinear and Stochastic Optimization I](#methods-for-large-scale-nonlinear-and-stochastic-optimization-i)
19. [Recent Advances in Mixed-Integer Nonlinear Programming](#recent-advances-in-mixed-integer-nonlinear-programming)
20. [Journal of the Operational Research Society 75th Anniversary](#journal-of-the-operational-research-society-75th-anniversary)
21. [Advances in Nonlinear, Stochastic, and Constrained Optimization](#advances-in-nonlinear,-stochastic,-and-constrained-optimization)
22. [New Applications of Routing Problems](#new-applications-of-routing-problems)
23. [Transportation and Optimization for Strategic Decisions and Policymaking](#transportation-and-optimization-for-strategic-decisions-and-policymaking)
24. [AAS Student Presentation Competition I](#aas-student-presentation-competition-i)
25. [Railway Timetabling and Demand Forecasting](#railway-timetabling-and-demand-forecasting)
26. [Causal Inference & ML](#causal-inference-&-ml)
27. [Active Learning for Optimization](#active-learning-for-optimization)
28. [Decision Making in Healthcare](#decision-making-in-healthcare)
29. [Machine Learning Applications on Inventory Models](#machine-learning-applications-on-inventory-models)
30. [Sustainability and Technology](#sustainability-and-technology)
31. [Operations Research and Machine Learning for Maternal Health](#operations-research-and-machine-learning-for-maternal-health)
32. [Analytics for Social Good: Public Sector Operations](#analytics-for-social-good:-public-sector-operations)
33. [Transportation and Network Optimization at Amazon](#transportation-and-network-optimization-at-amazon)
34. [Nonconvex, Nonsmooth, and Nonregular Optimization: A Computational Framework](#nonconvex,-nonsmooth,-and-nonregular-optimization:-a-computational-framework)

## K-clusters to K-chains as m increases. Our work aims to provide a framework for stakeholders to enhance their supply chain networks’

Room: 10/18/24, 2:13 PM Program Book

10/18/24, 2:13 PM Program Book
K-clusters to K-chains as m increases. Our work aims to provide a framework for stakeholders to enhance their supply chain networks’
adaptability in an increasingly volatile world.
3 - Environmental impacts of pickup point networks in last mile logistics
Ilkka Leppänen, Aalto University, Espoo, Finland, Lauri Kuula, Juuso Leskinen, Linus Antell
Pickup point networks (PPNs) are being increasingly used by logistics companies in e-commerce. Because PPNs reduce the need for last mile
home deliveries, they are often viewed as an environmentally sustainable alternative. However, previous research has not comprehensively
taken into account the environmental impacts that incur when customers travel to pickup points. Moreover, the problem is complicated by
customers' preferences to chain their parcel pickup trips with other daily commute. Building on the vehicle routing problem, we use
operational data and behavioural survey data from a large national postal logistics operator and compare the total environmental impacts that
different PPN configurations have, including a configuration with trip chaining. Our results show the importance of accounting for customer
habits and preferences in network models that are used to study last mile parcel deliveries.
4 - Correspondent Banking Networks Optimization
NIMA SAFAEI, Scotiabank, Toronto, ON, Canada
Correspondent Banking (CB) Network refers to a network of financial institutions providing cross-border payment services for customers
through different channels such as SWIFT, Fedwire, etc. We employ the mathematical programming approach in conjunction with the graph
theory to optimize a CB network. Optimizing the network requires decisions to be made to onboard, terminate or restrict the bank
relationships to optimize the size and overall risk of the network. This study provides theoretical foundation to detect the components, the
removal of which does not affect some key properties of the network such as connectivity and diameter. We find that the correspondent
banking networks have a feature we call k-accessibility, which helps to drastically reduce the computational burden required for finding the
above mentioned components.
SB78
Regency - 709
Optimization Modeling Software II
Invited Session
Computing Society
Chair: Susanne Heipcke, FICO, Xpress Optimization, Birmingham, United Kingdom
Co-Chair: Bob Fourer, AMPL Optimization Inc., Evanston, IL, 60201, United States
1 - Building and Solving Optimization Problems with the Augmented Api for Fico Xpress Solver
Alexander Biele, FICO, Darmstadt, Germany
The new FICO Xpress Solver API for Java, C# and C++ is designed as an object-oriented layer achieving an optimized interaction between
Xpress Solver and model. This leads to a dramatically reduced overall model building time and increased memory efficiency. With the new
API all Xpress Solver features are accessible, including full access to the recently added Global Solver. Among the key features of the new
Xpress Solver API are the ability to use modern programming concepts such as Collections, Streams, Lambdas, and operator overloading to
build expressions and constraints. In the presentation we show that the interface guarantees a consistent user experience across different
programming languages. Furthermore, we will provide guidelines on how to efficiently implement optimization models in the new object-
oriented API and discuss selected examples.
2 - A modeling language-based approach to automatically recommend first-order optimization methods
Sofiane Tanji, UCLouvain, Louvain-la-Neuve, Belgium, Francois Glineur
Since the advent of modern computational mathematics, the literature on optimization algorithms has been ever-growing, and a wide range of
methods are now available. Usually, published algorithms are shown to be applicable to given templates of optimization problems, and are
often accompanied by proofs characterizing their convergence rates.
There exist many variations of such templates. Indeed, a template may describe the objective as a single function or a sum of functions with
different characteristics (say, smoothness, convexity, existence of a computable proximal operator, etc.), accompanied by constraints that can
also be described in various ways (e.g. linear constraints, feasible set with a computable projection operator, functional constraints, etc.).
Hence, for a given instance of an optimization problem, it is not always easy to identify which templates it can match, especially if one wants
to consider equivalent reformulations of the problem. This makes the task of choosing an optimization method tedious for the practitioner.
In this talk, we present a modeling language that aims at describing oracle-based optimization problem formulations. Using this language, we
propose a framework that automatically checks whether a user-provided optimization problem fits a known template. The framework,
implemented as a Python package, relies on an extensive library of optimization methods, described using the above-mentioned language,
with their associated known convergence rate for one or several templates, with the goal of automatically ranking applicable optimization
methods according to their worst-case theoretical guarantees. The framework also handles some reformulation techniques, allowing more
corresponding templates to be potentially identified.
3 - Challenges in Automated Conversion of Optimization Problems
Bob Fourer, AMPL Optimization Inc., Evanston, IL, United States
The range of expressions recognized by modeling languages and solvers has been steadily extended, in ways that make optimization models
easier to describe, validate, and maintain — but that make conversion possibilities ever more numerous and complex. Following an
introductory survey, this presentation describes a variety of challenges that have been faced in detecting formulations that solvers can handle,
https://submissions.mirasmart.com/InformsAnnual2024/Publication/ProgramBook.aspx 137/127610/18/24, 2:13 PM Program Book
and in implementing conversions to forms that solvers require. Issues include distinguishing easy from hard cases, recognizing alternatives
preferred by solvers, accounting for side-effects of tolerance settings, and verifying results. Examples are taken from a new solver interface
library designed for the AMPL modeling language.
4 - User Friendly Modeling from LINDO Systems
Linus Schrage, LINDO Systems, Inc., Chicago, IL, United States
We describe the latest enhancements to 1) LINDO API, the solver engine, and 2) LINGO, the algebraic modeling language. 3) What'sBest!,
the Excel add-in optimizer, Some of the new features are: LINDO API: smarter handling of polynomial functions, and more generally, exotic
functions for which derivatives are not available. Substantial improvements to the Global solver, especially linearization, improved
reproducibility when using concurrent solver(simplex vs barrier). LINGO: Enhanced editor including model-smart text auto-completion so
spelling errors are a thing of the past, and improved performance on large files/models.. What'sBest!: Smart support for more Excel functions,
especially in large spreadsheets, plus improved Black Box solver supporting all Excel functions. To help users get started, we also describe
our expanded Models Library with smart lookup of various application types: financial models, supply chain, scheduling, production line
design, cutting stock, and more.
Sunday, October 20, 12:45 PM - 2:00 PM
SC01

--------------------------------------------------------------------------------

## New Models for Project Management and Scheduling Under Uncertainty

Room: 320

320
New Models for Project Management and Scheduling Under Uncertainty
Invited Session
Scheduling and Project Management
Chair: Caihua Chen, Santiago, N/A, United States
Co-Chair: Houcai SHEN, Nanjing University, Nanjing, 210008
1 - An approach for robust resource constrained project scheduling problem
Qian Hu, Nanjing University, Nanjing, China, People's Republic of
Uncertain activity durations and resource disruptions are often found in project scheduling. We study a robust version of the resource
constrained project scheduling problem. A robust optimization model, reformulation and a new algorithm are studied. With a computational
study, we provide insights about the robustness and analyze the performance of the approach.
2 - Distributed Robust Optimisation Approaches for Resilience and Low Carbon Enhancement of Distributed Power Systems
Caihua Chen, Nanjing University, Nanjing, China, People's Republic of, Houcai SHEN
Resilience and Low Carbon are two trend to quantity the ability of distribution power system. In this work, we discuss the approach which
accesses to microgrids in the main grid, which can handle significant power-disrupting incidents due to their islanding capabilities and the
potential to support renewable energy penetration. In terms of the definition of resilience, we propose a multi-stage distributionally robust
optimization model which capture the preparation, resoinse, and restoration three states. We further add the consideration of the cost of
carbon emissions in our objection. The proposed framework stipulates that the chance constraints hold under the worst-case distribution
within a novel ambiguity set, which incorporates the Wasserstein distance and the first-order moment. To solve the issue, the model is
reduced to a mixed-integer linear programming problem that can readily be implemented. Numerical experiments are carried out on the IEEE
34-bus test systems to show the significantly enhancement of the addtion of microgrids.
3 - R&D Project Investment with Delay Option and Information Spillover Effect
Houcai SHEN, Nanjing University, Nanjing, China, People's Republic of, Yu Ge
This study explores how two competing companies make investment decisions in R&D markets, considering positive policy uncertainty and
negative technology uncertainty. Additionally, this paper demonstrates that the delay option may bring profits for the investors from
information spillover of the pioneer firm and from the waiting value of the market. Since different firms may have different information
spillover effect on the followers, the optimal investment decisions of firms are also considered under asymmetric information spillover effects
in this paper. Game theory is also employed to obtain the Nash equilibrium investment policies for these two competing firms.
4 - Dynamic Assortment with Online Learning under Threshold Multinomial Logit Model
Wenxiang Chen, Nanjing University, Nanjing, China, People's Republic of, Caihua Chen, Houcai SHEN, Ruxian Wang, Weili Xue
Consumers often find themselves overwhelmed by extensive assortments offered by online retailers and show bounded rationality behavior.
However, existing literature on assortment optimization didn’t consider consumers' such bounded rationality behavior. This motivates us to
employ a simple and effective two-stage consider-then-choose model, namely the Threshold Multinomial Logit (TMNL) model to investigate
the online assortment optimization problem. The TMNL model characterizes consumers' endogenous consideration sets formation by the
threshold effect. This endogenous dependency can capture more flexible substitution patterns than the classical MNL choice model, but it
also creates great difficulties for online learning. In the offline assortment setting, we analyze the properties of optimal assortment and
propose an efficient assortment optimization algorithm outperforms the benchmark. In the online setting with unknown customer preferences
and consideration set formation, we propose online learning algorithms that achieve nearly optimal regret bounds under both instance-
independent and instance-dependent conditions. To the best of our knowledge, we are the first work to consider online assortment problem
with consumers' endogenous consider-then-choose behavior. Moreover, our algorithm is extended to the contextual learning setting,
effectively mitigating the impact of the number of products on its performance. Extensive numerical experiments validate the efficacy of our
proposed algorithms.
SC02
https://submissions.mirasmart.com/InformsAnnual2024/Publication/ProgramBook.aspx 138/127610/18/24, 2:13 PM Program Book

--------------------------------------------------------------------------------

## DEI Ambassador Program

Room: 324

324
DEI Ambassador Program
Flash Session
Diversity, Equity, and Inclusion
Chair: Daniel Reich, Naval Postgraduate School, Monterey, CA, United States
Co-Chair: Banafsheh Behzad, California State University, Long Beach, Long Beach, CA, United States
Co-Chair: David Czerwinski, San Jose State University, San Jose, CA, United States
1 - Informs K-12 Education Outreach and Networking Program
Neil Desnoyers, Saint Joseph's University, Philadelphia, PA, United States, Zihan Zhang, Fenglian Pan
The primary goals of the K-12 Education Outreach and Networking Program 2024 DEI Ambassador Project are to extend the footprint of and
strengthen INFORMS’ K-12 outreach program. The major components of our work this year are:
1. Organize an in-person focus group for secondary math teachers in the Philadelphia region, thereby extending the footprint of
INFORMS’ K-12 outreach program.
2. Strengthen INFORMS’ K-12 outreach program by holding a series of webinars introducing K-12 outreach to the INFORMS
membership and others.
3. Facilitate communication and collaboration among the K-12 community within INFORMS by organizing the second K-12 panel at the
2024 INFORMS Annual Meeting.
An update on the project will be provided.
2 - Uncovering Racial Bias in Automated Traffic Law Enforcement: Extensions
Chrysafis Vogiatzis, University of Illinois Urbana-Champaign, Urbana, IL, United States, Chris Raymond-Bertrand, Eleftheria
Kontou
https://submissions.mirasmart.com/InformsAnnual2024/Publication/ProgramBook.aspx 141/127610/18/24, 2:13 PM Program Book
In 2023, two of the classes at the University of Illinois Urbana-Champaign adopted a novel culturally relevant case study that focused on a
contemporary societal issue: injustice in automated traffic law enforcement. The case study, which received the INFORMS Best Case
Runner-up recognition in the INFORMS Annual Meeting in 2023, had two main components. The first one focused on proper statistical
hypothesis testing using real-life demographic and traffic data. The second one introduced a new network optimization problem in the form of
a side-constrained shortest path. Students that were introduced to this case study found it very interesting, specifically due to the connection
and implications that operations research and the management sciences have for policy. In this extension, we study different origin-
destination pairs in the city of Chicago. That way, we can establish how automated traffic law enforcement disproportionately hurts travelers
and commuters from different parts of the city (with different socioeconomic characteristics) when trying to reach specific points of economic
and cultural activity.
3 - Data Analysis&Mdash;Perspectives of DEI Initiatives in Academia & Industry
Mubarak Iddrisu, University of Massachusetts Boston, Boston, MA, United States, Ogechi Vivian Nwadiaru, Hamid Arzani, Andrea
Hupman, Allison Reilly, Jun Zhuang, Gul Kremer
This study examines the alignment between organizational Diversity, Equity, and Inclusion (DEI) commitments at the management level and
individual employee perceptions in practice. Organizational commitment to DEI initiatives is pivotal for fostering inclusive workplace
cultures. However, a gap could exist between aspirational ideals and actual behaviors, raising questions about the practical implementation of
DEI values within organizations. Drawing from existing literature and frameworks, including McKinsey & Company's "Diversity Wins: How
Inclusion Matters," we address this gap by investigating the specific barriers and challenges hindering the translation of DEI commitments
into action. Our research questions include; How do DEI commitments at the management level align with individuals' perceptions in
practice? How do organizations manifest their DEI value statements into actions? How to design metrics to measure DEI initiatives?
Together these questions explore how DEI commitments at the management level align with employees' day-to-day experiences, shedding
light on the effectiveness of DEI initiatives within organizations. Through survey-based methodology, we analyze perceptions from both
managerial and non-managerial roles, focusing on aspects such as inclusion efforts, alignment with institutional DEI statements, and
perceptions of equity. By probing into these dimensions, our study provides evidence-based insights to support organizations, especially in
the operations research and management sciences striving to create genuinely inclusive cultures.
4 - Unite: Uplifting Networks for Inclusive Transformation and Equity
Fatemeh Nosrat, Rice University, Houston, TX, United States, Aysenur Karagoz
We intend to implement tailored mentorship programs, workshops, and resources to address the unique challenges faced by to address the
unique challenges faced by women, African Americans, Asians, Middle Eastern individuals, Hispanic and Latino Americans, Native
Americans, individuals with disabilities, and members of the LGBT+ community within the INFORMS community. Through collaboration
with existing minority communities, such as WORMS and Pride Forum, we aim to foster partnerships and amplify diverse voices. Our
initiatives focus on creating a more inclusive environment, attracting a broader range of students, and celebrating diversity through events and
success stories. By incorporating feedback mechanisms and regular assessments, we are committed to continuous improvement and ensuring
the success of our DEI efforts.
5 - Active Engagement with DEI Students from Data-Related Undergraduate Programs in the INFORMS Data Mining Society and
Beyond
Ying Lin, University of Houston, Houston, TX, United States, Andi Wang, Chun-An Chou, Nathan Gaw
Data science (DS) has drawn increasing attention in academia and every conceivable industry. As a result, there is ever-increasing motivation
for students of diverse backgrounds to learn data mining and decision analytics (DMDA) skillsets for their future career. However, it brings
apparent difficulties and barriers to train and foster students who were not in DS-related majors. This project is a continuation of the project
last year titled, Active Engagement with DEI Students from Data-Related Master’s Programs in the INFORMS Data Mining Society and
Beyond. Due to the massive success in this project from last year, we chose to continue the direction of these efforts by (1) engaging with
DEI students in undergraduate programs, (2) gaining insights on how we can better engage with minority K-12 populations in STEM topics,
and (3) initiating with universities that have underserved Hispanic demographic. To achieve the goals, we initiate the first data mining
undergraduate poster session, host a data competition with K-12 focus, organize a DEI-oriented DMDA workshop and evaluate the impact on
DEI students post the DMDA workshop.
6 - Operations Research in Women's Health and Healthcare: a Scoping-Inspired Narrative Review and Roadmap for the Future
Rachel Stephenson, University of Toronto, Toronto, ON, Canada, Rachel Wong
Operations Research (OR) approaches have been applied to derive meaningful policy insights and effective solutions to a broad range of
healthcare problems, but we believe that there is a gap in OR literature addressing issues that differently or disproportionately impact
women’s health and healthcare (hereafter referred to as “issues in women’s health and healthcare” for brevity). As such, we are undertaking a
project to identify this gap and promote a research agenda to close it. The project consists of three phases:
Phase 1: Perform a scoping-inspired review of literature on significant issues in women’s health and healthcare studied and published in top
OR journals over the past 10 years.
Phase 2: Distribute a survey to experts in women’s health and healthcare with the research gaps identified in Phase 1. Experts will be asked to
prioritize research areas, identify specific subproblems within broad areas that may be solved using OR approaches, and identify any
emerging issues that were excluded from the list provided.
https://submissions.mirasmart.com/InformsAnnual2024/Publication/ProgramBook.aspx 142/127610/18/24, 2:13 PM Program Book
Phase 3: The results from Phases 1 and 2 will be synthesized and disseminated in a survey to the OR community. Experts will be asked to
describe how OR could be applied to solve these problems.
The primary outcome of this work will be the identification of opportunities for novel applications of OR in women’s health and healthcare.
The goal of this project is to promote applications of OR to women’s health and healthcare issues, and therefore, to ultimately improve
women’s health outcomes.
7 - Innovation In Operations Research: Fostering Equity and Justice In Problem Formulation
Sadan Kulturel-Konak, Penn State Berks, Reading, PA, United States, Abdullah Konak, Sara Abu Aridah
The fields of Operations Research (OR) and Management Science (MS) have traditionally prioritized cost, benefit, and efficiency in problem-
solving. While these considerations are undoubtedly vital, our proposed project seeks to push the boundaries of traditional thinking by
introducing new dimensions – justice and equity- in problem-solving. By integrating case studies that highlight social justice and equity
aspects, we aim to broaden the scope of discussions within introductory-level OR and MS courses. Our emphasis extends beyond merely
solving problems efficiently by only focusing on conventional cost/profit optimization; we aspire to actively contribute to the cultivation of a
framework that also prioritizes and fosters principles of equity and justice; by doing so, we hope to provide practices for students to nurture
an ‘equity mindset.’ Integrating social justice and equity perspectives into OR and MS with a multi-objective approach is essential for several
reasons. Firstly, it aligns with the dynamic evolution of societal values, where ethical considerations are increasingly taking center stage.
Secondly, by instilling these concepts at the foundational level, we are not only equipping future professionals to tackle real-world challenges
but also fostering a more inclusive and diverse community within the fields of OR and MS. Our approach involves the development of
problems and case studies that illustrate the intersection of OR with equity and social justice principles. We plan to focus on the basic and
introductory level problems that are typically used in the first OR and MS courses.We will also create instructor companions summarizing
pedagogical strategies to support instructors.
8 - Techfest Dayton: Large-Scale K-12 Stem Outreach Targeting DEI-Focused Organizations, Presenters, & Communities
Kara Combs, Air Force Research Laboratory, Wright-Patterson AFB, OH, United States, Nathan Gaw, Trevor Bihl
TechFest Dayton is the premiere K-12 Science, Technology, Engineering, and Mathematics (STEM) Outreach event held over two days in
Dayton, OH. Dayton faces several socio-economic challenges that affect the city’s poverty, substance abuse, and education levels. Hosted by
its namesake 501(c)3 foundation, TechFest Dayton aims to improve the statistics of these areas by exciting local kids about STEM and higher
education through accessible hands-on activities and demonstrations. In 2024, over 1,100 kids and an estimated 2,500+ attendees came to
TechFest Dayton. With 2023 attendee feedback focused on greater consideration for DEI-related issues, the 2024 committee was happy to
triple the number of DEI-focused exhibitors, schedule presenters such that 20% were female, and add a sensory-friendly hour. This is in
addition to ensuring the event remains free for all participants – attendees, exhibitors, and presenters. Of the children in attendance 42.5% of
them belonged to female- and minority-identifying populations and over half were first-time attendees at the event. The local Cincinnati-
Dayton INFORMS chapter co-exhibited with the Institute of Industrial and Systems Engineers Dayton-Cincinnati professional chapter and
provided an introduction to operations research and engineering for the kids in attendance. Their joint booth had multiple activities including
a getting ready for school activity (management/planning) and two computerized games aimed to teach the kids about statistics and
programming. In conclusion, the DEI Ambassador grant program provided crucial financial support for TechFest Dayton and also played a
pivotal role in diversifying and raising awareness about operations research within the Dayton community.
9 - Diverse Voices in Operations Research and Management Sciences: a Storytelling Platform
Will Cong, Cornell University, Ithaca, NY, United States, Jihye Jang, Yutong Meng, Luyao Zhang
<div overflow-hidden"=""><div h-full"="">
<div text-token-text-primary"="" dir="auto" data-testid="conversation-turn-3" data-scroll-anchor="true">
This project aims to create an inclusive online platform showcasing the diversity within the INFORMS community, fostering interdisciplinary
collaboration, and amplifying underrepresented voices. Through a series of activities and deliverables, including email Q&A interviews,
eCornell webinars, and interview article publications, the project highlights contributions from academia, industry, and policy circles. These
efforts are designed to enhance engagement and promote diversity and inclusion across various spheres. Interview questions tailored for
scholars, practitioners, and policymakers delve into research impact, practical applications, policy implications, challenges, and future
directions, emphasizing the importance of interdisciplinary collaboration in addressing contemporary challenges in operations research and
management sciences.
SC06

--------------------------------------------------------------------------------

## Facility/Hub Location 2

Room: 325

325
Facility/Hub Location 2
Invited Session
Location Analysis
Chair: Gita Taherkhani, Loyola University Chicago, Chicago, IL, United States
1 - Towards Classifying Instances for the Capacitated Facility Location Problem
Stefan Nickel, Karlsruhe Institute of Technology, Karlsruhe, Germany, Hannah Bakker
The Capacitated Facility Location Problem (CFLP) is a core problem of location. As such, it attracted much research presenting novel
solution methods that are typically evaluated with an empirical assessment of a chosen set of benchmark instances. An algorithm is
https://submissions.mirasmart.com/InformsAnnual2024/Publication/ProgramBook.aspx 143/127610/18/24, 2:13 PM Program Book
considered successful if it outperforms other algorithms on average. Such an average-case analysis comes with limitations as it fails to
incentivize a thorough discussion of an algorithm's strengths and weaknesses, and bears a risk of inherent bias in the selected instances. The
latter risk is particularly prevalent for the CFLP for which only very few real-world-based test instances exist, and a significant proportion of
the available instances are based on the same generation procedure. We perform a systematic analysis of the features that make a CFLP
instance hard or easy for a particular algorithm using Instance Space Analysis (ISA), a recent framework that supports objective testing and
assessing the diversity of test instances. While several works exist that propose a variety of characterizing features for different combinatorial
optimization problems, the CFLP differs from these problems in that it contains a more diverse set of parameters that are even measured on
different scales (costs and volume) and have partially overlaying effects on the solution structure. We identify hardness revealing features and
link them to the performance of a greedy heuristic, Kernel Search, and a commercial MIP solver with different parameter settings.
2 - Strategic Expansion of Freight Transportation Hub Networks Under Uncertainty
Gita Taherkhani, Loyola University, Chicago, IL, United States, Hao Li, Sibel Alumur Alev, Mike Hewitt
We focus on freight transportation carriers that transport shipments that are small relative to vehicle capacity and incur transportation costs
that exhibit economies of scale. To achieve profitability, such carriers route shipments through a network of hubs. The locations of a carrier's
hubs dictate the customer markets it can profitably serve. We consider a carrier that seeks to increase its profitability by growing its customer
base. To do so, it seeks to expand its network into new regions by merging with carriers that already operate in those regions. We focus on
how, and by how much, the network that results from such a merger should be redesigned to maximize profitability. To study this issue we
present deterministic and stochastic profit-maximizing hub location models that capture the interaction between economies of scale and
profitability. The models differ in whether and how they model the recognition of uncertainty in shipment sizes in the decision-making
process. We perform a case study based on operations from a multi-regional less-than-truckload freight transportation carrier to derive
insights into the profitability of different redesign strategies. We analyze the networks prescribed by solutions to these optimization models to
derive insights into how a network that results from a merger should be redesigned. We also study how uncertainty impacts the structure of
the redesigned networks.
3 - Periodic Vehicle Location Routing and Scheduling in Humanitarian Logistics
Bahar Yetis Kara, Ihsan Dogramaci Bilkent University, Ankara, Turkey, Cagla Dursunoglu, Okan Arslan
In the recovery phase of a humanitarian crisis, periodic demand service is an important aspect of mobile services, referring to the requirement
for a service that occurs at regular intervals. Mobile barbers, mobile laundry, and mobile showers are examples of mobile services that are in
demand on a regular basis. The demand occurs at certain periods since the impacted population may frequently need such services. In order to
service this demand type, the customer locations need to be visited at certain intervals. This creates a regular pattern of demand that the
mobile service provider must satisfy by providing flexible scheduling and efficient service delivery. Each service type has its own set of
mobile service units and each demand point should be served at least a certain number of times by the various mobile services within the
specified planning horizon. The initial and terminal locations for vehicles and their routes should be determined, which collectively induces
the order of demand visits. Furthermore, the duration of mobile service visits at a demand location is also a decision. There may be
incompatible services that are forbidden to visit a location simultaneously. Additionally, the visits by mobile service centers offering the same
services should be spaced apart by at least a certain duration of time.
SC07

--------------------------------------------------------------------------------

## Innovative Applications in Business and Technology

Room: 327

327
Innovative Applications in Business and Technology
Flash Session
Flash
Chair: Molly Goldstein, University of Illinois, Urbana-Champaign, 104 S. Mathews Ave., Urbana, IL, 61801, United States
1 - Craftsmanship versus Investment: Exploring Factors Affecting the Nonfungible Token (NFT) Projects Performance
Yanxin Wang, Xi'an Jiaotong University, Xi'an, China, People's Republic of, Jingzhao An, Xiaoni Lu, Xi Zhao
The nonfungible token (NFT) market has experienced tremendous volatility in recent years, prompting a need for deeper investigation into
the effective design of NFT projects amid this period of uncertainty. This study therefore investigates how NFT design-related features
influence the performance of NFT projects in the short- and long-term. From the perspective of consumption, this study distinguishes NFT
consumption into competency-based and investment-based consumption, according to the dual attributes of information goods and tokens
associated with NFTs. Correspondingly, we identify two types of design features of NFT collections, namely, competency-related features
(i.e., NFT image complexity and consistency), as well as investment-related features (i.e., initial prices and royalties). Using transaction data
of 3,037 NFT projects on Ethereum blockchain, we find that NFT image complexity has a significant inverted U-shaped effect on long-term
performance due to the possible information overload. Image consistency within NFT projects has positive impacts on both short-term and
long-term performance due to the formulation of brand symbolism. Royalty as the resale cost negatively affects the short-term performance.
Additionally, royalty and initial price exhibit inverted U-shaped impacts on long-term performance, owing to their mixed roles as both costs
and signals. As for the moderating effect of market uncertainty, time pressure brought by uncertainty enhances the impact of image
complexity and royalty, while weakening the role of initial price as a quality signal. Our findings provide both theoretical and practical
implications for NFT project design, enhancing the understanding of consumer behavior in the Web3 context.
2 - The Impact of Social Effect on Funding Success
Sanghyun Park, Baruch College, City University of New York, New York, NY, United States, Qiang Gao
The exponential growth of online donation-based crowdfunding for education has raised questions about the influence of social effects on
funding success and the justification behind backers' decisions. This pioneering study extends beyond conventional crowdfunding research by
examining the unique dynamics of donation-based crowdfunding. Utilizing a comprehensive dataset from DonorsChoose.org and the U.S.
https://submissions.mirasmart.com/InformsAnnual2024/Publication/ProgramBook.aspx 144/127610/18/24, 2:13 PM Program Book
Department of Education, the analysis reveals an inverted U-shaped relationship between predicted social effects and funding success,
indicating that projects with moderate social effects are more likely to succeed compared to those with low or high appeals. The study also
investigates post-donation interactions between fundraisers and backers, providing insights into the reasoning behind backers' decisions. The
results demonstrate that social effects reflected by fundraisers’s unobservable attribute, such as dedication, have a curvilinear relationship
with funding impact. Robustness tests and checks for endogeneity confirm the consistency of the findings. This research highlights the
critical need for strategically balanced social effects in educational crowdfunding narratives to optimize funding success. It significantly
contributes to the broader crowdfunding literature by elucidating the distinct behaviors of donors in the non-return, charitable context of
educational fundraising.
3 - Using Simulation to Enhance Customer Experience and Reduce Congestion Within a Non-Profit Food Bank
Meredith DePuy, Virginia Polytechnic Institute and State University, Blacksburg, VA, United States, Kara Gossard, Madison Harmon,
Kimberly Ellis
A non-profit food bank, sponsored by more than 25 local churches, provides service to 300 families (approximately 900 people) monthly in
southwest Virginia. The food bank operates four days a week (for 1.5 hours each day) and is fully staffed by volunteers, with no paid
employees. With increasing demand and rotating volunteers, the pantry needs to improve and standardize processes to enhance customer
experience. For this project, we evaluate multiple layout, process, and flow alternatives using discrete event simulation. The results are used
to streamline processes and justify capital investments to improve customer flow and overall experience. In addition, the results also provide
valuable insights for similar food banks.
4 - Managing cross-departmental knowledge flows to accelerate learning in a high-tech firm
Alex Alblas, Eindhoven University of Technology , Eindhoven, Netherlands, Massimo Manca, Fred Langerak
This study evaluates the differential effect of cross-departmental knowledge flows on organizational learning and operational performance in
a firm operating in the semiconductor industry. Knowledge transfers across departments via several mechanisms, which have different returns
and require different resources. Previous research examined knowledge transfer mechanisms between departments (R&D, production,
service) separately. Our study integrally estimates knowledge flows’ differential effect on learning and performance simultaneously. Using a
rich panel dataset concerning knowledge flows between departments and service performance, we analyze how the service department learns
to improve machine availability via each knowledge flow. Knowledge flows are quantified with the cumulative number of knowledge transfer
tasks spanning departmental boundaries, such as problem-solving tasks, work instructions, and design changes. Our study contributes by
shedding light on the most effective cross-departmental knowledge flows to accelerate the learning curve in the context of a knowledge-
intensive firm.
5 - Traditional vs. Generative Design: Student Artifacts and Decisions
Molly Goldstein, University of Illinois, Urbana-Champaign, Urbana, IL, United States
The rise of generative tools powered by AI has created an urgent desire in many industries to capitalize on the powerful capabilities of these
tools, but an understanding of how these tools are utilized and perceived by students is lacking. Generative Design is an AI tool in the
computer-aided design software Fusion 360 that can be used to accelerate the design-to-make process. Generative Design achieves this by
taking user-defined constraints, existing assembly geometry, and obstacle geometry to generate numerous potential design solutions that have
been iterated upon one another. The designer can then select which model to continue developing based on several calculated metrics.
However, since generative design produces a range of possible solutions, students need to be able to articulate why those chose a particular
solution.
Research Questions: How do generative design tools affect the quality of engineering student's design artifacts and their design decisions?
An observational study was conducted with engineering student participants of varying levels of standing (undergraduate and graduate) and
CAD experience. Students were tasked with completing two separate challenges in which they had to design and model a structural
component for an existing assembly with traditional parametric CAD techniques and with generative design tools. The collected design
artifacts were assessed for quality using the Trade-off Value Protocol and the Informed Design Teaching and Learning Matrix. We compared
student rationale for their design process in both the traditional and generative design environments. Preliminary results indicate that students
differ on their design rationale in the two environments.
6 - Investigating the impact of behavior representation on targeted ads effectiveness: An empirical study
Wei Xiong, Mercer University, Atlanta, GA, United States
The success of behavioral targeting (BT) hinges largely on how effectively online users are classified, as advertisers prioritize reaching the
most relevant audience for their ads. This study considers BT as a user classification problem and describes a machine learning-based
solution. We introduce three user behavior representation methods and compare them empirically using the area under the receiver operating
characteristic curve (AUC). The experimental results confirm the effectiveness of BT on user classification and provide a validation of BT for
online advertising. Notably, combining user search queries, clicked URLs, and clicked ads yields the most significant performance
improvement compared to a baseline of user location. Furthermore, this study also investigates the temporal dimension of user behavior by
analyzing how historical data length influences targeting performance.
SC08

--------------------------------------------------------------------------------

## Optimization Applications in Defense

Room: 328

328
Optimization Applications in Defense
Invited Session
https://submissions.mirasmart.com/InformsAnnual2024/Publication/ProgramBook.aspx 145/127610/18/24, 2:13 PM Program Book
Military and Security
Chair: Robert Curry, University of Arkansas, Fayetteville, AR, United States
1 - Mixed-integer Programming Models for solving Network Flow Optimization Problems in Contested Settings
Robert Curry, University of Arkansas, Fayetteville, AR, United States
Various applications require dynamic flow solutions characterized by a schedule of network flows consecutively transmitted over a sequence
of successive periods. For these schedules, we assume flows transmit via arcs during periods while flows reside at nodes from one period to
the next. Within this context, we introduce the Maximum Value Dynamic Network Flow Problem (MVDFP) in which we seek to maximize
the cumulative value of a non-simultaneous network flow schedule that accumulates node value whenever some minimum amount of flow
resides at a node between periods. For solving the MVDFP, we first introduce a large mixed-integer program (MIP). As this MIP can become
computationally-expensive for large networks, we also present a computationally-effective heuristic that sequentially solves a series of
smaller, more manageable MIPs. This heuristic approach determines a high-quality solution significantly faster than the MIP obtains an
optimal solution by dividing the full network flow schedule into a sequence of consecutive shorter network flow subschedules. In many cases
this approach produces an optimal solution in a fraction of the MIP's computational time. We present extensive computational results to
highlight our heuristic's efficacy and discuss future research avenues.
2 - Approximate Linear Model for Optimal Surveillance Deployment
Daphne Skipper, United States Naval Academy, Annapolis, MD, United States, Sebastian Martin, Anna Svirsko, Esra Toy
We consider the problem of optimally selecting and locating surveillance devices to detect stealth traffic transiting an operational theater.
These surveillance decisions are based on historical traffic data, as well as sensor ranges and detection capabilities. Accounting for
overlapping sensor coverage leads to a mixed-integer non-convex optimization model. A log transformation converts the model’s nonlinearity
into convex univariate constraints. By careful selection of linearization points, we develop an approximate linear model that is within a
specified error tolerance. We solve this problem in the context of detecting illegal fishing off the coast of Thailand to demonstrate the viability
of the approximate linear model.
3 - Optimal Multi-Searcher Maneuver with Uptime / Downtime Constraints
Anna Svirsko, United States Naval Academy, Annapolis, MD, United States, Sebastian Martin, Miles Dixon, Daphne Skipper, Esra
Buyuktahtakin Toy
When surveillance devices detect a stealth target that is transiting through an operational theater, an area of uncertainty (where the target may
be located) is generated and continues to grow until the target is precisely located. We consider the problem of optimally routing search assets
to find and apprehend targets following the generation of one or more areas of uncertainty by surveillance devices. The integer programming
model allows for searchers with varying initial positions, search speeds, transit speeds, search endurance, and downtime requirements. To
demonstrate its effectiveness, we solve the model to determine an optimal scheme of maneuver for a team of searchers of three different types
seeking targets in two areas of uncertainty.
4 - Prioritizing High Schools for Marine Corps Recruiting
Gary Lazzaro, United States Naval Academy, Annapolis, MD, United States
Marine Corps Recruiting Command needs an improved quantitative method to classify high schools for optimizing recruiting efforts. Marine
recruiters currently use a qualitative model to classify high schools as priority “1” or “2” or “3” or “N” that is based on school population,
number of past recruits, and recruiter opinion. Our analysis shows the current classification process produces inconsistent priority codes for
high schools with similar statistics across the nation and even in the same recruiting areas. We use a variety of analytical techniques to assist
with the determination of high school priority codes. We use linear regression, machine learning methods, and data wrangling techniques to
show how the high school classification process can be improved. Specifically, we demonstrate a data visualization for California high
schools overlaid with additional data fields. We merge publicly available United States census tract data for population, income, education
levels and veteran population with past Marine Corps Recruiting Command data for each high school into one interactive chart.
SC09

--------------------------------------------------------------------------------

## Network Analysis and Graph Neural Networks I

Room: 331

331
Network Analysis and Graph Neural Networks I
Invited Session
Telecommunications and Network Analytics
Chair: Wenting Li, Los Alamos National Laboratory, Los Alamos, NM, United States
1 - Complex-Valued Graph Convolutional Neural Networks and Their Applications in Energy Systems
Tong Wu, Cornell University, New York, NY, United States
The effective representation, processing, analysis, and visualization of large-scale structured data over graphs, especially power grids, are
gaining a lot of attention. So far most of the literature considered exclusively real-valued graph signals. However, graph signals are often
sparse in the Fourier domain, and more informative and compact representations for them can be obtained using the complex envelope of
their spectral components, as opposed to the original real-valued signals. This is the case for the AC carrier, and motivates its complex phasor
representation. Focusing on applications in power systems, in this paper we generalize graph convolutional neural networks (GCN) to the
complex domain, incorporating a complex-valued graph shift operators (GSO) based on the admittance matrix of power grids in the definition
of graph filters (GF) and process complex-valued voltage phasor graph signals (GS). The theory developed is generalized to handle spatio-
temporal complex network processes. We prove that complex-valued GCNs can be stable with respect to perturbations of the underlying
graph support, by bounding of the error propagation through multiple NN layers. The paper showcases the benefits of complex GCN relative
to several benchmarks, in power grid state forecasting and cyber-attack detection and localization.
2 - Demystifying and Mitigating Unfairness for Learning over Graphs
https://submissions.mirasmart.com/InformsAnnual2024/Publication/ProgramBook.aspx 148/127610/18/24, 2:13 PM Program Book
Yanning Shen, The University of California, Irvine, Irvine, CA, United States
We live in an era of big data and ``small world’’, where a large amount of data resides on inter-connected graphs representing a wide range of
physical and social interdependencies, e.g., smart grids and social networks. Hence, machine learning (ML) over graphs has attracted
significant attention and has shown promising success in various applications. Despite this success, the large-scale deployment of graph-
based ML algorithms in real-world systems relies heavily on how socially responsible they are. While graph-based ML models nicely
integrate the nodal data with the connectivity, they also inherit potential unfairness. Using such ML models may therefore result in inevitable
unfair results in various decision- and policy-making in the related applications. To this end, this talk will introduce novel fairness-aware
graph neural network (GNN) designs to address unfairness issues in learning over graphs. Furthermore, theoretical understandings are
provided to explain the potential source of unfairness in GNNs and prove the efficacy of the proposed schemes. Experimental results on real
networks are presented to demonstrate that the proposed framework can enhance fairness while providing comparable accuracy to state-of-
the-art alternative approaches for node classification and link prediction tasks.
3 - Provable Efficient Graph Neural Network Learning via Joint Edge-Model Sparsification
shuai zhang, NJIT, Newark, NJ, United States
Due to substantial computational challenges associated with training large-scale graph neural networks (GNNs), various sparse learning
techniques have been developed to minimize memory and storage demands. These methods include graph sparsification, which involves
sampling a subgraph to reduce data aggregation, and model sparsification, which entails pruning the neural network to decrease the number
of trainable weights. While these strategies have demonstrated empirical success in reducing training costs while preserving test accuracy, a
comprehensive theoretical generalization analysis for sparse learning in GNNs remains underdeveloped. This presentation will concentrate on
the theoretical characterization of joint edge-model sparse learning, exploring its impact on sample complexity and convergence rates towards
bounded generalization error. We will also provide analytical justifications for the strategies of selectively sampling critical nodes and
pruning low-magnitude neurons, showing how these approaches can reduce sample complexity and improve convergence without
compromising test accuracy. Finally, we will discuss our latest progress in the generalization analysis of graph transformers, which examines
the optimization and generalization of a one-layer Graph Transformer for semi-supervised node classification. This analysis shows that the
training mechanism of Graph Transformers, through self-attention and positional encoding, enhances generalization by making the attention
map sparse and promoting the core neighborhood during training.
SC12

--------------------------------------------------------------------------------

## Sustainability and Power System Efficiency

Room: 332

332
Sustainability and Power System Efficiency
Flash Session
Contributed
Chair: Carlos Mateo Samudi Lezcano, Carnegie Mellon University, Pittsburgh, PA, 15213, United States
1 - Equity and Demand Tradeoffs in Level 2 EV Chargers Siting Optimization
Carlos Mateo Samudi Lezcano, Carnegie Mellon University, Pittsburgh, PA, United States
Electric vehicles are a plausible way to decarbonize the US private vehicle fleet. One of the main obstacles to hinder this transition is the
availability of public charging infrastructure. This is particularly true for households that do not possess off-street parking, or low income
households where owners cannot afford to install their own chargers. In deciding where to place these equipment, there is a tension between
serving those locations that are expected to have high demand in the short term, and placing chargers in locations that lack this
infrastructure to be able to adopt electric vehicles. Our work provides a framework to investigate this tension, through a bi-objective mixed
integer linear program, and a series of metrics we develop to quantify the tradeoffs between different demographic groups in an ex post
analysis. Our results indicate that including equity in charger siting and sizing decisions can lead to a more spread out spatial coverage and
a higher number of total households with access to chargers. However, this does not come without drawbacks, since our equity objective
prioritizes disadvantaged populations. Hence, as we increase our preference for this objective, we leave other demographics relatively worse
off. The method was implemented in the cities of Pittsburgh, PA, and Seattle, WA, and can serve as a tool for energy public policy and urban
planning inquiries.
2 - An Electric Vehicle Routing Problem applied for a Chain of Swiss Supermarkets
Reinaldo Garcia, University of Brasilia (UnB) - Director ORLaB (www.orlab.com.br), Brasília, Brazil, Bruno Souza, Ian Amaral
The use of electric vehicles by citizens and companies is already a reality in the world. Moreover, the replacement of vehicles powered by
fossil fuels has been motivated by the most of the governments aiming to mitigate carbon dioxide emissions. In this work, a route
optimization model that includes the electric vehicles distance travel and their load capacity besides the vehicles battery life is implemented
to a chain of supermarkets located in Zurich, Switzerland. The implemented model considered crucial points for the solution such as the labor
working hours, vehicle load capacity, battery life and demand stores in order to minimize the total distance traveled by the delivery trucks.
3 - Integrated electric vehicle routing with time window and mobile charging problem for logistics delivery system
Senyan Yang, Beijing University of Posts and Telecommunications, Beijing, China, People's Republic of, Ruiyan Zhang
Electric vehicles have gradually emerged as a green transportation mode for urban logistics, considering the increase in global warming and
environmental pollution. However, their limited driving range and short battery life limit their widespread deployment. This study proposes
an integrated electric vehicle routing with time window and mobile charging problem for logistics delivery system. In this system, mobile
charging vehicles, serving as the moving charging stations, can charge the electric vehicles flexibly at customer locations, which can reduce
the energy consumption generated by detour visits. The effect of charging on battery degradation is considered to optimize the charging
https://submissions.mirasmart.com/InformsAnnual2024/Publication/ProgramBook.aspx 149/127610/18/24, 2:13 PM Program Book
strategies for extending the battery life. A hybrid heuristic algorithm is developed in the adaptive large neighborhood search framework,
embedding a dynamic programming algorithm. Mobile charging strategies are designed to optimize the charging scheme based on the
synchronization of the electric vehicles and mobile charging vehicles. The numerical experiments are conducted to explore the impact of
battery degradation and mobile charging on the electric vehicle routing and charging solutions. The proposed algorithm performs well in
solving large-scale problems, demonstrating its efficiency and accuracy.
4 - Coordinated Scheduling and Adaptive Control in a Shared Charging System with On-demand Mobility and Individually Owned
Electric Vehicles
Zihao Zhu, Southeast University, Nanjing,China, China, People's Republic of, Xinlian Yu, Xiaofeng Li
Electric vehicles (EVs) have been widely adopted among both individual users and mobility On-demand (MoD) operators, privately due to
their environmental and economic benefits. However, the full potential of EVs has not yet been realized, due to the spatial and temporal
mismatches between uncertain charging demands and the availability of charging stations. In contrast to existing studies, which focus on
charging of MoD fleet or private EVs separately, we aim to optimize the real-time charging scheduling and fleet management for EMoD
considering charging admission of private EVs. A bi-level framework is designed to maximize the revenue of shared charging stations and
EMoD fleet, combing deep reinforcement learning (DRL), queuing, and combinatory optimization. At the upper level, the number of EVs for
charging, serving, and relocating in the fleet are determined via reinforcement learning, incorporating the expected charging revenue from
private EVs which are obtained by a queuing model. In particular, the arrival process of private EVs at each charging station is exogenous,
while that of fleet EVs is scheduled based on the passenger trip requests and charging accessibility. At the lower level, an adaptive admission
strategy of private EVs as well as real-time dispatching for EMoD fleet are optimized based on the upper-level outcomes.
Numerical studies conducted using Manhattan's dataset in New York, USA demonstrate that the proposed framework effectively enhances
both the utilization of charging stations and the fulfillment ratio of passenger trip orders.
5 - Improved Parameterization Schemes for WRF Wind Speeds
Siddhant Srivastava, Carleton University, Gatineau, QC, Canada, Kristen Schell
Wind power forecasting plays a critical role in enhancing the reliability of renewable energy sources; however, significant errors remain in
current forecasting methods. The most detailed computational representation of local wind speeds is delivered by the Weather Research
Forecasting (WRF) model. Despite the widespread use of the WRF model, its performance in diverse terrains remains suboptimal, which is
crucial for grid integration and operational planning. Our study proposes a novel WRF model configuration that improves wind speed
prediction accuracy across these varied terrains. The refined model significantly reduces wind speed prediction errors, thereby improving the
reliability and viability of wind energy as a major power source.
6 - Optimization of Ship-deployed AUVs Synergistic Scheduling for Offshore Wind Turbines Underwater Foundations Inspection
Xu Han, Harbin Engineering University, Harbin, China, People's Republic of, Yuzhen Hu
Guided by the IMO's GHG reduction strategy and the "dual-carbon" goal, offshore wind power has become vital in renewable energy, and
more attention has been paid to the regular inspection of offshore wind turbines (OWTs). The Autonomous Underwater Vehicle (AUV) has
significantly improved inspection, but the current technology limits it to independently perform long-distance and complex tasks. We propose
a ship-deployed AUVs synergistic mode to cover larger areas inspections in a shorter period. A mixed-integer programming model is
developed to optimize the vessels’ routes and schedule AUVs’ drop and pick-up time. An adaptive large neighborhood search heuristic based
on constraint programming called ALNSCP is developed for large-scale instances to help update the current solution during the repair
operation. The simulation instances-based computational experiments verify the superiority of the synergistic mode and solution method in
improving inspection efficiency. Sensitivity analysis further reveals how AUV debugging time and allowed float time affect inspection
efficiency and cost. The experimental results can realize the cost reduction and efficiency of OWTs underwater foundations inspection and
provide decision support for relevant practitioners to develop safety inspection plans.
7 - Condition Monitoring and Fault Detection of Gearbox Subsystem in Mid-sized Onshore Wind Turbines using SCADA Sensor
Data
Jacob Shusko, University of Texas at Austin, Austin, TX, United States
Intelligent operation and maintenance optimization for wind turbines is a crucial area of research, driven by the growing prevalence of wind
energy for electricity generation. The gearbox systems within wind turbines are particularly prone to failures within a five-year timeframe due
to strenuous operating conditions. Effective monitoring of these critical systems can facilitate predictive maintenance strategies, mitigating
the impact of potential total failures which necessitate expensive repairs and prolonged downtime. In this study, we develop condition
monitoring models for key subsystems of mid-sized onshore wind turbines, employing both conventional time series modeling techniques
and machine learning-based approaches. To demonstrate the efficacy of these models, we conduct a case study utilizing an open-source
dataset comprising three years of sensor data from five mid-sized onshore wind turbines, encompassing a range of fault scenarios.
8 - Approximate Maintenance Policies for Large-Scale Offshore Wind Farms
Morteza Soltani, Clemson University, Clemson, SC, United States, Amin Khademi, Jeffrey Kharoufeh
We consider maintenance strategies for a large-scale, offshore wind farm in which wind turbines progressively degrade over time due to
normal usage and exposure to a randomly varying environment. The turbines exhibit both economic and stochastic dependence due to shared
setup costs and their common environment. The objective is to minimize the expected total discounted setup, replacement and lost power
production costs over an infinite horizon. The problem is formulated using a Markov decision process (MDP) model; however, due to the
curse of dimensionality, we examine an approximate formulation that is amenable to solution via column generation. Provided are upper and
lower bounds to assess the quality of the approximation, as well as conditions under which the optimal policy is obtained via the approximate
value function. The sensitivity of the maintenance policy to environment condition, setup cost and farm size is also discussed.
https://submissions.mirasmart.com/InformsAnnual2024/Publication/ProgramBook.aspx 150/127610/18/24, 2:13 PM Program Book
SC13

--------------------------------------------------------------------------------

## Blockchain and Governance

Room: 334

334
Blockchain and Governance
Invited Session
Finance
Chair: Fahad Saleh, University of Florida, Winston-Salem, NC, United States
Co-Chair: Kose John, New York University, New York, NY, United States
1 - Personal Experience Effects across Markets: Evidence from NFT and Cryptocurrency Investing
Chuyi Sun, University of North Carolina, Chapel Hill, NC, United States
I examine how personal experiences causally impact investor behaviors and market boom-bust episodes by exploiting a unique experimental
setting in the non-fungible token (NFT) market. Using blockchain transaction-level data for about 1 million wallets, I find that NFT investors
who randomly receive more valuable NFTs in the primary market are more likely to participate in subsequent primary market sales and trade
more NFTs in the secondary market. These experience effects spill over to the cryptocurrency market as investors who randomly receive
more valuable NFTs purchase more lottery-like cryptocurrencies. I also find that personal experiences and new investor inflows have
contributed to the formation of bubbles in the NFT market. A model-free reinforcement learning framework best explains the empirical
results.
2 - The Economics of Decentralized Autonomous Organizations
Valerie Laturnus, Durham University Business School, Durham, United Kingdom
The advent of blockchain, smart contracts, and Web3 has empowered new concepts for equity partnerships with autonomous operating
systems and democratic corporate governance. This paper explores 2,377 of such new partnerships and uses detailed transaction data (from
2017 through 2022) to examine the performance of so-called decentralized autonomous organizations (DAOs) on Ethereum. As a result, I
find that DAOs with greater participation rates in voting are associated with superior performance. Small members are a prevalent and
important class of investors, while the degree of decentralization in DAOs (ownership concentration) plays only a minor role in firm
valuation. Overall, DAOs are an effective organizational structure, when members take an active interest in the venture.
3 - Does Vote Trading Improve Voting Outcome?
Konstantin Sokolov, University of Memphis, Memphis, TN, United States, Nirmol Das, Sailendra Mishra
Decentralized autonomous organization design implies that stakeholders vote to express their individual opinions. Nonetheless, this design is
disrupted by vote trading. We take advantage of the blockchain data transparency and explore how vote trading affects voting outcome. Our
findings indicate that vote trading facilitates the decision-making by better informed stakeholders. Specifically, informed stakeholders use
purchased votes to signal the quality of their contributions to the platform and thereby attract the non-purchased votes of uninformed
stakeholders. Vote buying typically attracts 51% more non-purchased votes, and the reputation of vote buying stakeholders improves over
time. Therefore, it is unlikely that vote trading leads to overselling the platform contributions. We conduct an experiment to confirm the
robustness of our findings. Finally, an event study reveals that a demand shock in the market for votes encourages voting by those
stakeholders who used to abstain from voting before the shock. Our findings lend support to theoretical and experimental research showing
the benefits of vote trading in the absence of the majority rule.
4 - Centralization vs. Decentralization: First Evidence from the Laboratory
Nir Chemaya, UC Santa Barbara, GOLETA, CA, United States, Gabriele Camera, Gary Charness
We study trading networks where governance of payments flows—or, validation—relies on a digital registry. Strategic manipulation of the
registry causes validation failure and inefficiency. We contrast two architectures: centralized, where validation authority is concentrated in a
single participant (e.g., a Central Bank in a traditional monetary network), and decentralized, with a consensus-based validation mechanism
(e.g., blockchain networks such as Bitcoin). Both architectures admit multiple Pareto-ranked equilibria. Pre-play communication, a natural
feature of a trading environment, may facilitate coordination on efficient play. In our experimental data, decentralization reduced the
incidence of validation failures and boosted trading activity. This governance advantage shows that there is scope for decentralization in
innovating monetary and payments systems.
SC15

--------------------------------------------------------------------------------

## Advancements in Choice Modeling and Assortment Optimization

Room: 335

335
Advancements in Choice Modeling and Assortment Optimization
Invited Session
Revenue Management and Pricing
Chair: Sumit Kunnumkal, ISB, India, India
Co-Chair: Omar El Housni, Cornell Tech, New York, NY, United States
https://submissions.mirasmart.com/InformsAnnual2024/Publication/ProgramBook.aspx 152/127610/18/24, 2:13 PM Program Book
1 - Store Network Design for Omnichannel Retailing
Mert Cetin, Rotterdam School of Management, Erasmus University, Rotterdam, Netherlands, Victor Martínez-de-Albéniz, Laura
Wagner
We investigate the impact of physical store networks on purchase decisions in omnichannel retailing. Using geolocated consumer-level data,
we explore how factors like store proximity, store density, assortment breadth, and inventory availability affect consumer behavior across
online, offline, and hybrid channels, where orders are placed in-store but fulfilled online. Our analysis distinguishes between the effects of
accessibility, such as the number of nearby stores and distance to the closest store, and service quality, including the variety of products
available and stock levels. Our findings indicate that while better physical access to stores generally increases overall sales across all
channels, the service quality provided by physical stores has a more nuanced effect: it boosts offline sales but decreases online and hybrid
sales. This suggests that consumers who have access to well-stocked and varied physical stores may prefer to complete their purchases in
person rather than online. To support retailers in optimizing their strategies, we conduct a counterfactual analysis showing that omnichannel
retailers should consider maintaining dense store networks to sustain high sales levels. This contrasts with the recent trend of store closures,
indicating that a well-distributed physical presence remains crucial for success in omnichannel retailing.
2 - OMGPT: A Sequence Modeling Framework for Data-driven Operational Decision Making
Hanzhao Wang, Imperial College Business School, London, United Kingdom, Kalyan Talluri, Xiaocheng Li
We build a Generative Pre-trained Transformer (GPT) model from scratch to solve sequential decision-making tasks arising in contexts of
operations research and management science which we call OMGPT. We first propose a general sequence modeling framework to cover
several operational decision-making tasks as special cases, such as dynamic pricing, inventory management, resource allocation, and
queueing control. Under the framework, all these tasks can be viewed as a sequential prediction problem where the goal is to predict the
optimal future action given all the historical information. Then we train a transformer-based neural network model (OMGPT) as a natural and
powerful architecture for sequential modeling. This marks a paradigm shift compared to the existing methods for these OR/OM tasks in that
(i) the OMGPT model can take advantage of the huge amount of pre-trained data; (ii) when tackling these problems, OMGPT does not
assume any analytical model structure and enables a direct and rich mapping from the history to the future actions. Either of these two
aspects, to the best of our knowledge, is not achieved by any existing method. We establish a Bayesian perspective to theoretically understand
the working mechanism of the OMGPT on these tasks, which relates its performance with the pre-training task diversity and the divergence
between the testing task and pre-training tasks. Numerically, we observe a surprising performance of the proposed model across all the above
tasks.
3 - Sequential Choice Model with Representative Products: Behavior, Modeling and Optimization
Yu Sun, Chinese University of Hong Kong, Hong Kong, Hong Kong, Daniel Zhuoyu Long, Ruxian Wang
Modeling decision behavior among multiple choice options has been an active research area for several decades. Typically, classic discrete
choice models consider that customers observe all available products simultaneously and select the one with the highest utility (e.g., the
widely used MNL and NL models). In this paper, we propose a novel sequential choice model with representative products to describe
customers' purchase behaviors, in which the products are grouped in multiple nests, and each nest has a representative product that is viewed
in the first stage and other products that are viewed in the second stage. Customers make their purchase decision sequentially following the
product information disclosure process in two stages: in the first stage, customers only observe incomplete information about the product set
(i.e., limited information of all representative products) and choose a nest with the most attractive representative; in the second stage,
customers unlock full information about all products in the selected nest and purchase the product in this nest with the highest utility, where
the no-purchase option may exist in both stages and all nests. We employ the widely used MNL choice model in each stage to formulate the
sequential choice process. Under this sequential choice model, we first investigate the problem on the selection of representative products.
We then investigate the assortment and nest planning problem and establish the optimality of the adjusted revenue-ordered assortments.
Finally, we study the pricing problem and show the optimality of adjusted same-markup pricing policies.
4 - Placement Optimization of Substitutable Products
Omar El Housni, Cornell University, New York, NY, United States, Rajan Udwani
Strategic product placement can have a strong influence on customer purchase behavior in physical stores as well as online platforms.
Motivated by this, we consider the problem of optimizing the placement of substitutable products in designated display locations to maximize
the expected revenue of the seller. We model the customer behavior as a two-stage process: first, the customer visits a subset of display
locations according to a browsing distribution; second, the customer chooses at most one product from the displayed products at those
locations according to a choice model. Our goal is to design a general algorithm that can select and place the products optimally for any
browsing distribution and choice model, and we call this the Placement problem. We give a randomized algorithm that utilizes an α-
approximate algorithm for cardinality constrained assortment optimization and outputs a Θ(α)/log(m)-approximate solution (in expectation)
for Placement with m display locations, i.e., our algorithm outputs a solution with value at least Ω(α)/log(m) factor of the optimal and this is
tight in the worst case. We also give algorithms with stronger guarantees in some special cases. In particular, we give a deterministic
Ω(1)/log(m)-approximation algorithm for the Markov choice model, and a tight (1-1/e)-approximation algorithm for the problem when
products have identical prices.
SC16

--------------------------------------------------------------------------------

## [tent] Emerging Topics in Pricing and Revenue Management in Retail Industry

Room: 336

336
[tent] Emerging Topics in Pricing and Revenue Management in Retail Industry
Invited Session
Revenue Management and Pricing
https://submissions.mirasmart.com/InformsAnnual2024/Publication/ProgramBook.aspx 153/127610/18/24, 2:13 PM Program Book
Chair: Mehmet Altug, George Mason University, Fairfax, VA, United States
Co-Chair: Aditya Jain, Baruch College, Zicklin School of Business, New York, NY, United States
1 - Buyout Price Optimization for the Rent-to-Own Business
Metin Cakanyildirim, The University of Texas at Dallas, Richardson, TX, United States, Milad Armaghan, Andrew Frazelle, Divakar
Rajamani
We study the multidimensional price optimization problem faced by a rent-to-own (RTO) firm, which rents a product and during the rental
offers it for sale repeatedly at a sequence of prices forming the buyout price path. We first employ calculus of variations to obtain optimal
buyout prices in closed form for a special case. Next, to overcome the nonconcavity of the profit in the general problem, we formulate an
equivalent bilevel optimization over the resource utilization and price path. We then transform the inner pricing problem into an efficient
deterministic dynamic program (DP). Standard practice in the RTO business is to steeply decrease buyout prices early in the agreement and
gradually later. However, we prove for the special case that the optimal prices should optimally decrease gradually early in the agreement and
steeply later. Applying our methodology to jointly optimize buyout prices and inventory, our results reveal that higher inventory levels tend to
entail higher optimal prices. Last, in a calibrated case study, we again find that prices should optimally decrease gradually early in the
agreement and steeply later, validating our insights from the special case. Moreover, our methodology yields approximately a 22% increase in
profit relative to industry prices.
2 - If the Blockchain Could Block: Strategic Blockchain Adoption by Manufacturer as Deterrence to the Selling of Counterfeits by
Retailer
Jingjing Weng, Fox School of Business, Temple University, Philadelphia, PA, United States, Abhishek Roy, Subodha Kumar
Counterfeiting is a significant concern for many industries, especially in the online channel. We first investigate an online retailer's motivation
to sell counterfeits along with the genuine product. We then explore the strategic adoption of blockchain technology by the manufacturer to
combat deceptive counterfeit sold by the retailer. Our results show that the retailer is incentivized to sell counterfeits when the production cost
is relatively high. However, a higher production cost does not necessarily imply a higher counterfeit rate. Additionally, we demonstrate that
the introduction of deceptive counterfeits can sometimes benefits the manufacturer while hurting the retailer. That is, the sales of deceptive
counterfeits may lead to win-win, win-lose, lose-win, and lose-lose outcomes for the manufacturer and the retailer. Moreover, we establish
the conditions under which the manufacturer adopts blockchain technology. Furthermore, we identify that blockchain technology can serve as
a commitment device and thus benefit the retailer by eliminating the prisoner's dilemma region. We extend the base model by relaxing several
assumptions, our analysis reveals that our qualitative findings remain consistent with those derived from the base model. Our research
provides novel insights for managers and policymakers in combating counterfeits.
3 - Impact of Price Strategy on Returns: A Price Transparency and Valuation Uncertainty Story
Jane Jiang, The Ohio State University, Columbus, OH, United States, Wedad Elmaghraby, Ozge Sahin
We study how discount strategies affect net sales -- sales after accounting for returns. We evaluate two prevalent discount strategies: bundle
discounts, which apply discounted prices exclusively to product bundles, and single-item discounts, where the discount extends to all
products. Two primary factors are considered in analyzing the discount strategies’ performance in enhancing net sales: the leverage effect of
bundles at the return stage, and customers’ attentiveness to the discount pricing structure. The latter is particularly relevant under bundle
discounts, where customers retaining any items must pay full price, a detail that could be overlooked by those inattentive to pricing, leading
to an overestimation of bundle values. We adopt a forward-looking approach to customer behavior by modeling the interconnected purchase
and return decisions, incorporating post-purchase valuation changes and potential pricing inattentiveness. Partnering with one of Turkey's
largest fashion retailers, we apply structural estimation to ascertain how customers reassess product valuations post-purchase and their
attentiveness to pricing details. Furthermore, we simulate generic retail scenarios to discern the optimal discount strategy for retailers. Our
findings reveal that factoring in returns shifts the retailer’s preference in discount strategies. By tailoring bundles to maximize the leverage
effect post-purchase, the retailer's net sales can be enhanced by 15.6%. We also find that customers tend to overlook pricing structures,
leading to increased returns. Enhancing customer attentiveness to pricing can decrease the retailer's return rates by 20.9%. Moreover,
improving customer attentiveness benefits retailers by enabling them to create more versatile bundle offers, further optimizing their sales
strategy.
4 - Sales Versus Subscription Business Models in Retail: Price and Assortment Competition
Mehmet Altug, George Mason University, Fairfax, VA, United States, Aditya Jain, Tolga Aydinliyim, Oben Ceryan
Motivated by retailers' introduction of subscription services, we study consumers' self-selection between "buying from seller firms" versus
subscribing to renter firms. Modeling price and assortment competition among sellers and renters, we characterize a solution wherein the
renter's large assortment at premium prices and the seller's limited assortment split equilibrium demand.
SC17

--------------------------------------------------------------------------------

## Empirical Research in Revenue Management

Room: 338

338
Empirical Research in Revenue Management
Invited Session
Revenue Management and Pricing
Chair: Abhishek Deshmane, Georgia Institute of Technology, Atlanta, GA, United States
1 - Operations and Analytics to Improve Welfare in Artisanal Supply Chains
Somya Singhvi, USC Marshall School of Business, Los Angeles, CA, United States, Divya Singhvi, Xinyu Zhang
This research investigates strategies to enhance welfare and productivity in artisanal supply chains, focusing on two critical directions:
supervising geographically distributed weavers and using information transparency to increase productivity. First, because weavers work
from geographically remote areas and a limited number of supervisors are expected to visit weavers physically, optimizing supervision
operations is a key challenge. Using supply chain data, we provide robust empirical evidence that frequent supervisor visits can play a crucial
role in improving artisans' productivity. Our results indicate that a one-day decrease in the average number of days between supervisor visits
to remote weavers can increase weaving rates and monthly income by 8.6%-11.3%. We also find that (i) visits to looms with difficult-to-
weave rugs and (ii) visits that are consistently scheduled have a more substantial positive impact on weavers' productivity. To capitalize on
these insights, we propose an optimization framework for scheduling supervisor visits in the supply chain using a Mixed Integer Linear
Program (MILP) and show its impact in practice. Second, we investigate the role of information transparency in improving productivity
within artisanal supply chains. By providing weavers with transparent access to information regarding work completion and potential increase
in income, we hypothesize that productivity will increase. This transparency could also mitigate severe financial stress that weavers often
face during the course of the month. These insights offer practical implications for policymakers and supply chain managers aiming to
improve welfare and efficiency in artisanal supply chains.
2 - Choice Models and Permutation Invariance: Demand Estimation in Differentiated Products Markets
Amandeep Singh, University of Washington, Seattle, WA, United States, Ye Liu, Hema YOGANARASIMHAN
Choice modeling is at the core of understanding how changes to the competitive landscape affect consumer choices and reshape market
equilibria. In this paper, we propose a fundamental characterization of choice functions that encompasses a wide variety of extant choice
models. We demonstrate how non-parametric estimators like neural nets can easily approximate such functionals and overcome the curse of
dimensionality that is inherent in the non-parametric estimation of choice functions. We demonstrate through extensive simulations that our
proposed functionals can flexibly capture underlying consumer behavior in a completely data-driven fashion and outperform traditional
parametric models. As demand settings often exhibit endogenous features, we extend our framework to incorporate estimation under
endogenous features. Further, we also describe a formal inference procedure to construct valid confidence intervals on objects of interest like
price elasticity. Finally, to assess the practical applicability of our estimator, we utilize a real-world dataset from S. Berry, Levinsohn, and
Pakes (1995). Our empirical analysis confirms that the estimator generates realistic and comparable own- and cross-price elasticities that are
consistent with the observations reported in the existing literature.
3 - Data-Driven Real-TIME Coupon Allocation in Online Platform: Theory and Experiment
Weiming Zhu, The University of Hong Kong, Hong Kong, Hong Kong, Hanwei Li, Jinglong Dai, Jianfeng Lin, Binqiang Huang
https://submissions.mirasmart.com/InformsAnnual2024/Publication/ProgramBook.aspx 155/127610/18/24, 2:13 PM Program Book
Recent advancements in machine learning and the availability of abundant customer data enable platforms to provide customized coupons to
individuals. In this study, we partner with Meituan, a leading shopping platform, to develop a real-time, end-to-end coupon allocation system
that is fast and effective in stimulating demand while adhering to marketing budgets when faced with uncertain traffic from a diverse
customer base. Leveraging comprehensive customer and product features, we estimate CVR under various coupon values and employ
isotonic regression to ensure the monotonicity of predicted CVRs with respect to coupon value. Using calibrated CVR predictions as input,
we propose a Lagrangian Dual-based algorithm that efficiently determines optimal coupon values for each arriving customer within 50
milliseconds. We theoretically and numerically investigate the model performance under parameter misspecifications and apply a control loop
to adapt to customer features in real-time to better adhere to the marketing budget. Finally, we demonstrate through large-scale field
experiments and observational data that our proposed coupon allocation algorithm outperforms traditional approaches in terms of both higher
conversion rates and increased revenue. To date, our framework has been implemented by Meituan's bike-sharing division and has distributed
coupons to over 100 million users across more than 110 major cities in China. This implementation has resulted in a 0.7% increase in
revenue, which is equivalent to an additional 8 million RMB in annual profit.
4 - Unlocking the Power of Exchangeable Tickets: a Study of Consumer Behavior in spORts Events
Ovunc Yilmaz, University of Colorado Boulder, Boulder, CO, United States, Hayri Alper Arslan, Yao Cui
Using transactional data from a US professional sports company that owns a baseball and a soccer team, we investigate the impact of selling
exchangeable tickets that provide fans flexibility to change the game they attend for a small additional fee. We find that the use of
exchangeable tickets leads to significant changes in customers’ purchase time and total spending.
5 - Managing Cart Abandonment: Evidence from an Online Delivery Platform
Can Kucukgul, Rutgers University - Camden, Philadelphia, PA, United States, Gad Allon, Dmitry Mitrofanov
In the era of e-commerce, the phenomenon of cart abandonment has become a significant challenge for online retailers. Reduction in
abandonment rates allows retailers not only to recover lost revenue but also to improve customer satisfaction. Utilizing a large-size consumer
click data from a prestigious market-leader delivery platform, in this paper, we first aim to uncover both store and cart-level characteristics
impacting abandonment. Further, employing a hazard-logit model, we investigate the intricate interplay between cart abandonment and
customer shopping behavior. Our first set of results indicate that sticker shocks fail to fully explain why customers are deterring from
checking their carts. In addition, we provide valuable insights for online delivery platforms on how to tailor their data-driven promotion
strategies to different customer segments.
SC19

--------------------------------------------------------------------------------

## Revenue Management: From Theory to Practice

Room: 339

339
Revenue Management: From Theory to Practice
Invited Session
Revenue Management and Pricing
Chair: Georgia Perakis, Massachusetts Institute of Technology, Cambridge, MA, United States
Co-Chair: Manuel Moran-Pelaez, MIT Operations Research Center, Cambridge, MA, 02141, United States
1 - Price restraining policies and search cost: economic analysis and implications
Roman Kapuscinski, Ross School of Business, Ann Arbor, MI, United States, Ozge Sahin, Mojtaba Abdolmaleki
During the last twenty years, many traditional retailers have been facing an increasing competition from online retailers and local discounters.
Customers are able to experience products in a brick-and-mortar store but purchase online for lower prices. As a result, traditional retailers’
sales decrease and some of them stop promoting or carrying such products. For manufacturers, however, traditional retailers play a crucial
role by showcasing and advertising products to customers. Manufacturers use price restraining policies to protect retailers’ margins. Resale
Price Maintenance (RPM) and Minimum Advertised Price (MAP) are two most commonly-used policies intended to protect retailers’ margin.
Under RPM policy, the manufacturer sets a minimum price for each product and requires all retailers not to price below it. Under MAP
policy, manufacturers set a “suggested” retail price. While retailers can sell at lower prices, they are not allowed to advertise a price lower-
than-suggested retail price. In this paper, we build a stylized model to study RPM and MAP under various market situations. In particular, we
explore which policy is more beneficial for the manufacturer, retailers, and consumers. We find that MAP policy is beneficial for the
manufacturer when the search cost is high and consumers are highly heterogeneous in their valuation of the product. Otherwise RPM policy
is preferred. Traditional retailers and consumers may prefer MAP to be used, and prefer a market with a higher search cost and higher
heterogeneity in consumer valuations, compared to the manufacturer. Online retailers always prefer MAP policy.
https://submissions.mirasmart.com/InformsAnnual2024/Publication/ProgramBook.aspx 156/127610/18/24, 2:13 PM Program Book
https://submissions.mirasmart.com/InformsAnnual2024/Publication/ProgramBook.aspx 157/127610/18/24, 2:13 PM Program Book
https://submissions.mirasmart.com/InformsAnnual2024/Publication/ProgramBook.aspx 158/127610/18/24, 2:13 PM Program Book
2 - A Revenue Management Approach to Fair Hospital Diagnostic Service Scheduling
Joyce Luo, Massachusetts Institute of Technology, Cambridge, MA, United States, Maureen Canellas, Dessislava Pachamanova,
Georgia Perakis
The management of diagnostic imaging services is a challenging and important task for hospital systems. Suboptimal scheduling of
diagnostic services increases patient wait times and prevents the availability of overnight beds that could otherwise be utilized by incoming
patients. Patients are served by a limited number of radiology technicians, who have been experiencing higher rates of exhaustion and
burnout. Current diagnostic scheduling systems overlook the importance of distributing scans fairly among technicians, which could help
alleviate these problems. In collaboration with a large Level I Trauma hospital in Central Massachusetts, we develop a real-time optimization
algorithm that manages daily radiology technician schedules more fairly and efficiently than current practice. Our algorithm balances the
need to maximize the number of patients receiving scans, with fairness considerations for technicians in terms of scan load and difficulty. We
also consider that patients have different urgency levels and characteristics, so certain more urgent patients need to be seen sooner. We
provide analytical guarantees in comparison to a full-knowledge formulation and analyze our algorithm’s performance using echocardiogram
order data from our hospital partner. Simulation results show that in higher demand scenarios, our algorithm’s policy shortens patient wait
time and improves throughput for certain patient types compared to current hospital policy. In addition, our algorithm’s recommended
allocations are more fair for technicians compared to current hospital allocations in terms of total scan time, total number of scans performed,
difficult scan time, and number of difficult scans performed.
3 - Bayesian Demand Learning and Revenue Management under Limited Capacity
Mihalis Markakis, IESE Business School, Barcelona, Spain, Victor Martínez de Albéniz, Marcos Serrano
In Revenue Management (RM) problems with limited capacity, the optimal price or quantity decision is mainly driven by the ratio of supply
over total demand during the sales season. Under ex ante uncertainty about certain demand statistics, primarily its rate, the seller not only
adjusts pricing or capacity allocation to optimize revenues but also to learn the demand. We thus consider a stylized quantity-based RM
problem whereby a fixed amount of capacity needs to be sold within a given horizon, into either a high margin, low volume channel, or a low
margin, high volume one. The demand rate in either channel is uncertain, but a prior distribution over it is available. We formulate the
dynamic optimization problem with Bayesian demand learning. We provide a clean and intuitive structural characterization for the general
case of the problem; a closed-form solution for the special case where there is one unit of capacity to sell (precisely the regime of limited
capacity); and an efficient heuristic policy for the multi-unit case, which provides near-optimal performance in various regimes in our
numerical experiments. We find that higher uncertainty regarding the demand rate may push the seller to opt for a high margin, low volume
https://submissions.mirasmart.com/InformsAnnual2024/Publication/ProgramBook.aspx 159/127610/18/24, 2:13 PM Program Book
position for longer, rather than look for higher volumes to accelerate learning, as intuition may suggest. Finally, we show that the monetary
value of Bayesian demand learning is comparable to the value of allocating capacity in an optimal way, suggesting that demand learning
could be a first-order consideration in RM.
4 - Robust One-Shot Price Experimentation
Ali Daei Naby, Rotman School of Management, University of Toronto, Toronto, ON, Canada, Setareh Farajollahzadeh, Ming Hu
We assess the benefit of one-shot price experimentation for a seller in setting a price with limited information about the customer valuation
distribution. Specifically, we consider a seller who knows the exact sales probability associated with a single historical price and aims to
maximize the worst-case revenue ratio compared to an oracle with complete knowledge of the valuation distribution, referred to as the
competitive ratio. For the class of regular distributions, we assess the benefit of one-shot price experimentation for the seller, who can first
choose an experimental price and then set a final price based on the realized sales probability. We quantify the value of experimentation,
defined as the difference between the optimal competitive ratio with and without experimentation. More specifically, first, we fully
characterize the optimal distributionally robust experimental price point and its guaranteed performance. Second, we quantify the value of
experimentation. For example, when the market share of the historical price is 0.05 (resp., 0.75), the performance of the optimal robust price
without experimentation is 0.37 (resp., 0.25), while with one-shot price experimentation, the performance is 0.63 (resp., 0.5), where the value
of experimentation is 0.26 (resp. 0.25). Third, we provide insights into experimenting with higher or lower prices compared to the historical
price. For instance, we show that when the historical sales probability exceeds a threshold at around 0.3, the firm should opt for
experimenting with a higher price; otherwise, it should opt for a lower experimental price.
5 - A Robust Optimization Approach to Assortment Planning with Cross-Item Effects
Georgia Perakis, Massachusetts Institute of Technology, Cambridge, MA, United States, Manuel Moran-Pelaez
Assortment planning in retail involves the strategic selection of products to offer customers and their inventory levels, with the goal of
maximizing profit. While cannibalization or more generally substitution effects are widely studied in the assortment planning literature,
complementarity effects remain relatively unexplored. Cannibalization occurs when a product is bought instead of a similar one, and
complementarity occurs when two products are bought together because they will be used together. This paper introduces a novel demand
model that captures cannibalization and complementarity effects. We also introduce a fixed point method to compute solutions for the
assortment planning problem under the aforementioned demand model. Additionally, we introduce a novel robust method to deal with cross-
item effects and their uncertainty while using much less computation power than the fixed point method. Through experiments on synthetic
and real-world retail datasets, we demonstrate the effectiveness of our proposed approaches in improving assortment planning outcomes,
particularly in scenarios with complex interdependencies between items. Our findings underscore the importance of considering both
cannibalization and complementarity effects in assortment planning and provide practical insights for retailers.
SC20

--------------------------------------------------------------------------------

## Theory, Algorithms, and Applications of Multi-objective Optimization

Room: 343

343
Theory, Algorithms, and Applications of Multi-objective Optimization
Invited Session
Multi Criteria Decision Making
Chair: Margaret Wiecek, Clemson University, Clemson, SC, United States
Co-Chair: Nathan Adelgren, United States Naval Academy, Annapolis, MD, United States
1 - A Benders' Decomposition Algorithm for Multiobjective Linear Optimization
Benjamin Hamlin, Clemson University, Clemson, SC, United States, Margaret Wiecek
We consider a multiobjective linear program (MOLP) suitable for Benders' decomposition. Applying the weighted-sum method to the MOLP
yields a parametric linear program which can be solved with a newly developed parametric Benders' decomposition algorithm. We
demonstrate the application of the algorithm to a problem in economics.
2 - Line Decomposition for Multiobjective Programs
Emma Soriano, Clemson University, Clemson, SC, United States, Margaret Wiecek, Mishko Mitkovski
https://submissions.mirasmart.com/InformsAnnual2024/Publication/ProgramBook.aspx 163/127610/18/24, 2:13 PM Program Book
We present a modification of a decomposition theorem for computing the efficient set of multiobjective programs we have earlier established.
The decision space, formerly decomposed into subsets, is now decomposed into lines. The efficient sets on these lines can be obtained in a
closed form by solving multiple single-objective programs. The new theorem shows that the entire efficient set can be computed as the
intersection of the efficient sets on lines. While in general this intersection is infinite, we identify the special cases in which it is finite.
Convergence results allowing for a decomposition implementation and examples are included.
3 - On Subproblem Tradeoffs in Multiobjective Optimization
Philip de Castro, Clemson University, Clemson, SC, United States, Margaret Wiecek
We are interested in obtaining efficient solutions for multiobjective optimization problems (MOPs) with many criteria by decomposing an
MOP into a set of multiobjective subproblems (MOSPs). Notably, we show that (weakly) efficient solutions of the subproblems can be used
to construct (weakly) efficient solutions for the original problem. However, in order to gain a deeper understanding of the interaction between
subproblems, we introduce the concept of subproblem tradeoffs which measures the performance of a feasible decision with respect to each
MOSP. To compute these tradeoffs, we present extensions to the theory of achievement scalarizing functions. Our new theory leads to an
interactive coordination procedure to compute a preferred MOP-efficient solution, and if necessary, achieve an acceptable compromise
among the MOSPs. Finally, we demonstrate the utility of our proposed procedure on an assignment problem inspired by humanitarian aid
management.
4 - Optimization- and Wargaming-Based Approaches to Improve Medevac Mission Planning and Decision Making
Armin Fugenschuh, Brandenburg University of Technology, Cottbus, Germany
In the highly dynamic environment of military operations, efficient medical evacuation (MEDEVAC) of casualties is a critical strategic
challenge. This talk presents a method for planning MEDEVAC missions using mathematical optimization and gamification. By applying
Mixed-Integer Programming (MIP), a mathematical optimization technique, to solve a MEDEVAC dispatching problem, a structured
approach to decision making in emergency situations is presented. Our MIP model enables the detailed consideration of operational
conditions and the mission support with combat helicopters as escorts, and thus improves the strategic planning and efficiency of MEDEVAC
operations. We also discuss ethical considerations in prioritizing evacuation decisions and emphasize the need to integrate ethical principles
into the development. When it comes to prioritize mission safety versus execution speed, we will enter the realm of multicriteria optimization
models. Moreover, we discuss the potential impact of technologies such as wearable monitoring devices on future triage and MEDEVAC
planning that could further improve the precision and effectiveness of medical evacuation.
As a further aspect, the role of gamification is emphasized by developing a board game that simulates the planning task and thus promotes
understanding of the planning software. This approach allows planners to compare the quality of their manual planning with the computer-
generated solution, improve their own planning skills and realize the benefits of software support for automated, AI-assisted planning.
5 - A Multiobjective Exploration of Compact Ecological Reserve System Design
Nathan Adelgren, United States Naval Academy, Annapolis, MD, United States, Lakmali Weerasena
In this work we consider the design of compact ecological reserve systems. Various metrics that have been used in the past to measure the
compactness of such a system are analyzed, and new alternatives are proposed. We then discuss modeling techniques that can be used to
incorporate a number of the most suitable of these metrics into multiobjective optimization models containing binary variables. We note that
many of these metrics require the use of nonlinear objectives and/or constraints. As such, during computational testing we consider both the
original forms of these models as well as linearized alternatives. In order to solve these models, we make slight modifications to an existing
technique for solving multiobjective integer programs and develop a parallel implementation. Details of the implementation are provided
along with computational results.
SC24

--------------------------------------------------------------------------------

## Innovative Approaches in Stochastic and Explanatory Modeling

Room: 344

344
Innovative Approaches in Stochastic and Explanatory Modeling
Contributed Session
Contributed
Chair: Libo Li, University of Southampton, Southampton, United Kingdom
1 - Robust SHAP Computation for Problem-Level Explanatory Modeling
David Collins, Edward Jones, St Louis, MO, United States, Laura Kang
SHAP values are frequently used to understand the order, direction, and relative magnitude of the influence of the underlying features in a
model, however trained models of similar quality can have significant differences in SHAP values, which limits the ability to treat them as
explanatory of the underlying problem rather than just the model. This paper develops a technique for including SHAP values in the
hyperparameter optimization process, which results in a process that is more robust to perturbations in the training data and closer
approximations of the true values of an underlying generative process. This algorithm is broadly applicable to a wide range of models, and
can be computed efficiently using existing optimization techniques, without a significant decrease in the quality of the model.
2 - Optimal solutions with bounded inequality
John Hooker, Carnegie Mellon University, Pittsburgh, PA, United States, Ozgun Elci, Peter Zhang
When multiple stakeholders are affected by the solution of an optimization model, fairness may be a concern. We investigate the utility
distributions across stakeholders that result from maximizing total utility subject to a resource constraint and a bound on inequality, the latter
https://submissions.mirasmart.com/InformsAnnual2024/Publication/ProgramBook.aspx 164/127610/18/24, 2:13 PM Program Book
measured by the utility range or the Gini coefficient. We find that in both cases, the optimal solution consists of only two or three utility
levels, and if there are three, those on the lowest level receive nothing. We provide closed form solutions that depend on the resource costs of
generating utility for individual stakeholders. These results suggest that the occurrence of two or three major socioeconomic classes in many
societies may be partially rooted in the mathematics of optimal distributions.
3 - Managing Distributional Ambiguity in Stochastic Optimization through a Statistical Upper Bound Framework
Jian Hu, University of Michigan - Dearborn, Dearborn, MI, United States
Stochastic optimization is often hampered by distributional ambiguity, where critical probability distributions are poorly characterized or
unknown. Addressing this challenge, we introduce a new framework that targets the minimization of a statistical upper bound for the
expected value of uncertain objectives, facilitating more statistically robust decision-making. Central to our approach is the Average
Percentile Upper Bound (APUB), a novel construct that simultaneously delivers a statistically rigorous upper bound for the population mean
and a meaningful risk metric for the sample mean. The integration of APUB into stochastic optimization not only fortifies the process against
distributional ambiguity but also reinforces key data-driven decision-making attributes, such as reliability, consistency, and comprehensibility.
Notably, APUB-enriched optimization problems feature tractability, with particular advantages in two-stage stochastic optimization with
random recourse. Empirical demonstrations on two-stage product mix and multi-product newsvendor benchmark problems reveal the benefit
of the APUB optimization framework, in comparison with conventional techniques such as sample average approximation and
distributionally robust optimization.
4 - A Framework for Stochastic Fairness in Dominant Resource Allocation with Cloud Computing Applications
Jiaqi Lei, Northwestern University, Evanston, IL, United States, Sanjay Mehrotra
Allocation of limited resources often requires fairness considerations. Such situations arise in application areas such as computer systems,
health systems, and humanitarian logistics. This paper introduces a fairness framework in multi-resource allocation when the anticipated
needs are stochastic. We apply the distributionally robust optimization (DRO) model to the proposed stochastic fairness model when the
distribution of the required resources is uncertain. We show that it satisfies key properties of Stochastic Pareto-efficiency, Stochastic sharing
incentive, and Stochastic envy-freeness under suitable conditions. Empirically we find that the variance in the requested resources has
implications on resource allocation. Convergence and differences in solutions are illustrated using data from two cloud computing
applications.
5 - Explainable Artificial Intelligence via generative agent feedback
Libo Li, University of Southampton, Southampton, United Kingdom
Explainable Artificial Intelligence (XAI) empowers modern AI systems by enhancing their ability to provide clear and understandable
explanations of system outputs. Interactive machine learning, as a design option for producing XAI, shows promising progress in enriching
the machine learning paradigm and offering interfaces and contexts for developing machine learning use cases. Among various research
paradigms developing adaptive and incremental learning algorithms, we propose a novel large language model (LLM)-based generative
adaptive learning approach. Using LLMs as agents to respond to specific modelling tasks, we evaluate machine learning outputs based on
given metrics and embed the evaluation process to refine model building. We explore LLMs' role as generative agents and extensions such as
Supervised Fine-Tuning (SFT) and Retrieval-Augmented Generation (RAG) to assess whether extended architectures improve model
usability compared to general-purpose LLMs. We also assess the impact of auxiliary data sources on our use cases. The rich information
generated during the interactive process benefits system designers and business analysts in gaining insights into model performance. We
discuss the implications of our work through case studies in healthcare risk management, product diffusion, and knowledge management.
Stakeholders such as system developers, administrators, and end-users stand to benefit from the generative adaptive learning approach in XAI
development and deployment.
SC25

--------------------------------------------------------------------------------

## Optimization Society's Award Session II

Room: 420

420
Optimization Society's Award Session II
Award Session
Optimization
Chair: Oktay Gunluk, Georgia Tech, Atlanta, GA, United States
1 - Uniqueness of DRS as the 2 Operator Resolvent-splitting and Impossibility of 3 Operator Resolvent-splitting
Ernest Ryu, UCLA, Los Angeles, CA, United States
Given the success of Douglas–Rachford splitting (DRS), it is natural to ask whether DRS can be generalized. Are there other 2 operator
resolvent-splittings sharing the favorable properties of DRS? Can DRS be generalized to 3 operators? This talk presents the answers: no and
no. In a certain sense, DRS is the unique 2 operator resolvent-splitting, and generalizing DRS to 3 operators is impossible without lifting,
where lifting roughly corresponds to enlarging the problem size. Finally, we conclude by overviewing the recent line of research in
optimization focused on characterizing and classifying first-order optimization algorithms.
2 - On the Column Number and Forbidden Submatrices for Delta-modular Matrices
Luze Xu, University of California, Davis, Davis, CA, United States, Joseph Paat, Ingo Stallknecht, Zach Walsh
Totally unimodular (TU) matrices have been used for decades to solve various integer programs (IPs) in polynomial time using linear
programming. Given the importance of TU matrices, one may ask about generalizations. One such generalization is the family of Delta-
modular matrices, whose full rank subdeterminants are bounded by Delta. An open conjecture is whether IPs with Delta-modular constraint
matrices can be solved in polynomial time if Delta is fixed. A natural approach to tackle this conjecture is to understand structural properties
of Delta-modular matrices. In this talk, we explore one such structural property: how many distinct columns can a rank-r Delta-modular
matrix have? We give an overview of this question and discuss some implications for Delta-modular IPs. By identifying excluded
submatrices and combining these with results from matroid theory, we derive a new bound on this column number that is best possible up to a
polynomial in Delta.
3 - The Magic of Monotone Integer Programs: Polynomial Time Solvability with Min-cut Procedure and Fast High Quality Solutions
for Related Hard Problems
Dorit Hochbaum, UC Berkeley, Berkeley, CA, United States
Monotone Integer problems (IPM) are those formulated as integer programming with constraints that have at most two variables, x-variables,
appearing with opposite sign coefficients, and a third variable, z-variable, that can appear in one constraint only. All IPMs are solved as a
minimum s,t-cut on a respective graph.
Any NP-hard minimization problem formulated with two variables per inequality, has a 2-approximation algorithm, attained with one min-cut
procedure, resulting from
a transformation to IPM.
Many NP-hard problems are IPM plus a budget constraint, e.g. the quadratic knapsack problem and facility dispersion problem. For these, the
breakpoints algorithm that relaxes the budget constraint, delivers high quality solutions in very fast run times. This presentation focuses on
recent results for IPM ratio problems, that include the maximum density subgraph problem. We introduce the Incremental Parametric Cut
algorithm, that solves monotone ratio problems in the complexity of a single min-cut procedure. This algorithm is efficient not only in theory
https://submissions.mirasmart.com/InformsAnnual2024/Publication/ProgramBook.aspx 168/127610/18/24, 2:13 PM Program Book
but in practice as well. For large scale instances of the maximum density problems, it is orders of magnitude faster than recent leading
methods that are all based on heuristics, and also significantly faster than the theoretically-efficient parametric cut algorithm.
SC30

--------------------------------------------------------------------------------

## Large-Scale Methods for Linear and Nonlinear Optimization

Room: 421

421
Large-Scale Methods for Linear and Nonlinear Optimization
Invited Session
OPT: Linear and Conic Optimization
Chair: Zikai Xiong, MIT OR Center, Cambridge, 02139, United States
Co-Chair: Robert Freund, MIT Sloan School of Management, Cambridge, MA, 02139, United States
1 - The radius of statistical efficiency
Mateo Diaz, Johns Hopkins University, Baltimore, MD, United States, Joshua Cutler, Dmitriy Drusvyatskiy
Classical results in asymptotic statistics show that the Fisher information matrix controls the difficulty of estimating a statistical model from
observed data. In this work, we introduce a companion measure of robustness of an estimation problem: the radius of statistical efficiency
(RSE) is the size of the smallest perturbation to the problem data that renders the Fisher information matrix singular. We compute the RSE up
to numerical constants for a variety of test bed problems, including principal component analysis, generalized linear models, phase retrieval,
bilinear sensing, and matrix completion. In all cases, the RSE quantifies the compatibility between the covariance of the population data and
the latent model parameter. Interestingly, we observe a precise reciprocal relationship between the RSE and the intrinsic
complexity/sensitivity of the problem instance, paralleling the classical Eckart–Young theorem in numerical analysis.
2 - Level-Set Geometry and Improving the Performance of PDHG for Linear Optimization
Zikai Xiong, MIT OR Center, Cambridge, MA, United States, Robert Freund
It is now the case that many linear programming (LP) problem instances are at a scale where matrix-factorization-free methods are attractive
or necessary. The restarted primal-dual hybrid gradient method (rPDHG)—with heuristic enhancements and GPU implementation—has been
very successful in solving these huge-scale LP problems; however, its performance can be highly variable and lacks intuitive understanding.
We analyze the theoretical foundation and practical performance of rPDHG and generalize it to general conic linear optimization. Our
analysis reveals that the geometry of the primal-dual (sub-)level sets plays a crucial role in the performance of rPDHG. Specifically,
unfavorable geometry of some instances leads to the poor performance of rPDHG, both in theory and practice. To address this issue, we show
how central-path-based linear transformations - including conic rescaling - can markedly enhance the convergence of rPDHG. Furthermore,
we present computational results that demonstrate how such rescalings accelerate convergence to high-accuracy solutions, and lead to more
efficient methods for huge-scale linear optimization problems.
3 - Optimization on a Finer Scale: Bounded Local Subgradient Variation Perspective
Jelena Diakonikolas, University of Wisconsin-Madison, Madison, WI, United States, Cristobal Guzman
Nonsmooth optimization problems are ubiquitous in industrial and machine learning applications, arising from the need to address tasks such
as resource allocation, threat detection, and model training. Within the realm of mathematical optimization, nonsmooth problems stand as
some of the most challenging; yet they often offer the possibility of developing efficient algorithms with provable guarantees. The complexity
of these problems, encompassing both lower and upper bounds, has historically typically been examined under generic assumptions,
bounding the growth of the objective functions by assuming Lipschitz continuity of either the objective itself or its gradient. In this talk, I will
argue that these traditional assumptions defining classes of nonsmooth optimization problems inadequately capture local properties of
problems that may make them amenable to more efficient algorithms. I will introduce a notion of bounded local variation of the (sub)gradient
and discuss how, under this notion, we can obtain a more fine-grained characterization of the complexity of nonsmooth problems. One
consequence of these results is that, contrary to prior belief, the complexity of nonsmooth optimization problems, such as those with
piecewise linear objectives with polynomially many pieces, can be improved using parallelization even in high dimensions, either if the
problem is unconstrained or if the function is allowed to be queried outside the feasible set.
4 - Solving Saddle Point Formulations of Linear Programs with Frank-Wolfe
Matthew Hough, University of Waterloo, Waterloo, ON, Canada, Stephen Vavasis
We discuss the author's recent efforts toward applying the Frank-Wolfe algorithm to solving linear programs. The talk will introduce two
first-order primal-dual algorithms for solving saddle point formulations of linear programs, namely FWLP and FWLP-P. The former
iteratively applies the Frank-Wolfe algorithm to both the primal and dual of the saddle point formulation of a standard-form LP. The latter is a
modification of FWLP in which regularizing perturbations are used in computing the iterates. We will outline our convergence analysis of
FWLP-P, noting that FWLP convergence guarantees have not yet been established, and finally describe the advantages of using FWLP and
FWLP-P for solving very large LPs.
SC31

--------------------------------------------------------------------------------

## Methods for Large-Scale Nonlinear and Stochastic Optimization I

Room: 422

422
Methods for Large-Scale Nonlinear and Stochastic Optimization I
Invited Session
OPT: Nonlinear Optimization
Chair: Jiahao Shi, University of Michigan, Ann Arbor, MI, United States
https://submissions.mirasmart.com/InformsAnnual2024/Publication/ProgramBook.aspx 169/127610/18/24, 2:13 PM Program Book
Co-Chair: Shagun Gupta, UT Austin, Austin
Co-Chair: Albert Berahas, University of Michigan, Ann Arbor, MI, United States
1 - Retrospective Approximation for Stochastic Constrained Problems Using Sequential Quadratic Programming
Shagun Gupta, UT Austin, Austin, TX, United States, Raghu Bollapragada, Albert Berahas
Sequential Quadratic Programming (SQP) is one of the state of the art algorithms to solve deterministic constrained problems. In recent years
the framework has been extended to solve equality constrained problems with stochastic objective functions. To adapt the step size in
stochastic settings, new schemes like stochastic line search, Lipschitz constant estimation, hessian averaging have been introduced in SQP.
We use SQP algorithms in a Retrospective approximation framework that allows us to solve a series of subsampled deterministic
subproblems to solve the the stochastic constrained problem. This framework decouples the stochasticity from the SQP algorithm allowing us
to use legacy deterministic solvers (other than SQP as well) to solve constrained stochastic programs.
2 - High Efficiency in Stochastic Trust Regions
Sara Shashaani, North Carolina State University, Raleigh, NC, United States, Yunsoo Ha
Trust Region (TR) methods are celebrated for their flexibility in finding stationary points of non-convex problems and their automated step
size adjustments. In noisy settings, stochastic and deterministic sources of error are linked via the TR radius, governing the samples needed to
estimate the objective function value at each point. But this comes at the cost of significant added complexity. Recent developments such as
variance reduction mechanisms and regularization steps have shown promise for making TR competitive with typical worst case complexity
lower bounds of non-convex stochastic optimization algorithms. We discuss the features and risks within a TR framework that can guarantee
better complexity rates in this literature.
3 - A Double Stepsize Stochastic SQP Method with Complexity Guarantees
Michael ONeill, University of North Carolina at Chapel Hill, Chapel Hill, NC, United States
Stochastic gradient algorithms play a key role in the training of large-scale machine learning models. These algorithms can be readily
extended to problems with simple constraints, such as when projections onto the feasible region can be computed efficiently. Recently, there
has been a surge of interest in stochastic gradient-like algorithms for stochastic optimization problems with nonlinear and nonconvex
constraints due to modern machine learning applications such as fair machine learning and physics informed neural networks. In this talk, we
present a new stochastic Sequential Quadratic Programming (SQP) for deterministically constrained stochastic optimization. Unlike previous
work, this algorithm utilizes a different step size for each component of the orthogonal decomposition of the SQP step, which enables faster
convergence with respect to the constraint violation and an improved worst-case complexity result. In addition to improving the worst-case
complexity, this algorithmic approach also enables significant flexibility with respect to computing step sizes, such as employing a
(safeguarded) line search on the constraint violation. Some preliminary numerical experiments will also be presented.
4 - A Quasi-Newton Method for Nonsmooth, Nonconvex, Stochastic Optimization
We consider the minimization of a Lipschitz continuous and expectation-valued function over a closed and convex set. Our focus lies on
obtaining both asymptotics as well as rate and complexity guarantees for computing an approximate stationary point (in a Clarke sense) via
zeroth-order schemes. We adopt a randomized-smoothing-based approach reliant on minimizing a smooth approximation of our objective
function. In such a setting, we develop a zeroth-order stochastic quasi-Newton scheme reliant on a combination of randomized and Moreau
smoothing is proposed and analyzed, for which iteration and sample complexities are derived.
SC32

--------------------------------------------------------------------------------

## Recent Advances in Mixed-Integer Nonlinear Programming

Room: 423

423
Recent Advances in Mixed-Integer Nonlinear Programming
Invited Session
OPT: Global Optimization
Chair: Linchuan Wei, Northwestern University, Evanston, IL, 60201, United States
1 - An Outer Approximation Method for Solving Mixed-Integer Convex Quadratic Programs with Indicators
Linchuan Wei, Northwestern University, Evanston, IL, United States, Simge Kucukyavuz
Mixed-integer convex quadratic programs with indicator variables (MIQP) encompass a
wide range of applications, from statistical learning to energy, finance, and logistics. The outer approximation (OA) algorithm has been
proven efficient in solving MIQP, and the key to the success of an OA algorithm is the strength of the cutting planes employed. In this paper,
we propose a new technique for deriving cutting planes for MIQP from various convex relaxations, and, as a result, we develop new OA
algorithms for solving MIQP at scale. The contributions of our work are two-fold: (1) we bridge the work on the convexification of MIQP
and the algorithm design to solve large-scale problems, and (2) we demonstrate through a computational study on the sparse portfolio
selection problem that our algorithms give rise to significant speedups compared with the state-of-the-art methods in the literature.
2 - Mixed Integer Approach for Fair Regression
Anna Deza, University of California-Berkeley, Berkeley, CA, United States, Alper Atamturk, Andres Gomez
TBD
3 - The Treewidth-Convex Hull Theorem and DP for Cut Generation in MINLP
Sourabh Kumar Choudhary, Georgia Tech, Atlanta, GA, United States, Santanu Dey, Nick Sahinidis
We revisit unconstrained nonlinear binary programming problems with a limited treewidth of the associated hypergraph. We provide an
alternate proof of exactness of a reformulated LP with O(d2n) variables, n being the number of variables and d being the treewidth. Our proof
https://submissions.mirasmart.com/InformsAnnual2024/Publication/ProgramBook.aspx 170/127610/18/24, 2:13 PM Program Book
links the solution of this exact LP with the well-known dynamic programming algorithm which is known to run in O(d2n) time. In the second
part, we introduce column generation method for solving CGLP to add cuts to non-convex problems with multilinear intermediates. We use
the DP algorithm to solve pricing problem. The benefit over the currently practiced algorithm employing graycodes for navigating and pricing
columns is experimentally shown.
4 - Disjunctive Sum of Squares
Yixuan Hua, Princeton University, Princeton, NJ, United States, Amir Ali Ahmadi, Sanjeeb Dash, Bartolomeo Stellato
We present the disjunctive sum of squares (DiSOS) approach for certifying nonnegativity of polynomials. Our method strategically partitions
the certification problem into tractable subproblems, providing certificates with low polynomial degrees. We introduce the concept of DiSOS
cone, and show that it can approximate the set of positive definite forms arbitrarily well. Using the DiSOS cone, we construct a converging
hierarchy of optimization problems to solve general polynomial optimization problems. Furthermore, we provide a constructive approach to
iteratively partition the certification problem in order to develop efficient solution algorithms. Numerical experiments show that our method
is very versatile, as it is able to certify nonnegativity of non-sum-of-squares polynomials, prove copositivity of matrices, and compute the
clique number of graphs.
5 - What's new for the global solver in FICO Xpress
Tristan Gally, FICO, Birmingham, United Kingdom
This talk discusses the latest updates and improvements to the global optimization capability within FICO Xpress Solver, which allows to
solve general mixed-integer nonlinear problems to global optimality. We will discuss the internal workings of the solver, its features, and
recent performance improvements.
SC33

--------------------------------------------------------------------------------

## Journal of the Operational Research Society 75th Anniversary

Room: 424

424
Journal of the Operational Research Society 75th Anniversary
Invited Session
OPT: Optimization Under Uncertainty
Chair: Haitao Li, University of Missouri - St. Louis, Saint Louis, MO, United States
1 - Multi-Facility Location Models Incorporating Multipurpose Shopping Trips
Pawel Kalczynski, California State University-Fullerton, Fullerton, CA, United States, Zvi Drezner, Morton O'Kelly
This work continues to develop and explore the impact of multipurpose trips on retail location. We develop the model of locating multiple
competing facilities of a chain where several competing facilities exist in the area. There may be some existing facilities of the same chain as
well. The addition of multiple new outlets can cause cannibalization of existing sales, but this effect is mitigated by selecting good locations,
and the total market share captured by the chain increases. The introduction of multipurpose trips enhances the total market share of the
location decision maker. Since in reality many customers combine a visit to more than one facility in one shopping trip, the model predicts
the expected market share captured more accurately. Therefore, the selected locations for new facilities are more accurate as well.
2 - Efficient Drone Integration for Last-Mile Rural Healthcare Delivery
Shakiba Enayati, University of Missouri - Saint Louis, St Louis, MO, United States, Sina Ansari, Ziteng Wang, Srikanth Gururajan
Rural areas encounter challenges such as limited infrastructure, vast distances, rugged terrains, and harsh weather conditions. These factors
impede the timely and efficient delivery of medical supplies, putting the health and well-being of residents at risk. Existing literature
recognizes the potential of drones in healthcare logistics, yet a critical research gap exists in optimizing drone selections in the health supply
networks considering realistic energy consumption. We develop a decision-support tool to pragmatically integrate drones into existing last-
mile distribution networks, considering uncertain environmental conditions such as wind, air quality, precipitation, and terrain impacting
drone flight range and payload requirements. We apply our model to a case study from Shield Illinois, an organization that enabled the rapid
and widespread distribution of medical supplies, including test kits and samples, during the COVID-19 pandemic, and present our findings.
3 - On the Optimal Flexibility of Stochastic Service Systems with Multi-Class Customers
Zhe Zhang, Simon Fraser University/Western Washington University, Bellingham, WA 98229, USA, WA, United States
In this talk, by developing some stylized queueing models, we will discuss the issue of determining the optimal level of the service flexibility
for a stochastic service system with multiple class of customers. Both theoretical results and numerical illustrations are presented to the
insights for practitioners for these service systems.
SC34

--------------------------------------------------------------------------------

## Advances in Nonlinear, Stochastic, and Constrained Optimization

Room: 425

425
Advances in Nonlinear, Stochastic, and Constrained Optimization
Invited Session
OPT: Nonlinear Optimization
Chair: Baoyu Zhou, Arizona State University, Tempe, AZ, United States
1 - Universal First-Order Methods for Convex Optimization Problems
Jiaming Liang, University of Rochester, Rochester, NY, United States, Vincent Guigues, Renato Monteiro
We present a generic algorithmic framework for convex optimization without knowing the levels of convexity and smoothness of the
problem. In particular, the framework includes an adaptive subgradient method and an adaptive proximal bundle method. One advantage of
https://submissions.mirasmart.com/InformsAnnual2024/Publication/ProgramBook.aspx 171/127610/18/24, 2:13 PM Program Book
the framework is that it does not perform any line search on the strong convexity parameter. To the best of our knowledge, this is the first
universal method that does not use a backtracking procedure on the strong convexity parameter. We present complexity results in terms of
both the optimality gap and the stationarity. Their complexity bounds are as good as those obtained with known convexity and smoothness
parameters.
2 - Stochastic Nonlinear Constrained Optimization
Qi Wang, Lehigh University, Bethlehem, PA, United States, Frank E. Curtis
This presentation covers two recently proposed classes of stochastic algorithms for constrained optimization that I have explored in my
research: stochastic interior-point methods (sIPM) and stochastic sequential quadratic programming (sSQP). These methods are designed to
solve problems where information about the objective can only be accessed through stochastic gradient estimates wheras constraint function
and derivative values can be computed explicitly. I will discuss the challenges for ensuring the convergence of these algorithms and how our
methods overcome these challenges. Applications in fair machine learning and physics-informed learning are examined, demonstrating the
performance of our proposed algorithms compared to alternative unconstrained and constrained optimization techniques.
3 - A Quasi-Newton Method for Nonsmooth, Nonconvex, Stochastic Optimization
Luke Marrinan, Pennsylvania State University, State College, PA, United States, Uday Shanbhag, Farzad Yousefian
We consider the minimization of a Lipschitz continuous and expectation-valued function over a closed and convex set. Our focus lies on
obtaining both asymptotics as well as rate and complexity guarantees for computing an approximate stationary point (in a Clarke sense) via
zeroth-order schemes. We adopt a randomized-smoothing-based approach reliant on minimizing a smooth approximation of our objective
function. In such a setting, we develop a zeroth-order stochastic quasi-Newton scheme reliant on a combination of randomized and Moreau
smoothing is proposed and analyzed, for which iteration and sample complexities are derived.
4 - A First-Order Augmented Lagrangian Method for Constrained Minimax Optimization
Sanyou Mei, University of Minnesota, Minneapolis, MN, United States, Zhaosong Lu
Minimax optimization with coupled constraints has received tremendous amount of attention in the recent years, finding widespread
applications in many areas such as adversarial training, reinforcement learning and distributed computing. In this talk, we will introduce a
first-order augmented Lagrangian method for solving a class of constrained minimax optimization with coupled constraints. Under some
suitable assumptions, we will establish an operation complexity, measured by its fundamental operations, for the first-order augmented
Lagrangian method for finding an approximate KKT solution of the constrained minimax problem.
SC35

--------------------------------------------------------------------------------

## New Applications of Routing Problems

Room: 428

428
New Applications of Routing Problems
Invited Session
TSL: Freight Transportation
Chair: Rui Zhang, University of Colorado Boulder, Boulder, CO, 80309, United States
Co-Chair: Mengting Chao, University of Maryland-College Park, College Park, MD, United States
1 - On-Demand Routing for B2b Retailers
Mengting Chao, University of Maryland-College Park, College Park, MD, United States
COVID-19 significantly changed the last-mile delivery network for B2B retailers. Due to work-from-home, delivery locations were more
spread out in the suburbs with 1-2 boxes per stop compared to a larger number of boxes in more centralized office locations. This exposed a
general underlying problem in last-mile delivery when demand (total route time) exceeds delivery capacity (temporal). The challenge is to
deliver to all customers on the promised delivery day with the retailer driver (RD) staying within the regular shift hours. Evolving industry
practices include outsourcing some deliveries to on-demand drivers (ODDs). We aim to determine the route of the RD, the locations that the
ODDs will deliver, and the drop-off locations where the RD will hand over packages to the ODDs.
2 - Disaster Response on a Network with Stochastic Demand and Uncertain Edge Accessibility
Jessa Rhea, University of Iowa - Applied Mathematical and Computational Sciences, Iowa City, IA, United States, Jeffrey Ohlmann
We present a mobile facility routing problem to provide safe and immediate disaster response for a network in which there is uncertain edge
traversability and stochastic beneficiary demand. Our problem is formulated as a Markov decision process, and the objective is to maximize
the amount of beneficiary demand served by the capacitated fleet over the problem horizon. We note that a mobile facility can contribute to
this objective even if it does not serve demand at an epoch (e.g., when its capacity is entirely depleted) by determining accessibility of edges
and observing beneficiary demand at locations along its route. Our solution approach is a reinforcement learning algorithm benchmarked by
the vehicle routing problem with stochastic demand.
3 - The Truck-and-Robot Routing Problem with Pickups and Deliveries
Manuel Ostermeier, University of Augsburg, Augsburg, Germany, Tobias Huf
The increasing number of home deliveries paired with various delivery modes and channels require retailers to establish efficient last-mile
solutions. The innovative truck-and-robot concept poses a promising approach in this regard. The concept relies on Sidewalk Autonomous
Delivery Robots (SADRs) carried and released by trucks to serve customers in predefined time windows. We extend existing concepts by
integrating store locations and corresponding customer requests for direct deliveries from stores. This setup combines the first and the last
mile, giving rise to a novel concept where different delivery modes are established that provide robots as a service to pick up and deliver
goods to customers.
The described problem is formalized as the Truck-and-Robot Pickup-and-Delivery Problem (TnR-PDP) and solved using a tailored approach,
the Adaptive Genetic Algorithm (AGA). The AGA is based on a recombination-based search framework but tailored to the problem-specifics
https://submissions.mirasmart.com/InformsAnnual2024/Publication/ProgramBook.aspx 173/127610/18/24, 2:13 PM Program Book
(e.g., no or multiple visits per location) using specialized recombination operators and an adaptive search strategy for location and operator
selection.
The numerical experiments show that our approach outperforms existing benchmark approaches concerning runtime by up to 79 % while also
improving the solution quality. The AGA solves relevant instances in a reasonable amount of time and generates robust solutions. Further
analyses highlight the value of store integration into the truck-and-robot concept and show the benefits of the TnR-PDP compared with
commonly applied approaches in the industry.
4 - Estimating Optimal Solution Values in TSP, VRP, and SDVRP
Shuhan Kou, University of Maryland, College Park, College Park, MD, United States, Bruce Golden, Luca Bertazzi
Motivated by Basel and Willemain's (2001) work on estimating optimal tour lengths in the Traveling Salesman Problem (TSP), we identify an
intrinsic relationship between the distribution of the feasible solution space and the optimal solution value in combinatorial optimization
problems. We begin with a linear regression model that uses standard deviation as a predictor to predict the optimal TSP tour length. Building
on this, we improve our model by incorporating the mean and minimum predictors, significantly enhancing predictive accuracy. We also
extend this method to over 10,000 vehicle routing problem (VRP) instances. To tackle the more complex split delivery vehicle routing
problem (SDVRP), we integrate mean, standard deviation, and two other topological features into our model. Evaluating our approach across
95 diverse benchmark instances, our regression model provides an average margin of 3%. Moreover, our recent experiments reveal that
employing these predictors in random forest and neural network models further enhances the predictive accuracy beyond that of linear
regression for the TSP and the VRP.
5 - An empirical study of a compact formulation of the capacitated fixed-charge network flow problem
Eli Olinick, Southern Methodist University, Dallas, TX, United States
The triples formulation is a compact formulation of multicommodity network flow that provides an alternative to the traditional and widely
used node-arc and arc-path representations. When applied in mixed integer programming formulations of the capacitated fixed charge
network flow problem (CFCNFP), the triples formulation significantly reduces the number of constraints and continuous variables compared
to the standard node-arc formulations. In an empirical study, we find that CFCNFP instances that can be solved to provable optimality are
solved an average of 2.75 times faster with the triples formulation than with the node-arc formulation. For instances that cannot be solved to
provable optimality in our computing environment, we find that given a 30-minute time limit the triples formulation produces solutions with
lower costs and optimality gaps.
SC37

--------------------------------------------------------------------------------

## Transportation and Optimization for Strategic Decisions and Policymaking

Room: 429

429
Transportation and Optimization for Strategic Decisions and Policymaking
Invited Session
TSL: Urban Transportation Planning and Modeling
Chair: Arthur Delarue, Georgia Institute of Technology, Atlanta, GA, United States
1 - School District Design Under Uncertainty
Aysu Ozel, Northwestern University, Evanston, IL, United States, Karen Smilowitz
Many decisions in school district design aim to provide continuity in educational experiences, particularly long-term district design decisions
that include opening/closing schools, drawing attendance boundaries, and placing programs within schools. Given the need for continuity, it
is important to consider the robustness of district design options with respect to uncertain changes. We explore school district design under
uncertainty in the larger context of decision making under uncertainty, considering the unique features of school settings. We examine
multiple sources of uncertainty in school enrollment, including exogenous demographic changes and endogenous changes stemming from
community response to district design updates. Further, we consider modeling of decisions given different levers to address uncertain
enrollment and current practices in school districts.
2 - On-trip matching in shared rides
Julia Yan, University of British Columbia, Vancouver, BC, Canada, Yifan Shen, Chiwei Yan
We will discuss issues related to on-trip matching in shared rides.
3 - A Scalable Algorithm for Large-Scale Network Design with Service Time Guarantees
Myungeun Eom, Georgia Institute of Technology, Atlanta, GA, United States, Alejandro Toriello, Alan Erera
We propose a scalable algorithm for a large-scale network design problem with service time guarantees, motivated by small package delivery
systems. Our model captures an important constraint that commodities must be delivered within specified service time periods. We aim to
determine paths for commodities to minimize transportation and hold costs while ensuring the service time guarantees. We solve this problem
on the entire United States network, which includes approximately 250 hub buildings and 40,000 commodities. The main challenges of this
problem arise from the large network size and the high volume of commodities. To address the scalability issue from the network size, we
partition the network into smaller sub-networks considering geographic features. We solve each sub-network independently for commodities
whose origins and destinations are within the same sub-network. For commodities with origins and destinations in different sub-networks, we
identify networks containing potential transfer terminals and solve the problem within these chosen networks. To handle a large number of
commodities, we develop a batching method that prioritizes commodities with less flexibility. We iteratively solve the problem for each
https://submissions.mirasmart.com/InformsAnnual2024/Publication/ProgramBook.aspx 174/127610/18/24, 2:13 PM Program Book
batch, building on the previous solutions for consolidation. We demonstrate the effectiveness of our approach through a computational study
conducted on networks of various sizes.
4 - Two-stage stochastic optimization formulation for the police staffing problem
Lynn Xu, Georgia Institute of Technology, Atlanta, GA, United States
We present a two-stage stochastic optimization formulation of the police staffing problem, where staffing decisions are made in the first stage
and dispatch decisions in the second stage. Noticing the special structure of our problem, we provide a customized integer L-shaped method
for our second-stage problem and show its computational advantage compared to the existing methods using 911 call data from the Atlanta
Police Department. We also develop a new formulation for the second-stage problem, which adopts ideas from interval scheduling theory.
5 - Novel Regret Bounds based on Information Relaxations
Lavanya Marla, University of Illinois at Urbana-Champaign, Urbana, IL, United States, Tharunkumar Amirthalingam
We present a novel approach to evaluating policies for large-scale resource allocation problems on networks. Strategic and tactical policies
are often evaluated based on regret bounds, which may be too loose. Our approach is based on the idea of penalized information-relaxation
bounds, proposed earlier by Brown, Smith and Sun (2010). We identify and solve methodological gaps in the existing approach and
demonstrate how these can be successfully operationalized for large-scale decision-making problems on networks. We demonstrate our
methodology on applications involving airline recovery, emergency medical service policies and training autonomous vehicles.
SC38

--------------------------------------------------------------------------------

## AAS Student Presentation Competition I

Room: 430

430
AAS Student Presentation Competition I
Award Session
Aviation Applications
Chair: Wayne Ng, Singapore University of Technology and Design, Singapore, Singapore
1 - Airline competitive Hub-and-Spoke network design by reweighted norm-1
Fernando Real Rojas, King Juan Carlos University, Mostoles, Spain, Luis Cadarso, Antonio Marques
The widespread adoption of hub-and-spoke networks in the airline sector has sparked interest in hub location problems. Our research
introduces a profit-maximizing, flow-based mathematical model tailored for designing airlines' capacitated networks. Unlike traditional
methods assuming a predefined hub-and-spoke structure, we allow hubs to emerge based on cost-effectiveness, passenger preferences, and
airline competition.
Given the competitive nature of the airline industry, we focus on optimizing network design while considering passenger behavior. Discrete
choice models capture factors influencing passenger decisions, such as route price and travel time, integrating supply and demand
interactions. These models, like the logit model, are nonlinear and nonconvex, posing solver development challenges. We address this with a
reverse engineering approach, adding convex negative entropy passenger flow regularizers, leading to logit-based solutions.
Our formulation jointly considers key decision variables like airport base locations, route offerings, and associated capacities. This
complicates the optimization but integrates network infrastructure design with passenger choices and market shares. To solve this, we propose
a novel approach using convex relaxation and reweighted norm-1 minimization, combining decision and dimensioning network infrastructure
variables, and considering operating costs and passenger decisions.
A strategic element of our approach is tailored flow conservation constraints that only allow transfers at hubs, crucial for yielding hub-and-
spoke topologies and modeling passenger choices. Our method optimizes hub-and-spoke network design considering profitability, demand
forecasts, passenger choices, and competitor presence. We demonstrate effectiveness with a case study of a new domestic airline across 25
U.S. airports, validating the methodology and offering insights into the hub-and-spoke design.
2 - Award Session Speaker
3 - Award Session Speaker
4 - Learning for Collaborative Aircraft Trajectory Optimization in Airspace
5 - Accelerated Column Generation approach for Tail Assignment Problem
SC39

--------------------------------------------------------------------------------

## Railway Timetabling and Demand Forecasting

Room: 431

431
Railway Timetabling and Demand Forecasting
Invited Session
Railway Applications
Chair: Pengli Mo, N/A, United States
Co-Chair: Marcella Samà, Roma Tre University, Roma, Italy
1 - Delay risk evaluation in real-time train rescheduling under uncertain dwell times
Marcella Samà, Roma Tre University, Roma, Italy, Marco Pranzo, Carlo Meloni
https://submissions.mirasmart.com/InformsAnnual2024/Publication/ProgramBook.aspx 175/127610/18/24, 2:13 PM Program Book
Given a train timetable made infeasible by unexpected disturbances, the real-time Railway Traffic Management Problem (rtRTMP) returns a
new feasible plan of operations in which corrective decisions have been taken to minimize delay propagation. The rtRTMP is a very well-
studied problem in the literature. Usually, given the necessity to quickly return a good quality solution, the problem is solved considering
deterministic passengers dwell time, disregarding how uncertainty may affect the new constructed plans. Such simplification of the problem
may lead to unexpected worsening of the performance promised by an rtRTMP solver, which in turn may negatively impact the confidence of
dispatchers on such tools. A clearer picture of the actual rtRTMP solution quality, especially in case of worst-case scenarios realizations,
would be preferable, to better allow dispatchers to take more informed decisions. In this talk we present an approach to assess the risk of
worsening delays associated to an rtRTMP solution when dwell times are uncertain. We explore the evaluation of the Conditional-Value-at-
Risk of the maximum train delay as risk index. We model an rtRTMP solution as a temporal activity network and apply an innovative
numerical method to obtain a real-time risk evaluation for rtRTMP solutions. We test the proposed approach on a real case study analyzing
the effects of uncertainties of different severity. Different computational schemes are presented, testing and analyzing their suitability for
practical contexts requiring risk-averse or risk-aware scheduling processes. The methodology appears to be promising to deal with the
different risk attitudes of the decision-makers.
2 - A Fragility-based Approach to Timetable Design
Marta Leonina Tessitore, Roma Tre University, Rome, Italy, Marcella Samà, Giorgio Sartor, CARLO MANNINO, Dario Pacciarelli
In a typical tactical timetabling process, route planners are usually tasked to produce a timetable for the next months or year. They start from
an existing timetable and follow a time-consuming trial-and-error process to obtain a new timetable. Given the increase in traffic demand,
public service companies require the implementation of effective decision support systems that can guide practitioners throughout their
decision-making process, regardless of their experience level. In this work, we present a fragility-based approach to timetable design.
Specifically, we show how the concept of fragility can be easily exploited by route planners to design new and more robust timetables, i.e., to
suggest where to add new time supplements or redistribute the available ones. We propose a timetabling model that aims at building a more
robust timetable, modifying the existing timetable, and reducing the fragility of its most critical sections, that is a single large model capable
of simultaneously doing timetabling and dispatching. We solve this model using a delayed row generation algorithm similar to the one first
described in Lamorgese and Mannino (2015). Each iteration of the algorithm produces a new timetable that may contain different fragile
sections. The process can be iterated until the desired level of robustness is achieved. Considering real-life scenarios from a Norwegian
railway line, we show that we can significantly improve the fragility of a timetable, even when considering conservative strategies for
possible improvements.
3 - Collaborative Optimization of Rolling Stock Allocation and Timetable Coordination in a Multi-Modal Rail Network: MILP
Formulation and Decomposition-Based Algorithm
Jiateng Yin, Beijing Jiaotong University, Beijing, China, People's Republic of
The transportation system is shifting towards a shared mobility ecosystem, emphasizing resource aggregation and coordination among
different transport modes to enhance service quality and encourage passengers to shift from private cars to public transit. This study focuses
on the coordination between metro and rail networks in a city, to alleviate congestion at transfer hubs and to decrease the travel time of
passengers. Given the “tide-like” transfer passengers between rail and metro hubs, we investigate the potential of reserving a number of
rolling stocks, strategically allocated to the metro trains with the highest demand to avoid over-congestion. We present a mixed integer linear
programming (MILP) model to formulate this problem, with decision variables including rolling stock allocation and coordinated schedules
for both metro and rail networks, by considering the time-varying demand of passengers. The objectives aim to minimize the passenger
travel/transfer time and operational costs for managers. Then, we analyze the mathematical properties and propose an exact decomposition-
based solution algorithm. Our algorithm reformulates the original problem into a relaxed master problem (corresponding to rolling stock
allocation and timetabling) and a series of independent subproblems (corresponding to passenger flows). We provide optimality conditions
and prove that the subproblems can be solved by an analytical procedure. According to these properties, we further propose a set of feasibility
cuts and we prove that the new cuts provide tighter bounds in comparison with traditional Benders cuts. We test our integrated approach and
solution algorithms on real-world instances from the Beijing railway network.
4 - Passenger-oriented rolling stock scheduling in the metro system with multiple depots: Network flow based approaches
Entai Wang, HEC Montreal, Montreal, QC, Canada, Lixing Yang, Jiateng Yin
This study investigates a rolling stock scheduling problem on a metro line with multiple depots. Two novel optimization models, i.e., an arc-
based and a path-based network-flow models, are formulated with the aim of improving the service level and reducing the operation cost
simultaneously, in which the flexible train composition mode is also taken into consideration to well match the transport capacity and time-
varying passenger demand. To solve the proposed models, a branch-and-price (B&P) approach is designed to find the near optimal operation
schemes, in which the column generation is used to solve the relaxed problem at each node of the searching tree, where a dynamic
programming approach is embedded to solve the pricing sub-problem associated with each depot to generate promising paths (columns) for
each rolling stock unit, and then the branch-and-bound (B&B) procedure is incorporated to find integral solutions. To test the performance of
the proposed approaches, a series of numerical experiments are conducted both on small-scale and real-life cases of the Beijing metro Batong
line with historically recorded passenger data. The computation results have verified the improved operational efficiency and a better service
level of the solutions found by our proposed approaches.
SC40

--------------------------------------------------------------------------------

## Causal Inference & ML

Room: 433

433
Causal Inference & ML
Invited Session
Applied Probability Society
Chair: Kelly Zhang, Columbia Business School (DRO), New York, NY, United States
1 - On Experiment Design Under Network Interference
Mohsen Bayati, Stanford University, Standford, CA, United States, Yuwei Luo, William Overman, Sadegh Shirani, Ruoxuan Xiong
Randomized experiments are a powerful methodology for data-driven evaluation of decisions or interventions. Yet, their validity may be
undermined by network interference. This occurs when the treatment of one unit impacts not only its outcome but also that of connected
units, biasing traditional treatment effect estimations. This talk discusses a new framework to accommodate complex and unknown network
interference and produce practical estimators.
2 - Minimax Optimal Estimates of Individual Causal Effects in Panel Data under General Intervention Patterns
Christina Yu, Cornell University, Ithaca, NY, United States, Yudong Chen, Xumei Xu
Consider estimating individual causal effects of a treatment on an individual i at time t from panel data, where we observe the outcomes of
multiple units across a period of time. Each unit may be exposed to the treatment at different time points across the horizon. Assuming the
two-way linear fixed effects model, for any general intervention pattern, we present tight conditions for identification of individual causal
https://submissions.mirasmart.com/InformsAnnual2024/Publication/ProgramBook.aspx 177/127610/18/24, 2:13 PM Program Book
effects along with a simple unbiased estimator. The estimator is both minimax optimal and is a uniform minimum variance unbiased estimator
when the observation noise is independent from the treatment variables. In particular, the minimax achievable squared error for recovery of
the causal effect associated to the (i,t) unit-time pair is proportional to the effective resistance of (i,t) in the bipartite graph associated to the
intervention pattern, and the estimator is derived from the unit (i,t) electrical flow on that graph. We show that our estimator can be viewed as
a generalization of the difference in differences estimator, and our results provide insights for understanding two-way fixed effects regression
estimators.
3 - Adaptive Experimentation at Scale: A Mathematical Programming Approach
Hongseok Namkoong, Columbia University, New York, NY, United States, Ethan Che
Adaptive experimentation can improve statistical power significantly, but typical algorithms overlook important issues that arise in practice:
multiple objectives, non-stationarity, batched/delayed feedback, constraints, and personalization. Moving away from developing bespoke
algorithms for each setting, we present a mathematical programming view of adaptive experimentation that can flexibly incorporate a wide
range of objectives, constraints, and statistical procedures. By formulating a dynamic program in the batched limit, our modeling framework
enables the use of scalable optimization methods (e.g., SGD and auto-differentiation) to solve for treatment allocations. To spur algorithmic
progress, we build a suite of benchmark problems based on hundreds of real A/B tests at ASOS that model key practical issues such as non-
stationarity, personalization, multi-objectives, and constraints. Our empirical results show standard Thompson sampling-based policies fail to
reliably improve upon static designs, and demonstrate the effectiveness of a simple planning approach.
4 - Orthogonal Estimation of the Difference-of-Q-Functions
Angela Zhou, USC Marshall School of Business Data Sciences and Operations, Los Angeles, CA, United States
Offline reinforcement learning is important in many settings with available observational data but the inability to deploy new policies online
due to safety, cost, and other concerns. Many recent advances in causal inference and machine learning target estimation of heterogeneous
treatment effects, and more broadly causal contrast functions. Estimating causal contrasts is sufficient for making optimal decisions and can
adapt to potentially smoother structure. We develop a dynamic generalization of the R-learner for estimating the difference of Q-functions,
Q(s,1)-Q(s,0) (and for multiple actions, fixing a choice of a_0, Q(s,a)-Q(s,a_0). The method wraps around standard estimation procedures in
offline reinforcement learning via a sequence of sequential loss minimization problems, which makes it appealingly practical. But, the
residualized learning step allows for targeting the more structured smoothness of the Q-function contrast itself. We illustrate with relevant
examples, including "decision-theoretic sparsity" of the difference-of-Q function that arises due to joint structure of reward and transitions.
SC42

--------------------------------------------------------------------------------

## Active Learning for Optimization

Room: 436

436
Active Learning for Optimization
Invited Session
Simulation Society
Chair: Giulia Pedrielli, Arizona State University, Tempe, AZ, United States
1 - Recent Advances in Grey-Box Bayesian Optimization
Raul Astudillo, California Institute of Technology, Pasadena, CA, United States
Bayesian optimization (BO) is a framework for global optimization of expensive-to-evaluate objective functions. Classical BO methods
assume the objective function is a "black box". However, information about the objective function is often available. Recently, "grey-box"
BO methods leveraging such information have been shown to provide dramatic performance improvements in a broad range of problems. In
this talk, I will discuss new advances in grey-box BO and the applications these methods have unlocked.
2 - Fast and Effective Gaussian Process Based Bayesian Optimization Without Maximum Likelihood Estimation
Antonio Candelieri, University of Milano-Bicocca, Milano, Italy
A novel Gaussian Process based Bayesian Optimization framework is presented, in which the modelling of the objective function and the
selection of the new incumbent solution are simultaneously addressed by predicting the most suitable value of the Gaussian Process's kernel
kyperparameter(s), avoding expensive computation of the Maximum Likelihood Estimation. Indeed, the Gaussian Process model is fitted at a
constant time, independently on the number of function evaluations performed so far. At the same time, the prediction of the most suitable
value of the kernel's hyperparameter(s) also avoids instability in the kernel matrix. As a result, the approach can be applied to globally
optimize noise-free functions, without the need to add artificial noise (also known as nugget effect) to avoid ill-conditioning.
Results on a set of diversified test problems empirically show the benefits of the proposed approach, in terms of effectiveness and efficiency,
against the well known no-regret Gaussian Process Lower Confidence Bound. Specifically, the proposed approach provides a lower
cumulative regret until the best observed value can be improved. Otherwise, the regret increases because exploration is intrinsically triggered
by the impossibility to further improve, close to the current best solution, without incurring into ill-conditioning. This leads to a completely
new way to learn the Gaussian Process by simultaneously considering risk for ill-conditioning and exploration-exploitation balance provided
by the next incumbent solution.
3 - Stochastic Linear Bandits with Partial Observability
Gautam Dasarathy, Arizona State University, Tempe, AZ, United States, Vineet Gattani, Lalit Jain
We consider the problem of stochastic linear bandits where the decision vectors available to the agent may only be partially observed. This
models several modern sequential decision-making applications, such as recommendation systems, where it is impossible, impractical, or
even imprudent to fully observe these decision vectors. As one example, the recent migration from third-party cookies means that advertisers
only get limited observability into the user context. In this work, we propose Partially Observed Linear Bandit (POLB), an algorithm for
tackling this problem. Our method sequentially builds an estimate of the (possibly low-dimensional) subspace the decision vectors lie in, and
then leveraging this estimate in a sequential decision making framework. Our theory elucidates the effect of the amount of observability and
the presence of low-dimensional structure underlying the decision vectors on the regret performance of POLB. We complement this with
empirical results that highlight the advantages of the proposed framework.
4 - Respecting the limit: Bayesian optimization with a known bound on the optimal value
Matthias Poloczek, Amazon, San Francisco, CA, United States
Bayesian optimization (BO) has become a powerful method for the sample-efficient optimization of expensive black-box functions that arise
in materials discovery, hardware design, AutoML, or portfolio optimization, for example. These functions do not have a closed-form and are
evaluated by running a complex simulation or an experiment. In many real-world optimization problems, we have prior information about
what values are achievable under the objective function. We study the scenario that we have either exact knowledge of the value of the global
minimum or a, possibly inexact, lower bound on its value. We propose Bound-Aware Bayesian Optimization (BABO) that uses a tailored
surrogate model called SlogGP and a new acquisition criterion based on Expected Improvement to leverage the bound information. Empirical
results on a variety of benchmarks demonstrate the benefit of taking prior information about the optimal value into account. Interestingly, we
notice that even in the absence of such prior information, the new SlogGP surrogate model outperforms the standard GP model in many cases.
Joint work of Hanyang Wang, Juergen Branke (both Warwick Business School, UK), and Matthias Poloczek (Amazon, USA).
5 - Simulation-based digital twins: Identifying optimal alternatives using real-time data
Moones Keshvarinia, Iowa State University, Ames, IA, United States, Cameron MacKenzie
Digital twins have the potential to support decision making in real time in a manufacturing facility. The digital twin should run in parallel
with the manufacturing facility so that the digital twin represents the current state of the manufacturing system. Simulation within the digital
twin can be used to explore different scenarios and forecast potential outcomes in the manufacturing system. When problems arise in the
manufacturing system, the manufacturer can use the digital twin simulation to identify alternatives to resolve these problems while
incorporating real-time data from the manufacturing facility. This study investigates the use of a digital twin to resolve a severe supply
https://submissions.mirasmart.com/InformsAnnual2024/Publication/ProgramBook.aspx 180/127610/18/24, 2:13 PM Program Book
shortage in a manufacturing facility. We use a discrete-event simulation to determine if a digital twin relying on real-time data from the
manufacturing facility generates a different set of optimal alternatives that mitigate the effects of the supply shortage compared to the optimal
alternatives generated from a static simulation without access to real-time data. The simulation models different operational alternatives such
as adding more forklifts, employing condition-based maintenance, hiring more workers, and moving workers from one station to another
station. Machine learning algorithms identify the combination of alternatives that maximizes the manufacturer’s expected profit using data
generated by the simulation. The findings from this research quantify the potential benefit of digital twins in manufacturing for the purpose of
managing disruptions.
SC45

--------------------------------------------------------------------------------

## Decision Making in Healthcare

Room: 439

439
Decision Making in Healthcare
Invited Session
Health Applications Society
Chair: Oguzhan Alagoz, University of Wisconsin-Madison, Madison, WI, United States
Co-Chair: Yifan Lu, University of Wisconsin-Madison, Madison, 53706, United States
1 - A Transfer Reinforcement Learning Approach for Precision Follow-up Colonoscopy Recommendation for Colorectal Cancer
Screening
https://submissions.mirasmart.com/InformsAnnual2024/Publication/ProgramBook.aspx 182/127610/18/24, 2:13 PM Program Book
Mu Du, School of Economics and Management, Dalian University of Technology, Dalian, China, People's Republic of, hongtao yu,
Nan Kong
In precision public health (PPH), making well-informed intervention decisions tailored to sub-populations’ disease progressions is
challenging because of the scarcity of relevant data on target sub-populations. This study addresses precision follow-up colonoscopy
recommendations for colorectal cancer prevention in multiple cohorts, whose medical data can only be obtained progressively during their
follow-up visits. Specifically, each sub-population's disease progression is modeled as a Markov decision process (MDP) with unknown
transition parameters and rewards based on a well-established microsimulation colon model. To solve the MDPs, we propose a learning-
while-optimization framework with a transfer reinforcement learning (RL) approach facilitated by a general upper confidence bound value
iteration scheme. We design this transfer RL approach by integrating a model-based RL with a joint estimator, which can combine huge proxy
data historically collected from the entire population and scarce true data progressively obtained from target sub-populations. In addition, we
accelerate the proposed transfer RL approach by modifying its algorithmic procedure to employ short-term planning for value iteration
instead of full planning. We establish a theoretical performance guarantee by proving the regret bound of the proposed transfer RL approach.
We also prove the accelerated transfer RL can significantly decrease the computational complexity without suffering performance
degradation in regret. To the best of our knowledge, there is no regret guarantee for such transfer RL approaches, even in the tabular MDP
settings. By integrating the RL techniques and operations research, our study pioneers an actionable plan for precision follow-up screening.
2 - Optimizing HIV Point-of-care Testing Systems in Kenya Under a Conditional Value at Risk Objective
Yinsheng Wang, University of Washington, Seattle, WA, United States, Shan Liu, Chaoyue Zhao
Minimizing the turnaround time for HIV testing by allocating point-of-care testing machines in a hub-and-spoke network of clinics has been
proven effective in enhancing HIV care. Our previous work has successfully developed practical decision-support tools using queuing theory
for HIV viral load and drug resistance testing in Kisumu County, Kenya. The data from Kisumu County between 2019 and 2023 reveal a
distinct seasonal trend in viral load testing demand, with notable surges each spring. In light of the demand variations, this study extends a
deterministic mixed integer programming model to a two-stage stochastic model. In this framework, the allocation of testing machines is
determined in the first stage, while the referral network from clinics to hubs hosting these machines is optimized in the second stage. We have
developed and solved a queueing-location-allocation model that incorporates a Conditional Value at Risk objective to manage uncertainties
and risks associated with fluctuating testing demands. To ensure computational efficiency, the solution process employs Benders
decomposition and heuristic cuts to expedite convergence. Computational results based on a HIV viral load testing network in Kisumu
County are presented to compare the robust model to its deterministic counterpart and to demonstrate the robustness of the model.
3 - Early-Stage Clinical Trials with Patient Choice
Connor Van Ryn, Clemson University, Clemson, SC, United States, Amin Khademi, Qi Luo
Early-stage clinical trials (CTs) seek to identify safe dosages for new drugs for later trial phases. Clinical investigators (CIs) often face
difficulty in the recruitment and retention of patients during these early stages, which can cause substantial expense and delay to the trial. The
prevailing methods used in CTs are dose-escalation methods, where consecutive groups of patients receive increasing doses until significant
toxicities are seen. Patient dosing is entirely structured and scheduled by the CI, with the patient having no input. This form of trial is highly
conservative in its efficacy and toxicity and gives no agency to patients. This research involves the creation of a new CT design method
which balances CT participant satisfaction and CI information gain by increasing the control patients have while still ensuring that adequate
information about the drug is learned. The model being developed is a psuedo-market mechanism in which patients are attempting to
maximize their expected health gain from different dosages, and CIs are attempting to maximize the information gained about efficiency and
toxicity of the drug across different dosages.
4 - Personalized screening policy based on individual women’s breast cancer risk
Yifan Lu, University of Wisconsin-Madison, Madison, WI, United States, Oguzhan Alagoz, Eugenio Quessep, Rick Groeneweg, John
Hampton, Karla Kerlikowske, Harry Koning, Kathryn Lowry, Diana Miglioretti, Jeanne Mandelblatt , Clyde Schechter, Nicolien
Ravesteyn, Brian Sprague, Natasha Stout, Anna Tosteson, Amy Trentham-Dietz
Most existing breast cancer screening guidelines focus only on average-risk women, which may lead to over-screening of low-risk women
and under-screening of high-risk women. Several successful risk assessment models exist, which provide an individual woman’s risk of
developing breast cancer in the next several years (E.g. 5-year risk), however, their impact on long-term breast cancer outcomes is unknown.
We utilized University of Wisconsin Breast Cancer Simulation Model, a member of National Cancer Institute-funded Cancer Intervention and
Surveillance Modeling Network (CISNET) breast cancer working group, to incorporate risk-based screening based on risk assessment
models. CISNET models have been used by policy makers in 2009, 2016 and 2024 to set the breast cancer screening guidelines in the US. We
developed and evaluated several personalized screening strategies based on an individual’s breast cancer risk to improve the balance of
benefits and harms as well as better allocate the clinical screening resources more efficiently.
SC48

--------------------------------------------------------------------------------

## Machine Learning Applications on Inventory Models

Room: 440

440
Machine Learning Applications on Inventory Models
Invited Session
Manufacturing and Service Operations Management (MSOM)
Chair: Jingkai Huang, Zhejiang University, Hangzhou, N/A
Co-Chair: Kevin Shang, Duke University, Durham, NC, United States
1 - Data-Driven Optimal and Myopic Policies for Inventory Systems with Demand Covariates
Jingkai Huang, Zhejiang University, Hangzhou, China, People's Republic of, Kevin Shang, Yi Yang, Weihua Zhou
https://submissions.mirasmart.com/InformsAnnual2024/Publication/ProgramBook.aspx 183/127610/18/24, 2:13 PM Program Book
We study a multi-period backorder inventory system in which the random demand depends on exogenous covariates. The main goal is to
develop a data-driven inventory policy that minimizes the total expected inventory cost. Based on the optimal policy structure under full
distribution information, we first propose the Data-Driven Optimal (DDO) policy which utilizes a ``first predict, then optimize the residual-
based empirical dynamic programming" framework, and provide its finite-sample performance bound. However, the DDO policy may suffer
the curse of dimensionality, especially when the sample size is large. We then propose the simple myopic policy with covariates. With known
distribution information, we characterize its optimality conditions and optimality ratio. With unknown demand distribution, we introduce the
Data-Driven Myopic (DDM) policy and provide its finite-sample performance bound. Interestingly, we can show that the DDM policy
performs the same as the DDO policy under a small sample size. We further propose another simple policy, called semi-myopic policy which
takes the future costs into account, while maintaining a simple structure. The idea is to approximate cost-to-go functions by those from the
classic multi-period inventory system without covariates which can be easily solved. We prove that the optimality condition of semi-myopic
policy is looser than that of myopic policy. Accordingly, we also propose Data-Driven Semi-Myopic (DDSM) policy for the unknown
demand case. Finally, we extend the main results to the scenario with correlated demand covariates. Numerical studies demonstrate that both
heuristic policies perform well, while the semi-myopic policy performs better.
2 - Online Learning for Dual Index Policies in Dual Sourcing Systems
Jingwen Tang, University of Miami, Coral Gables, FL, United States, Boxiao Chen, Cong Shi
We consider a periodic-review dual-sourcing inventory system with a regular source (lower unit cost but longer lead time) and an expedited
source (shorter lead time but higher unit cost), under carried-over supply and backlogged demand. Unlike existing literature, we assume that
the firm does not have access to the demand distribution a priori and relies solely on past demand realizations. Even with complete
information on the demand distribution, it is well-known in the literature that the optimal inventory replenishment policy is complex and
state-dependent. Therefore, we focus our attention on a class of popular, easy-to-implement, and near-optimal heuristic policies called the
dual-index policy. The performance measure is the regret, defined as the cost difference of any feasible learning algorithm against the full-
information optimal dual-index policy. We develop a nonparametric online learning algorithm that admits a regret upper bound of O(\sqrt{T
log T}), which matches the regret lower bound for any feasible learning algorithms up to a logarithmic factor. Our algorithm integrates
stochastic bandits and sample average approximation techniques in an innovative way. As part of our regret analysis, we explicitly prove that
the underlying Markov chain is ergodic and converges to its steady state exponentially fast via coupling arguments, which could be of
independent interest. Our work provides practitioners with an easy-to-implement, robust, and provably-good online decision support system
for managing a dual-sourcing inventory system.
3 - Dynamic Pricing with Infrequent Inventory Replenishments
Boxiao Chen, University of Illinois Chicago, Chicago, IL, United States, Menglong Li, David Simchi-Levi
We consider a joint pricing and inventory control problem where pricing can be adjusted more frequently than inventory ordering decisions.
We consider the situation where the demand-price function and the distribution of random demand noise are both unknown to the retailer. We
propose an online learning algorithm that achieves the optimal convergence rate to the true optimal solution.
4 - Multi-Period Newsvendor Optimization for Capacity Planning
Gah-Yi Ban, Imperial College Business School, London, United Kingdom, Abhilasha Katariya, Chinmoy Mohapatra, Liron Yedidsion
Capacity planning in Amazon Logistics is inherently a multi-period decision problem, whereby decisions are made and adjusted weeks in
advance of deliveries. We formulate and solve a multi-period newsvendor optimization model for Amazon Logistics capacity planning. Since
the general multi-period problem is computationally intractable, we propose four computationally efficient heuristic solutions: (i) a solution
equivalent to the single-period newsvendor solution (NV), which is reflective of the current state-of-the-art, (ii) an extension of the
newsvendor solution that takes ramp-up and down costs into account (NV+), (iii) a linear decision rule-based solution (LDR) and (iv) a
hybrid of NV+ and LDR solution to solve the problem numerically. Our numerical results show that the multi-period solutions (LDR and
Hybrid) outperform the single-stage solutions (NV and NV+) when the demand model exhibits non-stationarity and dependence over time,
which can translate to substantial cost savings.
5 - Two-Sided Pricing and Learning with Inventory Constraints
Meichun Lin, Singapore Management University, Singapore, Singapore, Tim Huh
Motivated by online used-car platforms, we study pricing decisions for purchasing and selling a product in a two-sided market. With
uncertainty from both supply and demand, a platform sequentially adjusts purchase and selling prices to maximize profit while satisfying
inventory constraints. Moreover, the platform does not know in advance how supply and demand depend on the prices. Our work investigates
how the platform can manage demand and supply in the presence of the two sides of uncertainty and inventory constraints. When the demand
and supply functions are known, we show that simple fixed-price policies can be implemented with a small performance loss. Given limited
information on supply and demand, we propose pricing algorithms that achieve the best possible regret when the planning horizon is large,
which sheds light on how to balance the trade-off between two sides of learning and profit maximization (earning).
SC49

--------------------------------------------------------------------------------

## Sustainability and Technology

Room: 443

443
Sustainability and Technology
Invited Session
MSOM: iForm
Chair: Rowena Gan, Cox School of Business, Southern Methodist University, Dallas, TX, United States
Co-Chair: Rong Li, Syracuse University, Syracuse, NY, United States
1 - Contract Tokenization in the Renewable Energy Market
Rowena Gan, Southern Methodist University, Dallas, TX, United States
Endorsed by the blockchain technology, supply chain contracts can be digitally recorded and stored in crypto tokens, which is referred to as
being tokenized. Tokenized contracts offer new ways of financing, trading and owning an asset. Using the renewable energy market as a
backdrop, we study the impact of contract tokenization on different parties in the supply chain based on their respective incentives.
2 - Delivery Terms for Voluntary Carbon Offsets
Safak Yucel, Georgetown University, Washington, DC, United States, Vishal Agrawal, Gokce Esenduran
In addition to abating emissions, corporations purchase carbon offsets for voluntary decarbonization targets, such as reaching net-zero
emissions. A carbon offset represents emissions reduction achieved by a developer through investments in projects, such as forestry and
renewable energy. Corporations purchase offsets under two delivery terms. The first is forward delivery, where the buyer orders a certain
quantity before the seller undertakes the investment and yield uncertainty realizes. The second is prompt delivery, where the seller invests and
uncertainty realizes, before the buyer orders offsets. Motivated by the importance of choosing the right delivery term for buyers and sellers, in
this paper, we investigate economic and environmental implications of these delivery terms. Our results offer several managerial insights:
First, a seller should prefer forward delivery if investment cost is low, but prompt delivery otherwise. Although one would expect that the
delivery term that enables the seller to make a higher investment results in a higher profit, we find that prompt delivery may lead to a higher
profit despite lower investment. Second, a buyer should prefer forward delivery only for a project with either a low or high investment cost.
https://submissions.mirasmart.com/InformsAnnual2024/Publication/ProgramBook.aspx 186/127610/18/24, 2:13 PM Program Book
Interestingly, the delivery term that leads to a lower cost for the buyer may actually require higher abatement. Finally, environmental groups
should promote forward delivery only for a project with a low investment cost. Moreover, when the buyer and seller prefer different delivery
terms, the one preferred by the seller leads to greater emissions reduction, and should be promoted by environmental groups.
3 - Integrated Planning of Power System and Hydrogen Supply Chain
Siqiang Guo, University of Missouri-St. Louis, St. Louis, MO, United States, Erhan Kutanoglu, Shadi Goodarzi
Hydrogen is important as it can serve as a carrier of clean energy, providing decarbonization solutions to sectors that are highly carbon-
intensive and hard to electrify (e.g., steelmaking). As the proportion of renewable energy in the power system continues to grow, hydrogen's
role in mitigating the energy supply and demand discrepancies (both temporally and spatially) will become even more crucial. However, the
hydrogen economy currently faces many challenges, such as the high cost of green hydrogen and the lack of adequate infrastructure. The
diversity of the power system and hydrogen supply chain (in terms of energy/hydrogen production, transportation, and storage) and the close
correlation between them complicate the expansion of both systems. This complexity is intensified by the potential influence of various
environmental policies such as carbon allowance, carbon tax, and clean hydrogen subsidy. We provide a model for the joint optimization of
the power system and hydrogen supply chain, and we delve into the aforementioned environmental policies, examining their impact on the
entire system. Our detailed analysis reveals the rationales of different policies and yields several efficient policy combinations that can
achieve significant carbon reduction (50 - 80%), maintain corporations' decision-making flexibility, ensure a low decarbonization cost for
both corporations and society (< $65/ton of CO2), while not adding significant burdens to corporations (< 3% cost increase).
4 - Collective Power Purchase Agreement Negotiation and Risk Sharing
Xiaoxuan Hou, University of Washington, Seattle, WA, United States, Shi Chen
Over 400 companies spanning more than 175 markets worldwide have committed to 100% renewable electricity. But achieving these goals
can be challenging. In the past decade, long-term power purchase agreement (PPA) has gained popularity among large corporations like
Google and Amazon (buyers) as the main vehicle to source supply from renewable energy developers (the seller). Under a PPA, the buyer
commits to purchasing the renewable energy from the seller at a fixed price for a predetermined length of time (usually 10 to 15 years). Yet
signing a PPA remains difficult for small-to-medium-sized corporate buyers, who may be too small to sponsor a full project, not have the
expertise, or lack sufficient negotiation power on their own. As a result, collective PPAs emerged as a potential solution. We build Nash
bargaining-based economic models to compare the performance of three negotiation mechanisms for these small-to-medium buyers to sign
PPA contracts collectively: separate negotiation, agent-based negotiation, and joint negotiation. Our analysis reveals the impact of different
mechanisms on the renewable project size in equilibrium and different players' preference. Assuming risk neutrality, our findings indicate that
the three negotiation mechanisms lead to identical equilibrium project sizes, thus contributing equally to adding renewable capacity to the
grid. Yet conflicts of preference for different negotiation mechanisms exists among parties, and rising number of buyers simplifies the
consensus-building process for adopting joint negotiations. Further, buyers' perception of risks due to the uncertainty of the future market
prices can harm the achievement of the sustainable goals.
SC52

--------------------------------------------------------------------------------

## Operations Research and Machine Learning for Maternal Health

Room: Terrace Suite 1

Terrace Suite 1
Operations Research and Machine Learning for Maternal Health
Invited Session
Health Applications Society
Chair: Meghan Meredith, Georgia Institute of Technology, Atlanta, United States
Co-Chair: Lauren Steimle, Georgia Tech ISyE, Atlanta, GA, United States
1 - A Stochastic Programming Approach for Patient Acceptance Decision-Making in Prenatal Care
Leena Ghrayeb, University of Michigan, Ann Arbor, MI, United States, Amy Cohn, Ruiwei Jiang, Alex Peahl
In current practice, depending on physician and clinic-related capacity constraints, prenatal care clinics define the number of patients they can
accept by the number of patients that each physician can care for at any point in time. Once a patient is accepted to the clinic, they are
considered under the provider’s care for the entirety of their pregnancy. While this method is simple and provides clear guidelines, it does not
account for the stochastic nature of patients’ health states – patients may require additional appointments beyond their defined pathways due
to complications during pregnancy. This can result in unexpected overutilization, overbooking, and poor scheduling practices, which are
burdensome for clinics and can hinder access to care for patients. We propose a multi-stage stochastic programming approach to aid clinics in
deciding how many patients to accept on a weekly basis, given patient-related randomness inherent in pregnancy. We derive upper and lower
bounds for this model and draw insights about patient acceptance policies.
2 - Identify Medically Unnecessary Cesarean Deliveries
Emily Fainman, Texas State University, San Marcos, TX, United States, Beste Kucukyazici, TING WU
We first use semi-supervised learning methods to classify pregnancy women into low- and high- risk groups based on approximately 18
million individual birth records in the U.S. from 2016 to 2021.
We develop an innovative semi-supervising fuzzy clustering (S-FCMd) algorithm by introducing a distance measure for mixed data
consisting of numerical and categorical variables. We develop an innovative semi-supervising fuzzy clustering (S-FCMd) algorithm by
introducing a distance measure for mixed data consisting of numerical and categorical variables. The labelled observations incorporate prior
medical knowledge and effectively speed up the convergence. The method of updating cluster medoid ensures a computational complexity
proportional to the sample size. Our experiments on real-life and synthetic data demonstrate the efficiency of our algorithm for large datasets.
Then, we validate the optimal delivery methods for two risk groups through post-delivery variables in the dataset. We perform statistical
inferences to compare actual post-delivery complications resulting from natural births and C-Sections in each cluster, and thus figure out the
optimal delivery mode for each patient group.
3 - The Role of Hospital Occupancy on Maternal Health Disparities
Rachna Shah, University of Minnesota, Minneapolis, MN, United States, Alison Murphy
Racial disparities in maternal health are well documented, with Black women 2.5 times more likely than non-Hispanic white women to
experience a major complication or death. Unfortunately, both the overall rate of negative outcomes and the racial disparities are increasing.
Social determinants of health (e.g., income, housing, and access to medical care) and structural racism are significant contributors to racial
disparities. For example, perceived racism among Black women is associated with pre-term birth and racial disparities in infant health are
higher in counties with higher implicit or explicit racial bias. In this study, we (1) Determine if the effects of provider workload are moderated
by patient race. We ask if Black women are more impacted by workload than white women in the context of maternal health; (2) Determine if
the above relationship is stronger in areas which higher level of implicit or explicit bias compared to areas with lower bias, and (3)
Demonstrate the mechanism that we theorize to link workload with racial health disparities.
https://submissions.mirasmart.com/InformsAnnual2024/Publication/ProgramBook.aspx 191/127610/18/24, 2:13 PM Program Book
4 - Preventing Rural Obstetric Hospital Closures: a Multi-Level Optimization Approach
Meghan Meredith, Georgia Institute of Technology, Atlanta, GA, United States
Rural residents experience high rates of maternal mortality and travel far distances for obstetric care as rural obstetric hospitals are closing at
an increasing rate. Rural hospital administrators are faced with challenging decisions to close their hospitals as they confront patient safety
and financial viability concerns. In response to these closures, the US government has initiated many programs to prevent rural hospital
closures and maintain access to high-quality obstetric care. We propose a multi-level optimization modeling approach to determine the
optimal allocation of rural hospital closure prevention resources to maximize access to obstetric care. We incorporate multiple aspects of
access, including quality and risk-appropriateness. A case study of the state of Georgia is presented to illustrate geographic access to obstetric
care and the impact of optimally allocating resources.
SC58

--------------------------------------------------------------------------------

## Analytics for Social Good: Public Sector Operations

Room: Terrace Suite 2

Terrace Suite 2
Analytics for Social Good: Public Sector Operations
Invited Session
Health Applications Society
Chair: Xiaoquan Gao, Purdue University, West Lafayette, IN, United States
1 - Achieving Rawlsian Justice in Food Rescue
Gerdus Benade, Boston University, Boston, MA, United States, Aydin Alptekinoglu
We explore a practical idea to achieve more fair outcomes in food rescue platforms that operate on a first-come-first-serve basis: Give priority
to a select set of recipients – first dibs in claiming a food item – for a limited time window. We develop a general model of priority lists and
show several structural results that characterize the optimal priority list for a given donation. The objective is to maximize the benefit
(defined as pounds of food received) gained by the worst-off recipient on the platform, which is a Rawlsian notion of fairness. The model
captures priority lists in their most general form: For each donation, it sets a release time for each potential recipient after which they will be
notified of the donation. It can also accommodate perishability of food donations. Intuitively, our structural results show that it's optimal (in a
max-min sense) to give higher priority to recipients that have received less than others in the past (so, they are currently among the worst-off)
and to recipients that are slower than others in responding to posted donations. A simple index combines these two characteristics of
recipients and provides the optimal priority ordering. We then construct an efficient algorithm to compute the optimal priority list for a given
donation, which (in its most general form) boils down to setting notification times for each recipient. Finally, we present counterfactuals that
quantify the potential impact of implementing this algorithm in practice by using historical data from our partner platform’s Florida
operations.
2 - Volunteer Management
Xinyuan Zhang, UNIVERSITY OF NOTRE DAME, South Bend, IN, United States, Eunae Yoo, Alfonso Pedraza-Martinez
Online volunteering platforms allow individuals to contribute to humanitarian projects
remotely. This paper investigates user behavior in online volunteering platforms integrated
within larger online communities with broader purposes. We study volunteers shifting between
humanitarian and non-humanitarian projects, and its implications on volunteer retention.
3 - Enhancing Support for Survivors of Abuse Through Intelligent Volunteer Management
Rachel Wong, University of Toronto, Toronto, ON, Canada, Sheng Liu, Timothy Chan
In Canada, at least 1 in 5 women experience some form of abuse from a spousal partner with the economic repercussions of intimate partner
violence (IPV) reaching an annual sum of $7.2 billion. Survivors risk losing everything they own when leaving an abusive relationship since
moving and storing one's belongings is challenging due to financial, legal, and logistical barriers While shelters provide a temporary refuge,
they seldom can assist with retrieval and/or storage of survivor's belongings. Shelter Movers stands out as the sole organization in Canada
addressing this vital need. The organization uniquely offers free, survivor-focused moving and storage services, bridging a crucial service gap
for those in need. Like many other non-profit organizations, they call on volunteers to provide essential services. As the demand surges,
Shelter Movers face growing challenges in engaging and retaining volunteers to ensure a fast turnaround time for urgent service requests.
Notably, fewer than five percent of the active volunteers in the pool regularly participate in move requests. The high volatility of service
demand and volunteer availability further complicates the retention challenge. This work proposes an optimal model to match volunteers to
‘move teams’ considering underlying volunteer behavior based on past experiences. We utilize this matching and behavioral model in a
multiclass queuing model to better understand system dynamics and make managerial recommendations for how Shelter Movers can better
engage volunteers and improve volunteer retention.
4 - Keep Water Flowing: The Hidden Crisis of Rural Water Management
Chengcheng Zhai, University of Notre Dame, Notre Dame, IN, United States, Rodney Parker, Kurt Bretthauer, Alfonso Pedraza-
Martinez, Jorge Mejia
In rural areas of sub-Saharan Africa (SSA), people rely on communal handpumps for clean drinking water. But these handpumps break down
frequently. Thus, it is crucial to proactively maintain and reactively repair these handpumps to ensure continuous access to water. In this
research, we study the optimal design of a water point maintenance program implemented by NGOs. We first conducted field research in
Ethiopia and Malawi to better understand the context of water point maintenance. We then collected 56,344 water point functionality
observations from NGOs implementing water point maintenance programs in the Central African Republic, Ethiopia, and Malawi. Lastly, we
develop a Markov decision process that determines the optimal schedule for NGO mechanics to visit water points. We apply the optimization
model to data from the three countries to identify the gap between practice and optimality, while exercising two heuristic policies (cyclic and
responsive) observed in practice.
https://submissions.mirasmart.com/InformsAnnual2024/Publication/ProgramBook.aspx 192/127610/18/24, 2:13 PM Program Book
SC59

--------------------------------------------------------------------------------

## Transportation and Network Optimization at Amazon

Room: Ballroom 2

Ballroom 2
Transportation and Network Optimization at Amazon
Invited Session
The Practice Section of INFORMS
Chair: Nicholas Kullman, Amazon, Bellevue, WA, United States
Co-Chair: Meltem Ozmadenci, Amazon, Seattle, WA, United States
1 - Optimizing Amazon's Network Flow Plan: A Decomposition Approach in Balancing Truck Fill Rates and Labor Utilization
Hyemin Jeon, Amazon.com, Bellevue, WA, United States, Theodoros Pantelidis, Martin Bagaram
Amazon’s transportation network consists of different components whose operational plan rely on multiple layers of analytical tools to
forecast, simulate, and optimize. As obtaining a solution that satisfies all business objectives concurrently is unrealistic over a large complex
system such as Amazon’s, many decisions are made isolated without the full awareness of other components. In this work, we introduce a
coherent volume plan in order to maximize utilization of outbound truck capacity and labor supply on downstream processing sites
concurrently. However, developing a mathematical formulation that captures all necessary dynamics between origin-destination pairs, i.e.
fulfillment centers, sort centers and delivery stations has proven to be computationally intractable. Thus, we seek to decompose this complex
problem into two components, one which contains the fulfillment center outbound network subgraph and aims to maximize truck fill rate and
the other one that maximizes sort centers and delivery stations labor utilization. In this manner, each problem solves for a subgraph of the
network and objective while remaining in parity via the propagation of minimum volume targets on fulfillment center outbound trucks
generated by the second optimization problem that restrict the first problem solution. Hence, we approximate the global optimization solution
for the entire network.
2 - COPPER: Contract Price Optimization Using Willingness-to-Pay Models for Amazon Freight
Yebin Tao, Amazon, San Mateo, CA, United States, Juan Xu, Roger Lederman
We develop COPPER, contract price optimization using execution and award models for willingness-to-pay (WTP), to replace the current
manual and rule-based pricing mechanism for Amazon Freight (AF) contract. To capture shippers’ WTP, we first build an award model to
predict the probability of awarding a contract at a certain price, and then build an execution model to predict the number of executed loads
given a contract awarded at a certain price. To estimate the optimal prices, we first consider each lane separately using simple grid search
over a list of candidate prices, denoted as single-lane COPPER. As an enhancement, we further explore portfolio optimization by jointly
considering lanes within the same portfolio (e.g., same pickup location), denoted as portfolio COPPER. We utilize coordinate descent to solve
the high-dimensional portfolio price optimization efficiently. We conduct comprehensive model comparisons to build the WTP models and
test the price optimization using the AF contracts that we bid on in 2023. From the test, single-lane COPPER shows significant improvements
over the historical manual pricing with an estimated revenue increase of 13%, while portfolio COPPER would further increase revenue by
another 10%.
3 - Air Cargo Revenue Management: A Two-Stage Stochastic Program with Learning-based Recourse
Yan Zhang, Amazon, Seattle, WA, United States, Nilay Noyan, Roger Lederman
In a logistics system, the space in trucks or airplanes unfilled by existing loads is considered as opportunity loss. To mitigate such loss, it is
beneficial to find additional shippers who are willing to pay to use these space. However, finding such shippers could incur risk of displacing
existing loads, due to the uncertain knowledge of the amount of unfilled space in the network. When displacement happens, existing loads are
re-routed by a subsequent system at different displacement costs. To optimize the total revenue realized by the logistic system, it is critical to
predict such displacement cost rendered by the subsequent routing system so that we can price shippers properly. To solve this problem, in
this talk, we present a model-based machine learning approach to learn the routing decisions of the black-box system. Then, we incorporate
the learned module into a two-stage stochastic programming problem, and solve the formulated problem using decomposition framework.
The model estimates the value of the unfilled space in the logistic network, which will be used to set the price for shippers and to guardrail
the revenue of the network.
4 - Accelerating Deliveries: Decomposition-based Approaches for Large-Scale Air Network Optimization
Tulio Toffolo, Amazon, Bellevue, WA, United States, Haroldo Santos, Valentina Vaca, Ruilin Ouyang, Wendian Wan, Na An
The Amazon Air Network Design (A2ND) tool is a patented model that optimizes flight schedule and package flow problems for Amazon
Air. In recent years, features have been added to the model to incorporate optimization for package delivery speed, while maximizing aircraft
utilization and minimizing operating cost. Over 70 types of operational constraints and a configurable objective function are used to explore
network designs and perform trade-off analyses. As Amazon Air grew so did the size and complexity of the optimization model and
computational performance became a bottleneck. This talk discusses the decomposition-based approaches developed to improve the
https://submissions.mirasmart.com/InformsAnnual2024/Publication/ProgramBook.aspx 194/127610/18/24, 2:13 PM Program Book
scalability of the solver and the quality of the network solutions, including model generation and dynamic flight-set population.
Computational experiments show a runtime reduction of more than 50%.
SC61

--------------------------------------------------------------------------------

## Nonconvex, Nonsmooth, and Nonregular Optimization: A Computational Framework

Room: Signature Room

Signature Room
Nonconvex, Nonsmooth, and Nonregular Optimization: A Computational Framework
Invited Session
TutORial
Chair: Jamol Pender, Cornell University, Ithaca, NY, United States
1 - Nonconvex, Nonsmooth, and Nonregular Optimization: A Computational Framework
Michael Ferris, University of Wisconsin, Madison, WI, United States, Olivier Huber, Johannes Royset
Algebraic modeling languages presently lack the ability to effectively support the
formulation and solution of nonconvex and nonsmooth optimization problems. Since
an arbitrary problem of this kind is intractable, any hope to achieve practically useful
solutions would rely on means to convey specific problem structure to an algorithm.
In this tutorial, we present a framework for specifying nonconvex, nonsmooth, and
nonregular problems within an algebraic modeling language that makes available the
key structural properties to an algorithm. It also facilitates experimentation with
different model formulations and algorithmic approaches. The framework entails a
change of mindset away from the traditional formulation of objective and constraint
functions, and instead asks the analyst to specify a basic feasible set, a basic objective
function, one or more monitoring functions, and several performance functions. Eleven
examples ranging from goal programming to variational inequalities and engineering
risk analysis illustrate the practical implications of the framework.
SC63
Regency - 601
Innovative Application of AI in Information Systems Research
Invited Session
Information Systems
Chair: Ali Tosyali, Rochester Institute of Technology, Rochester, NY, United States
Co-Chair: Jeongsub Choi, West Virginia University, Morgantown, WV, United States
1 - An Explainable Ai Framework for Identifying Deceptive Reviewers in E-Commerce Platforms
Ali Tosyali, Rochester Institute of Technology, Rochester, NY, United States
https://submissions.mirasmart.com/InformsAnnual2024/Publication/ProgramBook.aspx 195/127610/18/24, 2:13 PM Program Book
This study addresses the pressing issue of fake reviews on e-commerce platforms, which significantly impact consumer choices and threaten
the credibility of online marketplaces. Recognizing the limitations of current methods focused on detecting individual fraudulent reviews, our
research introduces a novel approach by examining the broader network of review activities. Leveraging insights from recent findings, we
aim to identify and analyze the behavior of reviewers involved in fake review schemes. By using an advanced AI framework that integrates
textual analysis, metadata, and network structure, we explore the characteristics that distinguish genuine reviewers from fraudulent ones. This
comprehensive methodology not only enhances our ability to detect deceptive practices but also sheds light on the underlying mechanisms of
online review manipulation, offering new avenues for improving the transparency and trustworthiness of online review ecosystems.
2 - Xscanner An Explainable Ai Approach for Comprehensive Screening and Pattern Analysis
Salih Tutun, Washington University in St Louis, St Louis, MO, United States, Ali Tosyali, Kazim Topuz, Anol Bhattacherjee
In this research, we introduce XScanner, an innovative explainable AI (XAI) approach designed for comprehensive screening and pattern
analysis across various problems. XScanner leverages clinical data and protocols to ensure reliability, employing an XAI framework to
predict outcomes and provide clear explanations of the underlying features influencing each prediction. This transparency is critical for
building trust and acceptance of AI applications. XScanner distinguishes itself by generating full-color diagnostic images that visualize
patterns in the data, akin to radiological images, offering a novel way for experts to interpret AI-driven insights. Unlike traditional black-box
AI models, XScanner’s explainability enhances its utility by making the decision-making process accessible and understandable to experts,
thereby improving outcomes through informed decision-making. Our approach has been rigorously evaluated through various studies,
demonstrating high classification accuracy with average F1 scores. These results underscore XScanner’s potential as a powerful tool for early
screening and analysis, capable of addressing the critical gaps in different business problems. XScanner represents a significant advancement
in the screening and analysis. This research highlights the importance of transparency and explainability in AI, paving the way for future
innovations that can enhance the accessibility and effectiveness of mental health care.
3 - Adaptive Sparse Pca: Enhancing Interpretability and Robustness in High-Dimensional Data
Yifan Xie, Rutgers University, Piscataway, NJ, United States
The surge in high-dimensional datasets has highlighted the limitations of traditional Principal Component Analysis (PCA), particularly the
curse of dimensionality and the need for interpretable, sparse representations. Sparse PCA has emerged as a valuable tool to address these
issues, but existing methods often rely on fixed thresholds, which can be suboptimal and inflexible. We introduce Adaptive Sparse PCA (AS-
PCA), an innovative method that dynamically adjusts the sparsity threshold based on the data's intrinsic characteristics at each iteration. This
approach ensures better adaptability, robustness to noise, and computational efficiency. By focusing on sparse principal components, AS-PCA
enhances interpretability and generalization, making it especially suited for diverse datasets where fixed thresholds may fall short. Our
theoretical analysis and empirical results demonstrate AS-PCA's superiority in balancing variance explanation and sparsity, offering a
powerful tool for modern high-dimensional data analysis.
4 - Robust Wafer Defect Pattern Classification and New Defect Pattern Detection in Imbalanced Data Using Few-Shot Learning
Byunghoon Kim, Hanyang University, Ansan, Korea, Republic of
In the semiconductor industry, Wafer Bin Maps (WBMs) generated through Electrical Die Sorting (EDS) inspection are crucial for
identifying defect patterns on a wafer. The WBMs facilitate the classification of defect patterns based on the specific locations and
distributions of faulty chips. Defect patterns often stem from specific manufacturing processes. Using machine learning models to quickly
classify Wafer Bin Maps (WBMs) during production can greatly enhance process yield and product quality. Additionally, detecting new
defect patterns is also crucial to improve model reliability.
In this study, we utilize the Prototypical Network, a few-shot learning technique, to classify existing defective patterns and detect new ones.
This method effectively handles imbalanced data and requires minimal data for each class, making it particularly well-suited for scenarios
with limited data availability. To reflect real manufacturing conditions, we adapted the existing model and incorporated class prototypes into
the validation process without relying on labeled data. We developed a method for generating prototypes that excludes improperly labeled
data from the dataset. By establishing class prototypes and calculating the distances between validation data and these prototypes, we
effectively classified existing defect patterns and detected new ones. Compared to previous research, our approach demonstrated superior
classification accuracy, particularly in significantly enhancing the accuracy for minority classes.
5 - Machine learning based business competitor identification
Nathaniel Smith, West Virginia University, Morgantown, WV, United States, Jeongsub Choi
Competitor identification (CI) is an essential step for successful business strategy development by strategically navigating the business
environment and making informed decisions. Due to the complexity, various approaches to identifying competitors have been studied in the
literature to shed light on the blind spots of managerial radars. Recently, modeling with machine learning has been actively adopted for
efficient CI with vast amounts of data. In this talk, we present experimental results from predictive models for CI based on business
organizational profiles.
SC64
Regency - 602
Social media, Transfer Learning, and AI
Invited Session
Social Media Analytics
Chair: Wendao Xue, The University of Texas at Austin, Austin, United States
https://submissions.mirasmart.com/InformsAnnual2024/Publication/ProgramBook.aspx 196/127610/18/24, 2:13 PM Program Book
1 - What to Sell and When to Sell in Live Streaming? An Online Decision-Making Approach
Jingwen Zhang, University of Washington, Seattle, WA, United States, Shaohui Wu, Yong Tan
Livestream selling has emerged as a popular and influential e-commerce format, yet little is known about how hosts effectively sell products
in this dynamic environment. This study investigates the decision-making processes of hosts in livestream selling, focusing on their product
assortment and presentation timing strategies. We propose a novel approach that combines the online learning framework of Thompson
Sampling and the COX proportional hazards model to capture hosts' learning and decision-making behaviors. Specifically, we employ
Thompson Sampling to model how hosts learn consumer preferences, predict demand, and decide on the product assortment to present during
their livestreams. Concurrently, we utilize the COX proportional hazards model to examine the factors influencing hosts' decisions on the
timing of each product set presentation. By applying these models to a dataset from one of the largest livestreaming platforms in China, we
demonstrate how our approach effectively captures hosts' product assortment and presentation timing decisions in real practice. Our findings
provide valuable insights into the learning and decision-making processes of hosts in livestream selling and offer managerial implications for
enhancing the effectiveness of this increasingly important e-commerce format.
2 - Using Noisy Interval Data in Information System Research: Theory and Applications
Wendao Xue, University of Texas at Austin, Austin, TX, United States, Huidi Ma, Yifan Yu
This paper examines inference in regression models where one latent variable is known to belong to an interval with a certain probability,
while other variables are measured accurately. We demonstrate that, under certain assumptions, the parameters of interest constitute an
identified set. We propose an estimator for this identified set and show that it can be consistently estimated. Two applications are presented:
(1) regression using variables generated by large language models, and (2) protecting information privacy when sharing data. For each
application, we outline specific tailored assumptions, discuss the identification results, and detail the estimation process. The efficacy of our
proposed method is demonstrated through simulations in each application. Additionally, we employ semi-simulated data to further validate
the effectiveness of our method in these applications. This research illustrates that utilizing noisy interval data can enhance current
methodologies, facilitating more accurate results under less stringent assumptions and reducing potential biases.
3 - Emotion AI in Disguise Spurs Strategic Behavior in Customer Care
Yu Kan, University of Washington- Michael G. Foster School of Business, Seattle, WA, United States
As organizations increasingly turn to AI for addressing customer complaints, understanding the dynamics of user interactions with AI and
human agents becomes imperative. In this study, we investigate customers’ strategic behavior when interacting with agents of different
identity cues on a food delivery platform. We employ two AI-human interactive experiments, where participants are randomly assigned to AI
agents (including a general AI and a GPT-based AI agent), a human agent, or a non-disclosure agent group. We observe significant strategic
behavior and decreased satisfaction only in the non-disclosure group. The participants in the non-disclosure group present a unique paradox.
Even though they report experiencing less negative emotions, their expressions are more negative, and they have the lowest satisfaction
ratings among all groups. Our findings contribute to the growing research on AI-human interactions and emotion regulation, suggesting
intriguing dynamics when the agent’s identity is non-disclosed. This work provides valuable insights for organizations considering AI
adoption for customer service, highlighting the potential challenges and implications for user experience, customer satisfaction, and solution
acceptance.
SC66
Regency - 604
Frontiers in Causal Inference
Invited Session
Artificial Intelligence
Chair: Fei Fang, Yale University, New Haven, CT, United States
Co-Chair: Ruoxuan Xiong, Emory University, Atlanta, GA, United States
1 - Long-Term Causal Inference Under Persistent Confounding via Data Combination
Yuhao Wang, Tsinghua University, Beijing, China, People's Republic of, guido Imbens, Nathan Kallus, Xiaojie Mao
<blockquote mathjax"="">We study the identification and estimation of long-term treatment effects when both experimental and
observational data are available. Since the long-term outcome is observed only after a long delay, it is not measured in the experimental data,
but only recorded in the observational data. However, both types of data include observations of some short-term outcomes. In this paper, we
uniquely tackle the challenge of persistent unmeasured confounders, i.e., some unmeasured confounders that can simultaneously affect the
treatment, short-term outcomes and the long-term outcome, noting that they invalidate identification strategies in previous literature. To
address this challenge, we exploit the sequential structure of multiple short-term outcomes, and develop three novel identification strategies
for the average long-term treatment effect. We further propose three corresponding estimators and prove their asymptotic consistency and
asymptotic normality. We finally apply our methods to estimate the effect of a job training program on long-term employment using semi-
synthetic data. We numerically show that our proposals outperform existing methods that fail to handle persistent confounders.
2 - Regression Analysis for Conditional Spillover Effects Under Design-Based Uncertainty
Fei Fang, Yale University, New Haven, CT, United States, Laura Forastiere , Edoardo Airoldi
When interference exists, estimating spillover effects to correct bias for direct effects or the effects themselves is of interest. While regression
analysis is commonly utilized for this task, e.g., regressing the outcome on the neighboring treatments, the analysis often relies on strong
assumptions about the outcome model, such as homogeneity and additivity of the spillover effects. Alternatively, the weighted least squares
(WLS) estimator from regression, with appropriately designed weights and covariate matrix, can be equivalent to the Hajek estimator for
average spillover effects, which is model-free.
https://submissions.mirasmart.com/InformsAnnual2024/Publication/ProgramBook.aspx 197/127610/18/24, 2:13 PM Program Book
In this study, we focus on regression analysis for conditional spillover effects which are useful for understanding the heterogeneity of such
effects across different groups. We propose integrated weights for the WLS estimator, demonstrating its equivalence to the Hajek estimator
when the conditional covariate is categorical, without imposing assumptions on potential outcomes. For conditioning on continuous
covariates, we assume linear outcome models but with heterogeneous coefficients. Under this assumption and design-based randomness, we
define interpretable estimands and obtain consistent WLS estimators. We establish central limit theorem and derive a cluster-robust variance
estimator under partial interference assumption. If homogeneous spillover effects can be further assumed, then the spillover effects can be
decomposed and obtained from the dyadic regression between one’s outcome and others’ treatments separately. We examine the relationship
between these estimators and the Hajek estimator and provide a variance estimator accommodating repeated dyads. Finally, we apply these
methods to Honduras datasets to evaluate the spillover effects of educational programs on villagers' adoption behaviors.
3 - When the Bartik instrument meets topic models: a principled approach to text-based causal inferences
Tong Guo, Duke University, Durham, NC, United States
We propose the Bartik instrument as a principled source of IV for text-based causal inferences. The proposed procedure uses the estimated
topic distribution from topic models to construct the Bartik instrument, and provides a useful alternative when the traditional BLP or
Waldfogel IV does not work. We illustrate our approach in a study of the impact of social media news on Impossible Meats adoption by local
restaurants and stores. Understanding this connection is challenging because of the lack of empirical measurement of local entrepreneur
decisions at scale and, more importantly, the endogeneity of marketing communication to unobserved local demand shocks. We devise a
unique location-specific adoption metric based on social media announcements, tracking local business decisions from 2015 to 2019. This
metric is linked to comprehensive marketing communication extracted from social media using Natural Language Processing. We leverage
the quasi-random variations of county-quarter-level news production for different topics to causally identify the linkage between social media
publicity and adoption. We find that local coverage of social media news on the innovation increases the adoption of impossible meat
products by local entrepreneurs. Interestingly, news content about producer financials was as important as content about sustainability and
taste in driving local adoption of impossible meat products, potentially due to the signaling role of financial news for the trustworthiness of
the technology (thus lowering uncertainty) and the trendiness of the technology (thus providing free marketing to small businesses who
adopted the innovation).
4 - Data Integration for Efficient Causal Inference
Harsh Parikh, Johns Hopkins University, Durham, NC, United States, Elizabeth Stuart, Kara Rudolph
Recent works in the literature have focused on data integration from various studies for efficient causal effect estimation. These works
typically assume that the intervention and outcome measures are same across all studies. In our work, we explore scenarios where the
outcome measures are disparate across studies. We theoretically and empirically investigate when and how integrating data with disparate
outcome measures can yield performance enhancement. Our findings indicate that performance enhancement is possible either under strong
assumptions relating various outcome measures or on the existence of a calibration data where various outcome measures are simultaneously
observed. We apply our data integration approach to study the effect of medication for treating opioid use disorder on the strength of
withdrawal symptoms.
SC67
Regency - 605
Daniel H. Wagner Competition II
Award Session
Daniel H Wagner
Chair: James Cochran, The University of Alabama, Tuscaloosa, AL, United States
1 - Redesigning Zoning Systems for Equitable and Efficient Last-Mile Delivery at Ninja Van
Stanley Lim, Michigan State University, East Lansing, MI, United States, John Carlsson, Sheng Liu, Han Yu, Witsanu Arntong, Ee
Hsin Tan
Last-mile logistics poses many challenges and high costs for firms. We develop a zoning system for Ninja Van to enhance last-mile delivery
efficiency and boost customer satisfaction. By incorporating Voronoi Diagrams, stochastic and robust optimization techniques, we introduce a
novel zoning optimization procedure and tested the model's efficacy in a major Southeast Asian city. The results showed substantial reduction
in delivery work spans and the time drivers spend on deliveries. We project yearly savings exceeding $400,000 just by redefining zones and
their boundaries in one country.
2 - Transportation marketplace rate forecast using signature transform
Haotian Gu, UC Berkeley, Albany, CA, United States, Xin Guo, Timothy Jacobs, Philip Kaminsky, Xinyu Li
This work develops a novel statistical method leveraging signature transforms to predict freight transportation marketplace rates. Our
approach utilizes the universal nonlinearity property of signature transform to linearize the feature space and hence translates the forecasting
problem into linear regression, and uses signature kernels for efficient comparison of time series data. This enables precise feature generation
and identification of regime switching. Deployed by Amazon trucking operations, our algorithm surpasses industry models by improving
prediction accuracy by over fivefold.
SC68
Regency - 606
Undergradutate Operations Research Prize II
Award Session
Undergraduate Operations Research Prize
https://submissions.mirasmart.com/InformsAnnual2024/Publication/ProgramBook.aspx 198/127610/18/24, 2:13 PM Program Book
Chair: Rachel Cummings, Columbia University, New York, NY, United States
1 - Planning Adaptive Experiments: A Mathematical Programming Approach
Jimmy Wang, Columbia University, New York, NY, United States, Ethan Che
Standard adaptive experimentation algorithms overlook important practical issues including multiple objectives, non-stationarity, batched
feedback, constraints, and personalization. Moving away from developing bespoke algorithms for each setting, we present a mathematical
programming framework that flexibly incorporates a wide range of objectives, constraints, and statistical procedures. By formulating a
dynamic program in the batched limit, our modeling framework enables the use of scalable optimization methods (e.g., SGD and auto-
differentiation) to solve for treatment allocations.
2 - The maximum length car sequencing problem
Lara Pontes, Universidade Federal da Paraíba, João Pessoa, Brazil, Carlos Neves
The maximum length car sequencing problem aims to support the assembly operations arising in multinational automotive companies. We
propose an integer programming formulation to schedule the maximum number of cars without violating spacing constraints associated with
car options (sunroof, radio), a valid combinatorial upper bound, and iterative algorithms to solve the problem when good primal bounds are
not available. An ILS-based heuristic enhances the performance of the exact methods. Computational results achieve low gaps for benchmark
instances and optimally solve the company's demands.
3 - A Projection-Free Method for Solving Convex Bilevel Optimization Problems
Khanh-Hung Giang-Tran, University of Sydney, Sydney, Australia
We consider a class of convex bilevel optimization problems, where we minimize a convex smooth objective function over the optimal
solution set of another convex smooth constrained optimization problem. We propose a new projection-free method for convex bilevel
optimization requiring only a linear optimization oracle over the domain. We establish $O(t^{−1/2})$ convergence rate guarantees in terms of
both inner- and outer-level objectives and demonstrate how additional assumptions result in accelerated rates of up to $O(t^{−1})$ and
$O(t^{−2/3})$ for inner- and outer-levels respectively.
4 - Allocation of Surveillance Assets in Undersea Warfare
Sebastian Martin, United States Naval Academy, Annapolis, MD, United States
Recent advances in submarine technology require an effective national strategy for detecting undersea threats. We formulate an optimization
model to effectively deploy sensors, maximizing the expected number of detected targets based on historical threat data and sensor resources.
The model accommodates sensor overlap, introducing nonlinearity, which we handle by linearizing with a logarithmic transformation and
tangent line approximations. We solve the model with notional sensor and maritime shipping traffic data.
SC69
Regency - 607
Impacts of Emerging Technologies
Invited Session
eBusiness
Chair: Jiali Zhou, American University, Washington DC, DC, 200016, United States
1 - Navigating Product Diversification in Live Streaming E-Commerce: Evidence from Douyin
Jingyun Hu, Clemson University, Pendleton, SC, United States, Yi Gao, Keran Zhao
The past decade has witnessed the growing prevalence of live streaming selling (LSS). As streamers serve as a proxy between manufacturers
and consumers, product assortment management has been a critical strategic factor for their success. Relying on data collected from Douyin,
a prominent Chinese LSS platform, this study aims to investigate the impact of diversification on LSS performance through the lens of
adding new categories in live streaming sessions. The results in this study show that product category diversification, in general, has a
significant positive effect on sales during live streaming events. However, this effect is alleviated by the semantic similarity between the
newly introduced and existing categories. In addition, we find that product diversification has a negative impact on the sales of incumbent
categories. This study contributes to the literature on product assortment by providing insights into the role of product diversification in the
context of real-time selling. We also discuss the managerial implications for the platform and streamers.
2 - How Does Popularity Information Affect Product Design?
Guangrui Li, York University, Toronto, ON, Canada, Zheng Gong, Zhepeng Li
Popularity information serves as quality signal for consumers to learn about products. Past literature has shown that the revealing popularity
information may herd consumers to the popular products and leads to a superstar phenomenon, and narrow-appeal products benefit more
from the same popularity. Up to our knowledge, the previous literature takes the product design as given, but the firms could change their
product design in response to the popularity information revealing. In this paper, we examine the impact of popularity information revealing
on firms’ product design strategy by using a policy change on Wechat Official Account platform that reveals articles’ popularity to
subscribers. More specifically, we are interested in understanding the following aspects: (1) The quality and amount of advertisement in the
content; (2) The topic choice of the contents. Our results show that the official accounts tend to increase their topic diversity as a strategic
response to the demand shock induced by the reveal of popularity information.
3 - How Generative AI Shapes Video Sharing Platform: a Tale of Two Forces
Luying (Iris) Qiu, Hong Kong University of Science and Technology, Hong Kong, Hong Kong, Oliver Wei, Jiali Zhou, Weiyin Hong,
Kai-lung Hui
We study the impact of generative AI on participants of video-sharing platforms using a natural experiment setting: the introduction of AI
summary feature on one of the largest video-sharing platforms in China in November 2023. By leveraging a regression discontinuity strategy,
https://submissions.mirasmart.com/InformsAnnual2024/Publication/ProgramBook.aspx 199/127610/18/24, 2:13 PM Program Book
we find that AI summaries significantly reduced creators’ contribution, viewers’ in-video engagement, and the number of reviews. We also
find that the reduction in creators’ contribution is more severe for inexperienced creators. We draw related research and managerial
implications.
4 - Logistics Service Sharing in Cross-Border E-Commerce
Nevin Mutlu, Eindhoven University of Technology, Eindhoven, Netherlands, Faranak Khooban, Ton de Kok
As demand for cross-border e-commerce has grown rapidly, challenges have emerged for both retailers and consumers participating in this
global market. Retailers have been struggling with high logistics costs to fulfill cross-border demand, while they also suffer from consumers'
lack of trust in foreign retailers. In this paper, we study a cross-border collaboration scheme between a domestic and a foreign retailer to
mitigate these challenges. This entails a co-opetition framework where the domestic retailer is responsible for the last-mile delivery of the
foreign retailer's orders in exchange for a logistics service fee. We model demand via an MNL choice model incorporating trust- and price-
sensitive consumers. We compare the market outcomes of the two retailers in "pre-collaboration" and "post-collaboration" settings. We find
that there exist win-win outcomes where both retailers benefit from collaboration under realistic settings. We also show that a cooperative
mechanism can lead to higher profits for both retailers compared to the non-cooperative mechanism for setting the logistics service fee, if the
contract terms are decided carefully.
SC70
Regency - 701
Optimization Approaches to Enhancing Biodiversity
Invited Session
Energy: Natural Resources
Chair: Sabah Bushaj, SUNY, Plattsburgh, NY, 12901, United States
1 - Optimal Sampling Strategy for Probability Estimation: An Application to the Agricultural Quarantine Inspection Monitoring
Program
Huidi Ma, The University of Texas at Austin, Austin, TX, United States, Benjamin Leibowicz, John Hasenbein
Imported agricultural pests can cause substantial damage to agriculture, food security, and ecosystems. In the United States, the Agricultural
Quarantine Inspection Monitoring (AQIM) program conducts random sampling to estimate the probabilities that cargo and passengers
arriving at ports of entry carry pests. Assessing these risks accurately is critical to enable ef- fective policies and operational procedures. In
this paper, we formulate an optimization model that minimizes the mean squared error of the probability estimates that AQIM obtains. The
central decision-making tradeoff that the model explores is whether it is preferable to sample more arriving containers (and fewer boxes per
container) or more boxes per container (and fewer containers), given limited resources. We first derive an analytical solution for the optimal
sam- pling strategy by leveraging several approximations. Then, we apply our model to a numerical case study of maritime cargo sampling at
the Port of Long Beach. We find that, across a wide range of parameter settings, the optimal strategy samples more containers (but fewer
boxes per container) than the current AQIM protocol. The difference between the two strategies and the accuracy improvement with the
optimal approach are larger if the pest statuses of boxes in the same container are more strongly correlated.
2 - Managing Large-Scale Invasions: Simulation-Optimization with Gaussian Dispersal Kernels and Stochastic Seed Establishment
Sevilay Onal, University of Illinois Springfield, Springfield, IL, United States
Biological invaders cause substantial economic losses exceeding $21 billion annually for the US government by damaging crops, agricultural
lands, habitats, and sustainability. Aggressive invaders recognized under the Federal Noxious Weed Act of 2000, such as Sericea lespedeza,
drive continuous federal efforts to prevent, eliminate, and manage invasive species. This highlights the critical need for optimal control
methods to minimize their detrimental effects on biodiversity and the economy. To our knowledge, this paper is the first to integrate
completely random occurrences of an invader that are not explainable by biophysical impacts within an integrated simulation- optimization
model to control Sericea. The simulation model estimates the natural behavior of seed dispersal using Gaussian cell-to-cell transition
probabilities and an algorithm developed to estimate the random or unexpected behavior of the invader. The case study data and parameter
calibration are based on large-scale field data collected in Kansas and Oklahoma. We simulate Sericea growth over a 2500-acre landscape for
25 years and optimize search and treatment locations while minimizing its economic damage to forage production under a restricted budget.
The results of the simulation-optimization framework provide information on the optimal treatment location and frequency, the optimal
search speed, and the cost-benefit analysis of treatment in various invasion and seed dispersal scenarios. The computational results suggest
that more budget is allocated to search in lower-density areas than treatment. If the weed is detected in a cell, the entire area is searched for a
possible stochastic establishment. In higherdensity scenarios, treatment is preferred, and a search
3 -
An Integer Programming Approach for Designing Wildlife Conservation Corridors with Geometry Requirements
Chao Wang, Arizona State University, Tempe, AZ, United States, Jorge Sefair
Conservation corridors have been widely used to prevent biodiversity loss by linking fragmented habitats. One of the key design challenges
involves identifying corridors that meet ecological requirements, including considerations of connectedness, width, and length. In this study,
we propose a MIP model to enforce geometry requirements into the corridor design problem, aiming to maximize the corridor’s utility within
a grid landscape. To enhance solution performance, we develop a Branch-and-Cut algorithm, paired with a bound-improving heuristic and a
variable reduction preprocessing algorithm. A byproduct of our work is a polynomial-time algorithm for the solution of the minimization
variant of this problem, focusing on finding a least-cost corridor with a specified width. Our algorithm is tested on a real instance for the
protection of the Florida panther and a set of computer-generated landscapes. Test results demonstrate the algorithm’s ability to find near-
optimal solutions for landscapes comprising up to 3000 patches within a two-hour timeframe.
4 - An Adaptive Simulation-Optimization Framework to Survey and Control Invasive Species
Sabah Bushaj, SUNY, Plattsburgh, NY, United States
https://submissions.mirasmart.com/InformsAnnual2024/Publication/ProgramBook.aspx 200/127610/18/24, 2:13 PM Program Book
Invasive species are a serious threat to ecosystems worldwide, calling for innovative methods to survey and control them effectively. This
research presents a new approach—an adaptive simulation-optimization framework—specifically designed to tackle the complexities of
managing invasive species. This framework is highly adaptable, providing standardized features for different species and offering various
simulation options to match different environments and species dynamics. Researchers and practitioners can use this framework to tailor
simulations for accurate surveys and strategic control efforts. By incorporating advanced solvers and allowing for problem-specific
adjustments, it ensures reliable optimization customized to the specific challenges of invasive species scenarios.
SC71
Regency - 702
Advanced Algorithms for Machine Learning and Discrete Optimization
Invited Session
Data Mining
Chair: Yongchun Li, Georgia Institute of Technology, Atlanta, GA, United States
1 - Convergence of Gradient Descent with Small Initialization for Unregularized Matrix Completion
Salar Fattahi, University of Michigan, Ann Arbor, MI, United States, Jianhao Ma
We study the problem of symmetric matrix completion, where the goal is to reconstruct a positive semidefinite matrix of low rank from only
a subset of its observed entries. For the first time, we prove that vanilla gradient descent (GD) with small initialization converges to the
ground truth, without requiring any explicit regularization or projection. This convergence result holds true even in the over-parameterized
scenario, where the true rank is unknown and conservatively over-estimated. It also achieves a near-optimal sample complexity. The existing
results for this problem either require explicit regularization, a sufficiently accurate initial point, or exact knowledge of the true rank. At the
crux of our method lies a novel weakly-coupled leave-one-out analysis, which allows us to establish the global convergence of GD, extending
beyond what was previously possible using the classical leave-one-out analysis.
2 - Derivation and Generation of Path-Based Valid Inequalities for Transmission Expansion Planning with New Bus Integration
Behnam Jabbari Marand, North Carolina State University, Raleigh, NC, United States, Adolfo Escobedo
This research tackles an extension of the DC OPF-based transmission expansion planning problem where both new lines and buses can be
added to the existing network. To handle the general intractability of this problem, the primary computational strategy involves deriving valid
inequalities (VIs) that leverage the problem's structure. Effective VIs are generated by considering different paths within the existing and
expanded network both a priori and during the solution of the dispatch problem. While in the associated theorems identifying the longest
paths is necessary to ensure validity in the general case, the proposed approach circumvents such an inefficient procedure for the majority of
bus pairs. It is shown that the path-based VIs derived through this method dominate those obtained from solving the longest path problem,
leading to a stronger problem formulation. This work also proposes a methodological framework for constructing an effective cut pool and
identifying strong VIs to generate cutting planes. Two classes of VIs result from these insights, and their efficacy and computational
advantages are demonstrated through application to a modified Polish 2383-bus system.
3 - Statistically Optimal K-Means Clustering via Nonnegative Low-Rank Semidefinite Programming
Richard Zhang, University of Illinois, Urbana, IL, United States, Yubo Zhuang, Xiaohui Chen, Yun Yang
K-means clustering is a widely used machine learning method for identifying patterns in large datasets. Semidefinite programming (SDP)
relaxations have recently been proposed for solving the K-means optimization problem that enjoy strong statistical optimality guarantees, but
the prohibitive cost of implementing an SDP solver renders these guarantees inaccessible to practical datasets. By contrast, nonnegative
matrix factorization (NMF) is a simple clustering algorithm that is widely used by machine learning practitioners, but without a solid
statistical underpinning nor rigorous guarantees. In this paper, we describe an NMF-like algorithm that works by solving a nonnegative low-
rank restriction of the SDP relaxed K-means formulation using a nonconvex Burer--Monteiro factorization approach. The resulting algorithm
is just as simple and scalable as state-of-the-art NMF algorithms, while also enjoying the same strong statistical optimality guarantees as the
SDP. In our experiments, we observe that our algorithm achieves substantially smaller mis-clustering errors compared to the existing state-of-
the-art.
4 - Integer Programming for Learning Directed Acyclic Graphs from Non-Identifiable Gaussian Models
Tong Xu, Northwestern University, Evanston, IL, United States, Armeen Taeb, Simge Kucukyavuz, Ali Shojaie
We study the problem of learning directed acyclic graphs from continuous observational data, generated according to a linear Gaussian
structural equation model. State-of-the-art structure learning methods for this setting have at least one of the following shortcomings: i) they
cannot provide optimality guarantees and can suffer from learning sub-optimal models; ii) they rely on the stringent assumption that the noise
is homoscedastic, and hence the underlying model is fully identifiable. We overcome these shortcomings and develop a computationally
efficient mixed-integer programming framework for learning medium-sized problems that accounts for arbitrary heteroscedastic noise. We
present an early stopping criterion under which we can terminate the branch-and-bound procedure to achieve an asymptotically optimal
solution and establish the consistency of this approximate solution. In addition, we show via numerical experiments that our method
outperforms three state-of-the-art algorithms and is robust to noise heteroscedasticity, whereas the performance of the competing methods
deteriorates under strong violations of the identifiability assumption. The software implementation of our method is available as the Python
package micodag.
SC72
Regency - 703
Socially and Environmentally Responsible Operations
Invited Session
https://submissions.mirasmart.com/InformsAnnual2024/Publication/ProgramBook.aspx 201/127610/18/24, 2:13 PM Program Book
MSOM: Sustainable Operations
Chair: Vibhuti Dhingra, Schulich School of Business, York University, Toronto, ON, Canada
Co-Chair: Ali Kaan Tuna, Tilburg University, Tilburg, N/A, Netherlands
1 - Managing Carbon-Neutral Delivery in Online Retailing
Huseyn Abdulla, University of Tennessee, Knoxville, TN, United States, Seulchan Lee, Han Oh
Order-delivery related emissions constitute the vast majority of the environmental footprint of online retailers. A growing segment of
consumers has become concerned with these emissions, discouraging them from shopping online. To alleviate these concerns, many online
retailers consider committing to carbon-neutrality in the order delivery domain. These retailers partner with Sustainability as a Service (SaaS)
providers to offer a voluntary, carbon offset-based green shipping option to eco-conscious consumers to reduce the cost burden of offsetting
the emissions. Using a game theoretical approach, we examine the conditions under which it is optimal for a profit-maximizing online retailer
to become carbon-neutral in the order delivery domain and to offer a consumer-paid green shipping option by partnering with a SaaS
provider. We compare two common offset payment models that back the green shipping option, namely, preset and calculated offset payment
models, and explore the implications of a hybrid approach that combine the advantages of both models.
2 - An Operational Perspective on the Role of Liability in Microfinancing
Elaheh Rashidinejad, Rotman School of Management, University of Toronto, Toronto, ON, Canada, Opher Baron, Gonzalo Romero
We study the effects of different liability structures on the microfinancing performance in low- and middle- income countries through an
operational lens. We investigate this problem using a Newsvendor with financing and effort framework. The Base of the Pyramid
entrepreneur's outcome depends on her production quantity, effort level and business characteristics. The entrepreneur's effort directly
impacts the success of her business in a multiplicative way. Furthermore, she starts with zero initial budget and borrows a loan to operate her
business.One extreme of the liability structure we consider is a social bank that operates as an external lending institution and cannot collect
the entire debt in the presence of business losses. The other extreme is a community bank that consists of peers in a community gathering
their savings to form a bank and using their social ties to collect the debt eventually. We study these banks under profit-maximizing or zero-
profit objectives, where the banks may face bankruptcy costs. Our study reveals that under any of these microfinancing setups, the effort
exerted by the entrepreneur is the main driver of her equilibrium utility. Further, the effectiveness of microfinancing depends on two main
factors: the bankruptcy cost and the product's critical fractile. Surprisingly, we show that the peer pressure mechanism may not always benefit
both the entrepreneur and the bank. We establish conditions under which the zero-profit (profit-maximizing) community bank generates
higher social welfare than the zero-profit (profit-maximizing) low-liability banks. A numerical study provides additional insights on the
performance of moderate-liability banks.
3 - Responsible Operations in the Mining Sector: Do Penalties Improve Safety?
Vibhuti Dhingra, Schulich School of Business, York University, Toronto, ON, Canada, Anna Saez de Tejada Cuenca
Mining is one of the most hazardous industries in the world. Workplace injury rates in mining are 14.2 for every 100,000 workers—273%
higher than other sectors, and the industry had more than 500 fatalities between 2017-2022. With the expected growth in mineral extraction
necessary to power a green economy, the sector is only poised to expand further.
Given their massive footprint, mines are regularly inspected for compliance with workplace safety and health standards. The Mine Safety and
Health Administration (MSHA) in the U.S., for example, conducts over 12,000 inspections every year. If a violation is detected, the
inspectors enforce corrective measures and levy financial penalties on the responsible entities. In 2010, for instance, the MSHA imposed 160
million dollars in fines for mine safety violations.
Penalties can act as a deterrent against future violations by prompting mine operators to make process improvements such as enhancing
ventilation and ensuring proper machine maintenance. However, penalties can also be ineffective if mine operators do not pay the finesand
instead continue business as usual.
What is the causal effect of penalties on mine safety? In this paper, we answer this research question using data on mine safety inspections,
violations, and penalties from the MSHA—spanning over one million inspections conducted on more than 4,000 coal and 18,000 metal/non-
metal mines in the U.S. involving over 14,000 mine operators. Our identification strategy leverages the passage of the MINER Act in 2006
that significantly increased the penalties imposed for safety violations.
4 - Outsourcing, Reshoring, and Carbon Emissions in Supply Chains
Ali Kaan Tuna, Tilburg University, Tilburg, Netherlands, Robert Swinney
Nearshoring has garnered significant attention from firms, governments, and policymakers in recent years, and has emerged as one of the
most prominent supply chain strategies to build more sustainable (and resilient) supply chains. In decentralized supply chains, an agent's
reshoring decision might have an important impact on the whole supply chain. Motivated by the potential economic and environmental
consequences of nearshoring in decentralized supply chains, we study a two-tier decentralized supply chain, where a buyer needs to source a
critical component from a supplier to manufacture a single end-product.
Well before the buyer makes any procurement or production decisions, the firms first decide on their locations to form a supply chain to
manufacture, with the buyer acting as a leader in this phase. They can form a fully offshore supply chain, a hybrid supply chain with the
buyer onshore and the supplier offshore, or a fully nearshore supply chain. Firms' reshoring decisions change their fixed and variable costs,
the per-unit environmental impact of their production and distribution processes, and when the buyer has to choose a procurement quantity
for the component or a production quantity for the end-product. Our chief goal is to understand when the firms form each type of supply
chain configuration in equilibrium, and the environmental implications of the equilibrium outcome. We also seek to understand how the
relative power of the firms in the supply chain has an impact on their reshoring decisions and the subsequent environmental impact.
https://submissions.mirasmart.com/InformsAnnual2024/Publication/ProgramBook.aspx 202/127610/18/24, 2:13 PM Program Book
SC73
Regency - 704
MSOM Student Paper Competition Finalists
Award Session
Manufacturing and Service Operations Management (MSOM)
Co-Chair: Vasiliki Kostami, HEC Paris, Jouy-En-Josas, France
1 - Optimizing Health Supply Chains with Decision-Aware Machine Learning
Tsai-Hsuan (Angel) Chung, The Wharton School, UPenn, Philadelphia, PA, United States
We address the problem of allocating limited medical resources in a developing country by combining ML (to predict demand) with
optimization (to optimize allocations). A key challenge is aligning the ML model's loss function with the decision loss in optimization. We
propose a scalable decision-aware learning framework and successfully deployed in collaboration with the Sierra Leone government across
1,123 healthcare facilities nationwide, leading to a 15-29% increase in medicine consumption and improved real-world patient access to care.
2 - Signaling Competition in Two-Sided Markets
Yuri Fonseca, Stanford University, Stanford, CA, United States
We consider decentralized platforms facilitating many-to-many matches between two sides of a marketplace. In the absence of direct
matching, inefficiency in market outcomes can easily arise. For instance, popular supply agents may garner many units from the demand side,
while other supply units may not receive any match. A central question for the platform is how to manage congestion and improve market
outcomes. We study the impact of a detail-free lever: the disclosure of information to agents on current competition levels. Disclosing
competition reduces the perceived value of popular units, but, at the same time, it can help agents on the other side better elect across options.
How large are such effects, and how do they affect overall market outcomes? We answer this question empirically. We partner with the largest
service marketplace in Latin America, which sells non-exclusive labor market leads to workers. We propose a structural model which allows
workers to internalize competition at the lead level and captures the equilibrium effect of such reaction to competition at the platform level.
We estimate the model by leveraging agents' exogenous arrival times and a change in the platform's pricing policy. Using the estimated
model, we conduct counterfactual analyses to study the impact of signaling competition on workers' lead purchasing decisions, the platform's
revenue, and the expected number of matches. We find that signaling competition is a powerful lever for the platform to reduce congestion,
redirecting demand, and ultimately improving the expected number of matches for the markets we analyze.
3 - Dynamic Matching with Post-allocation Service and its Application to Refugee Resettlement
Soonbong Lee, Yale university, New Haven, CT, United States
Motivated by our collaboration with a major refugee resettlement agency in the U.S., we study a dynamic matching problem where each new
arrival (a refugee case) must be matched immediately and irrevocably to one of the static resources (a location with a fixed annual quota). In
addition to consuming the static resource, each case requires post-allocation services from a server, such as a translator. Given the uncertainty
in service time, a server may not be available at a given time, thus referred to as a dynamic resource. Upon matching, the case waits to avail
service in a first-come-first-serve manner. Bursty matching may result in undesirable congestion of the servers. Consequently, the planner
(the agency) faces a dynamic matching problem with an objective combining the matching reward (pair-specific employment outcomes) with
the cost for congestion for dynamic resources and over-allocation for the static ones. Motivated by the observed fluctuations in the refugee
pools across the years, we aim to design algorithms with no distributional knowledge. We develop learning-based algorithms that are
asymptotically optimal in certain regimes, easy to interpret, and computationally fast. Our design is based on learning the dual variables of
the underlying optimization problem; however, the main challenge is the time-varying nature of the dual variables associated with dynamic
resources. Our theoretical development integrates techniques from Lyapunov analysis, adversarial online learning, and stochastic
optimization. When tested on our partner agency’s data, our method outperforms existing ones making it a viable candidate for replacing the
current practice upon experimentation.
4 - Causal Message Passing: A Method for Experiments with Unknown and General Network Interference
Sadegh Shirani, Stanford University, Stanford, CA, United States
Randomized experiments are a powerful methodology for data-driven evaluation of decisions. Yet, their validity may be undermined by
network interference. This study introduces a new framework to accommodate complex and unknown network interference. Our framework,
termed causal message-passing, is grounded in approximate message passing methodology. Utilizing causal message-passing, we introduce a
practical algorithm to estimate the total treatment effect. We demonstrate the effectiveness of this approach across five numerical scenarios,
each characterized by a distinct interference structure.
SC74
Regency - 705
ENRE: Risk Analysis for Electricity Markets
Invited Session
ENRE: Electricity
Chair: Ryan Ent, University of Massachusetts-Amherst, Ringoes, NJ, 08551, United States
Co-Chair: Golbon Zakeri, University of Massachusetts - Amherst, Amherst, MA, United States
1 - Analysis of ISO New England's Energy Imbalance Reserve Product with Risk Averse Agents
Ryan Ent, University of Massachusetts-Amherst, Amherst, MA, United States, Golbon Zakeri
https://submissions.mirasmart.com/InformsAnnual2024/Publication/ProgramBook.aspx 203/127610/18/24, 2:13 PM Program Book
In wholesale electricity markets, operating reserves are contracts for generation capacity sold by electricity providers to provide the power
system with flexibility in the case of N-1-1 contingencies. In 2025, the independent system operator of New England(ISO-NE) will introduce
a co-optimized day ahead energy and reserve market that contains four reserve products with a call option settlement based on the difference
between the real time price of energy and a strike price set by ISO-NE. Part of this market is a new product, the energy imbalance reserve,
which will make up the difference between energy cleared on the day ahead market and the system operator's real time load forecast. ISO-
NE's goals for this market are to incentivize the generators in their fleet, mainly reliant on natural gas, to invest in advanced fuel and increase
energy purchased on the day ahead to a level equal to the expected real time load. To analyze how this market may or may not achieve these
goals, we model the simultaneous equilibrium of multiple risk-averse generators participating in day ahead and real time markets for energy
with and without the presence of energy imbalance reserve.
2 - ISO New England Regional Energy Shortfall Threshold Project
Jinye Zhao, ISO New England, Holyoke, MA, United States
ISO New England is pioneering the development of a Regional Energy Shortfall Threshold (REST), establishing an acceptable level of
regional energy shortfall risk during extreme weather conditions. The development of REST will utilize the Probabilistic Energy Adequacy
Tool, which is capable of identifying weather events that could cause energy deficits as well as assessing the likelihood and severity of such
events. REST will serve as an energy adequacy standard for extreme weather events, complementing the widely used 1-day-in-10-year
resource adequacy criterion. This presentation focuses on ISO’s current thinking regarding the selection criteria for extreme weather events,
the metrics used to quantify energy adequacy risk and the frequency of REST evaluations.
3 - Towards Completion of Risk Markets
Iman Khajepourtadvani, University of Massachusetts Amherst, Amherst, MA, United States, Golbon Zakeri
The seminal paper of Ralph and Smeers (2011) demonstrates that when risk markets are complete, the competitive risk-averse equilibrium of
a game, where agents are endowed with coherent risk measures is equivalent to a system optimization problem. This finding is of interest to
policymakers in many markets, including energy markets. In this presentation, we will lay out some foundational steps for completing risk
markets.
4 - The Impact of Capacity Accreditation on Long-term Resource Adequacy
KE XIN ZUO, Cornell University , Ithaca, NY, United States, Jacob Mays
The rapidly evolving global energy landscape reveals critical flaws in current resource adequacy (RA) mechanisms. Weak non-performance
penalties in capacity markets incentivize suppliers to over-promise and under-deliver, leaving the grid vulnerable during extreme events.
Central to this issue is capacity accreditation - the process by which ISOs assess each resource's contribution to system reliability during
stress periods. A popular approach to assigning capacity credits is measuring generators’ effective load carrying capability (ELCC) over
designated performance assessment hours (PAHs). However, the misalignment of these PAHs with actual shortage events, along with
inconsistencies in measuring shortfall risk, often lead to systemic over-accreditation. This paper first addresses the need for a standardized
characterization of resource adequacy in wholesale electricity markets. Building on this foundation, we introduce an economically aligned
ELCC and corresponding capacity accreditation framework that incentivizes optimal levels of capacity investment while meeting specified
reliability standards. Using a stochastic optimization framework, we then compute the market equilibria arising under various approaches to
accreditation for a numerical example, demonstrating the sub-optimality of standard practices. This study contributes to understanding how
capacity accreditation affects long-term resource adequacy and informs the design of efficient market mechanisms and policies to ensure a
resilient grid.
SC75
Regency - 706
Modeling the Impact of Climate Change on the Electric Power Sector: Inputs, Assumptions, and Use
Cases
Invited Session
ENRE: Energy-Climate
Chair: Ana Dyreson, Michigan Technological University, Houghton
1 - Enhancing Climate Resilience in Power System Planning: An Integrated Framework
Maren Ihlemann, EPRI International, Golden, BC, Canada
Climate change has been manifesting in more frequent and extreme weather conditions which expose power systems to escalating challenges.
These challenges are projected to intensify over the next decades. Concurrently, there's a growing dependency of electricity as main energy
carrier for multiple sectors, increasing the need for a robust infrastructure.
Traditionally, power system planning was based on extrapolating historically observed conditions, with limited representation of weather data
and visibility of climate-related hazards. However, modeling advances allow for more complex representations of hazards and their impacts
in power system planning. The robustness of the design will depend on the resiliency considerations and metrics adopted, as well as the
tolerance enforced by the planner to withstand stressful periods.
A holistic planning to operations solution requires an integrated planning framework that leverages climate data and asset vulnerabilities
against adverse conditions. By connecting various power systems planning functions, key climate hazards are identified, adapted to, and
ultimately mitigated. As a first step to this framework, an initial buildout is screened over a large set of climate-projected weather data
https://submissions.mirasmart.com/InformsAnnual2024/Publication/ProgramBook.aspx 204/127610/18/24, 2:13 PM Program Book
identifying potential periods at risk using the Risk Screening tool (RiSc). Key potential risks are then passed to the capacity expansions tool,
extended to explicitly incorporate climate hazards and adaptations. To assess expected system reliability, resource adequacy is assessed over a
large set of operating conditions considering system weather dependencies.
The framework is applied to a synthetic Texas case study and results demonstrate that resilient planning significantly enhances system
reliability and effectively mitigates extreme weather impacts.
2 - Generative Super Resolution for Estimating Climate Change Impacts on Power Systems and Communities
Brandon Benton, National Renewable Energy Laboratory, Golden, CO, United States, Grant Buster, Jordan Cox, Andrew Glaws,
Ryan King
As climate change progresses, high-resolution climate-change-impacted weather data is crucial for assessing the impacts on power systems
and communities. We introduce methods using generative adversarial networks (GANs) to downscale global climate model (GCM) data,
achieving enhanced spatiotemporal resolutions suitable for power system and community analysis. We have effectively downscaled key
power system modeling inputs, including wind, solar, temperature, and humidity, to 4km hourly resolutions. This advancement facilitates
more nuanced power system analysis and better adaptation planning. Similar methods combine satellite imagery and human settlement layers
to generate 1-km hourly urban heat island estimates. These estimates are crucial for evaluating heat risk and resilience in urban areas,
including the impact of climate change on energy cost and burden, especially during future extreme events. By leveraging GANs, our
approach significantly improves climate data resolution at low computational cost, offering new insights into the impacts of climate change
on power infrastructure and urban communities, thus aiding in the development of more informed and resilient adaptation strategies.
3 - Navigating Climate Uncertainty: A Collaborative Approach for Grid Planning in California
Liyang Wang, University of California, Berkeley, Berkeley, CA, United States, Owen Doherty, Kripa Jagannathan, Nancy Freitas, Julia
Szinai, Andrew Jones
The escalating frequency and intensity of climate change-driven extreme weather events expose critical vulnerabilities in our energy
infrastructure. Concurrently, the rapid shift towards electrification and renewable energy introduces deep uncertainties into long-term grid
planning. These interconnected stressors are challenging to address using traditional planning and modeling approaches. Planners are seeking
actionable and computationally tractable approaches that explore how climate change impacts grid design and that examine the tradeoffs
among the competing priorities among stakeholders. We describe a co-production effort between electricity sector practitioners, climate
scientists, and power system modelers to translate climate projections into grid model inputs.
Using California as a case study, we define extreme events and establish appropriate thresholds using cluster analysis and extreme value
theory, translating climate projections into demand scenario inputs for a capacity expansion model. We then introduce the application of
Decision-Making under Deep Uncertainty (DMDU) framework to capacity expansion planning. We discuss how DMDU allows for multi-
objective evaluation of different grid designs across a wide range of climate scenarios in a computationally tractable manner, enables planners
to explore different definitions of robustness and grid resilience, and incorporates different risk tolerance from different stakeholders. This
case study provides insight into the actionability of climate science and DMDU, illustrates the challenges in applying DMDU and
coproduction, and bridges the gap between climate science projections and the needs of grid planners.
4 - What is the Contribution of Transmission to U.S. Resource Adequacy?
Charalampos Avraam, National Renewable Energy Laboratory, Golden, CO, United States, Jessica Kuna, Brian Sergi
Resource adequacy encompasses the availability of resources to cover future electricity demand. In a fuel-dominated electricity mix, resource
adequacy metrics aimed to ensure sufficient future nameplate capacity. Fuel-fired generators can be sited in proximity to demand, and
generation capacity offers predictable electricity delivery under normal operating conditions. Traditional resource adequacy frameworks
either focus on ensuring generation capacity availability, or assess whether a system is resource adequate for some given generation capacity.
However, nameplate capacity availability of fuel-fired generators does not always translate into electricity delivery. Fuel delivery to natural
gas-fired plants failed during the 2021 Texas Power Crisis, rendering existing capacity inoperable. Moreover, the rapid deployment of energy-
limited resources, including renewables and batteries, challenges existing resource adequacy frameworks and couples regional electricity
demand and renewables potential via transmission. While ongoing research illuminates the contribution of transmission to resource adequacy,
long-term planning fails to quantify transmission contributions, which are eventually disregarded in real-life decision-making. Our
contribution is twofold. First, we go beyond the existing generation capacity-centered approaches to design metrics for the assessment of
transmission contributions to resource adequacy. Second, we leverage a large-scale capacity expansion model within a multi-objective
optimization framework to quantify the merits of transmission expansion under alternative decarbonization and policy pathways. We enhance
the Regional Energy Deployment System (ReEDS) and generate Pareto Frontiers across engineering and economic resource adequacy
metrics. This work aims to inform policy-makers and regional stakeholders engaged in long-term transmission planning under heavy
penetration of energy-limited resources in highly decarbonized futures.
5 - A Universal Synthetic Weather Data Generator for Use in Power System Resource Adequacy, Short-term Optimization, and
Control
Duc-Huy Pham, Edward P. Fitts Department of Industrial and Systems Engineering, North Carolina State University, Raleigh, NC,
United States, Jordan Kern
Both long-term capacity expansion and short-term operational models of power systems require some characterization of weather-based
uncertainties in electricity supply and demand. Resource adequacy, defined as the ability of the electric grid to satisfy end-user power demand
https://submissions.mirasmart.com/InformsAnnual2024/Publication/ProgramBook.aspx 205/127610/18/24, 2:13 PM Program Book
at any time, is typically measured by subjecting models to past observations (weather reanalysis), outputs from forward-looking global
climate models (GCMs), or a combination of both. To ensure more robust outcomes, system operators should evaluate planning and
operational strategies using very large samples. However, when using weather reanalysis or global climate datasets, increasing the sample
size pulls data from a wider temporal range, potentially obscuring meaningful shifts in the underlying system state (e.g., due to climate
change). Here, we present a fairly universal approach for creating synthetic meteorological ensembles (including short-term forecasts) of
unlimited size that replicate the statistical and time series characteristics of any user-defined “base” dataset (e.g., a 20-year window of GCM
outputs or a 10-year window of historical observations). We demonstrate how this approach can be used in multiple modeling applications
(resource adequacy, short-term optimization, and control) and how it can be easily transferred to any U.S. balancing authority.
SC76
Regency - 707
Critical Mineral and Lithium Supply Chains for Clean Energy Technologies
Invited Session
ENRE: Other Energy
Chair: Erick Jones, University of Texas at Arlington, Arlington, TX, United States
1 - Lithium Supply Chain Optimization
Sarasadat Alavi, University of Texas at Arlington, Arlington, TX, United States
The surging demand for lithium in the United States necessitates the development of efficient and sustainable supply chain strategies. This
research introduces a novel lithium closed-loop supply chain model that optimizes planning for different regions in the USA while
considering uncertainty. The model employs stochastic programming to address inherent uncertainties such as demand fluctuations and
resource availability. Capacity acquisition decisions are incorporated to ensure the supply chain can meet the growing demand for lithium.
The model emphasizes sustainability, seeking to minimize the ecological footprint of the lithium supply chain. Social fairness is also
addressed, ensuring the equitable distribution of benefits and burdens among different communities and stakeholders. This research
contributes to the development of a more resilient, sustainable, and socially responsible lithium industry in the United States by integrating
critical aspects into a comprehensive closed-loop supply chain model. The findings provide valuable insights for policymakers, industry
practitioners, and researchers, facilitating effective management of the lithium supply chain in the face of uncertainty and multiple objectives.
The proposed model is distinct in its holistic approach, balancing economic, environmental, and social considerations. It offers a framework
for evaluating and optimizing lithium supply chains, taking into account the unique challenges and opportunities presented by the U.S.
market. By incorporating stochastic programming and considering regional differences, this research provides a robust tool for decision-
makers to navigate the complexities of the lithium industry and drive sustainable growth.
2 - Lithium Recycling: An Experimental Approach
Oluwatosin Atitebi, University of Texas at Arlington, Arlington, TX, United States
As the need for critical minerals like lithium increases for clean energy technologies, new sources will have to be discovered and quickly.
However, unlike the fossil fuel energy system the minerals needed for the clean energy system can be recycled. Nonetheless, the science
behind recycling lithium is nascent. This work presents our lab scale attempts to efficiently develop experiments to recycle lithium using
industrial engineering and operations research techniques.
3 - A Reverse Logistics Supply Chain Network for Sustainable Value Recovery from Li-Ion Batteries in the United States
Apurba Kumar, University of Arizona, Tuscon, AZ, United States
Li-ion batteries (LIB) are increasingly used in renewable energy storage (e.g., solar and wind) and (hybrid) electric vehicles, contributing to
the UN sustainable development goals and net zero emissions goal. However, proper management of hazardous wastes after their end of life
(EOL) is a great concern. Value recovery from spent LIBs could facilitate sustainable EOL management and recover valuable materials to
create a resilient supply of critical raw materials such as lithium, cobalt, and nickel. This study focuses on the design of an optimal reverse
logistic supply chain network for sustainable collection and recycling of spent LIBs. An optimization model has been developed to (1)
quantify the impact of a governmental policy on promoting the large-scale adoption of LIB recycling technologies and (2) maximize the
economic benefits from LIB recycling in the next ten years. The model was applied to the case of United States and suggested the optimal
facility locations, processing capacities, recycling technology, and material flows for LIB recyclers. According to our analysis, up to 57% of
total available feedstock could be collected over a planning horizon of ten years without any government intervention. However, the
government must come forward with properly planned policies to improve the collection rate further. Depending on the budget available to
the government, mandating 90% collection of total available spent LIBs and matching 30% of recycling revenue can boost the feedstock
collection rate up to 94%.
4 - Critical Minerals for the Clean Energy Transition: An Evaluation
Erick Jones, University of Texas at Arlington, Arlington, TX, United States
Critical minerals, named because of their importance to the economy, are essential for the clean energy economy. The clean energy system is
much more mineral intensive than our current fossil fuel based economy. However, like the current economy these minerals are concentrated
in locations where securing a supply chain may make it more difficult. This work evaluates modeling strategies to assess this supply chain
and shore up the minerals needed for the clean energy transition.
SC77
Regency - 708
https://submissions.mirasmart.com/InformsAnnual2024/Publication/ProgramBook.aspx 206/127610/18/24, 2:13 PM Program Book
Towards a Sustainable Resilient Future - Part B
Invited Session
Computing Society
Chair: Himadri Sen Gupta, University of Oklahoma, 202 W Boyd St, Norman, OK, 73019, United States
Co-Chair: Andrés González, University of Oklahoma, Norman, OK, 73019, United States
1 - A Community Resilience Planning Framework for Maximizing Post-Disaster Healthcare Accessibility
Tasnim Faiz, University of Maryland, College Park, MD, United States, William Hughes, Kenneth Harrison
Accessibility to healthcare services following a disaster is a crucial metric of community resilience. Damages to hospital structures and
transportation networks caused by disaster events significantly impact a community’s accessibility to these services. Moreover, the increased
number of visits to the emergency departments resulting from injured occupants of damaged residential buildings further challenges a
community’s healthcare system capacity. By adopting suitable mitigation actions, a community can avert the reduction in healthcare
accessibility following a disaster, thereby improving resilience. However, identifying suitable mitigation actions for critical infrastructure and
buildings, i.e., a community’s built environment, to achieve that goal is challenging due to the uncertainty in hazard impact, budgetary
limitations, and other restrictions. This work presents a decision-making framework to develop alternative mitigation decision sets for
improved healthcare accessibility as a community resilience goal under varied hazard scenarios. The framework adopts a two-stage robust
optimization approach, which includes a first-stage model to identify optimal mitigation decisions for maximizing accessibility under a set of
hazard impact scenarios and a second-stage model to generate the worst-case scenarios for the current set of actions and evaluate their
consequences on a community’s built environment and social element. A case study with a community in Shelby County, Tennessee,
subjected to earthquake hazards is presented to showcase the applicability of the proposed framework in developing alternative mitigation
strategies.
2 - Text Mining of Practical Disaster Reports: Case Study on Cascadia Earthquake Preparedness
Julia Lensing, University of Washington, Lacey, WA, United States
Many practical disaster reports are published every day across the globe in various forms (e.g., after-action reports, response plans, impact
assessments, and resiliency plans). These reports enable the next generations to learn from past events to best mitigate and prepare for future
disasters. However, the extensive practical literature has limited impacts on research and practice because of the challenge in synthesizing and
analyzing the reports. In this study, we 1) present a corpus of practical reports for text mining and 2) an approach to extract insights from the
corpus using select text mining tools. We validate the approach through a case study that examines practical reports about the preparedness of
the U.S. Pacific Northwest for a magnitude 9 Cascadia Subduction Zone earthquake, which can disrupt lifeline infrastructures for months.
The case study illustrated the types of insights that the approach can extract from a corpus. For example, it identified potential differences in
priorities between Washington and Oregon state-level emergency management, highlighted latent sentiments expressed in the corpus, and
recognized the inconsistent vocabulary across the field. Based on a brief survey of potential user groups, we also discuss opportunities and
challenges with text mining of practical disaster reports. For example, simple tools can provide insights potentially only interpretable by those
experienced in the field, whereas more complex tools using large language models such as ChatGPT can provide more readily accessible
insights, if with known risks of current artificial intelligence. For reproducibility, supporting data and code are made publicly available (DOI:
10.17603/ds2-9s7w-9694).
3 - Machine Learning and Cge Models: A Synergistic Approach to Enhanced Resilience Optimization
Nushra Zannat, The University of Oklahoma, Norman, OK, United States, Charles Nicholson, Harvey Cutler, Himadri Sen Gupta,
Andres Gonzalez
This research presents an innovative framework that integrates machine learning (ML) with optimization techniques to analyze and mitigate
the impacts of disasters on community infrastructure and the economy. We used penalized regression models to interpret computable general
equilibrium (CGE) model outputs and extract predictive insights that quantify the effects of various disaster scenarios as coefficients. The
study focuses on utilizing these ML-derived coefficients to refine the objective of an optimization model for enhancing strategic planning in
disaster response.
Overall, this research highlights the potential of using advanced analytics in enhancing community resilience -- by transforming data into
actionable insights that improve preparedness, response, and mitigation strategies, thereby contributing significantly to sustainable disaster
risk management.
4 - Analyzing the Ability of AI Chatbots to Extract Data and Information About Natural Disasters
Ritvik Karthik, Iowa State University, Ames, IA, United States, Cameron MacKenzie, Wilson Diep, Ule Mewanu
Two major trends in the world right now are the increasing capabilities of artificial intelligence (AI) and climate change. One provides great
opportunities to advance the world, and the other represents one of the great challenges facing humanity. The risk of natural disasters is
increasing due to climate change. Operations researchers frequently sift through a variety of text documents in order to find information and
data that can inform a mathematical model for natural disasters. This research examines whether AI chatbots can be used to find accurate data
and statistics about natural disasters. The first phase compares the accuracy of ChatGPT 3.5 and Google Bard to answer different questions
about natural disasters. The second phase analyzes the ability of ChatGPT 4 to accurately answer questions about a specific natural disaster
based on receiving dozens of news articles as an input. In the first phase, Google Bard was more accurate than ChatGPT 3.5 for all the
different categories of questions. In the second phase, ChatGPT 4 correctly answered most questions. Our results show there is potential in
using AI chatbots to find and reveal information about natural disasters that can be incorporated into operations research models. The ability
of AI chatbots to provide input for modelers will likely increase in the future as AI continues to become more capable and accurate.
SC78
Regency - 709
https://submissions.mirasmart.com/InformsAnnual2024/Publication/ProgramBook.aspx 207/127610/18/24, 2:13 PM Program Book
Discrete Optimization and Machine Learning
Invited Session
Computing Society
Chair: Connor Lawless, Cornell University, Ithaca, NY, United States
1 - Physician Rostering with Downstream Capacity Constraints
Yaron Shaposhnik, University of Rochester, Rochester, NY, United States, Yashi Huang, Arik Senderovich
We collaborate with a large hospital that specializes in cancer treatment and develop an interactive interface that optimizes the physicians'
roster (i.e., sessions) to improve patient flow and the utilization of resources. We conduct numerical experiments using historical data to
assess the accuracy of the infusion load predictions and the potential improvement in different service quality metrics that can be obtained by
optimizing the roster.
2 - A Scalable Linear Programming Based Algorithm for Fair K-Means Clustering
Yakun Wang, Lehigh University, Bethlehem, PA, United States, Aida Khajavirad
In [De Rosa and Khajavirad, 2022], the authors introduce a new linear programming (LP) relaxation for K-means clustering, and in [De Rosa
and Khajavirad, 2024], they examine its theoretical properties, finding it nearly always tight on various data sets. However, this LP includes
$\Omega(n^3)$ inequality constraints, making it impractical for large data sets.
In this work, we propose an efficient algorithm for solving this LP relaxation. The main components of our algorithm are: a separation
algorithm to select a proper subset of inequalities, the LP solver cuPDLP, which is an efficient GPU-based implementation of the Restarted
Primal-Dual Hybrid Gradient method, and warm starting using the effective heuristic kmeans++.
We conduct a comprehensive numerical study on real-world data sets from the UCI library.
We are able to solve all instances with up to 2000 data points to global optimality within one hour, hence, significantly outperforming an
SDP-based exact solver for K-means clustering.
Subsequently, we consider a variant of fair K-means clustering first proposed by Chierichetti et al. In this framework, each data point is
characterized by a sensitive attribute, and a clustering of data is considered fair if the proportion of sensitive attributes in each cluster closely
matches their overall proportion in the dataset. We then propose a fair LP relaxation for this problem by showing that clustering fairness can
be obtained by adding linear inequalities to the K-means clustering problem.
Our numerical experiments demonstrate the effectiveness of the fair LP relaxation for both synthetic and real-world data sets.
3 - Okridge: Scalable Optimal K-Sparse Ridge Regression
Jiachang Liu, Duke University, Durham, NC, United States, Sam Rosen, Chudi Zhong, Cynthia Rudin
We consider an important problem in scientific discovery, namely identifying sparse governing equations for nonlinear dynamical systems.
This involves solving sparse ridge regression problems to provable optimality in order to determine which terms drive the underlying
dynamics. We propose a fast algorithm, OKRidge, for sparse ridge regression, using a novel lower bound calculation involving, first, a saddle
point formulation, and from there, either solving (i) a linear system or (ii) using an ADMM-based approach, where the proximal operators can
be efficiently evaluated by solving another linear system and an isotonic regression problem. We also propose a method to warm-start our
solver, which leverages a beam search. Experimentally, our methods attain provable optimality with run times that are orders of magnitude
faster than those of the existing MIP formulations solved by the commercial solver Gurobi.
SC99
Flex C
MIF Student Poster Session
Poster Session
Minority Issues Forum
Chair: Karen Hicklin, University of Florida, Gainesville, FL, United States
1 - Optimizing Vehicle Fleet Composition in Underground Mines
John Ayaburi, Colorado School of Mines, Golden, CO, United States, Aaron Swift, Jason Porter, Andrea Brickey, Alexandra Newman
The mining industry relies heavily on the use of diesel-powered equipment, which accounts for heat accumulation and exhaust emissions that
can create unsafe working conditions. We present a large-scale production scheduling model that (i) prescribes activity start times at daily
fidelity, taking into account ventilation and refrigeration; and, (ii) determines a fleet composition, relative to a diesel-only fleet, that improves
productivity. We find that the need for refrigeration is delayed and exhaust emission is reduced as more battery-powered equipment is
introduced, showcasing the utility of battery vehicles in maintaining productivity and improving the safety of underground work
environments.
2 - TBD
Edikan Udofia, Colorado School of MInes, Golden, CO, United States
https://submissions.mirasmart.com/InformsAnnual2024/Publication/ProgramBook.aspx 208/1276

--------------------------------------------------------------------------------

